[
  {
    "library_name": "Pandas",
    "url": "https://pandas.pydata.org/pandas-docs/version/2.3.0/whatsnew/v2.3.0.html",
    "version": "v2.3.0.html",
    "title": "Whatâs new in 2.3.0 (June 4, 2025) — pandas 2.3.0 documentation",
    "release_date": "Unknown release date",
    "content": "Release notes\nWhatâs new...\nWhatâs new in 2.3.0 (June 4, 2025)\n#\nThese are the changes in pandas 2.3.0. See\nRelease notes\nfor a full changelog\nincluding other versions of pandas.\nEnhancements\n#\nOther enhancements\n#\nThe semantics for the\ncopy\nkeyword in\n__array__\nmethods (i.e. called\nwhen using\nnp.array()\nor\nnp.asarray()\non pandas objects) has been\nupdated to work correctly with NumPy >= 2 (\nGH 57739\n)\nSeries.str.decode()\nresult now has\nStringDtype\nwhen\nfuture.infer_string\nis True (\nGH 60709\n)\nto_hdf()\nand\nto_hdf()\nnow round-trip with\nStringDtype\n(\nGH 60663\n)\nImproved\nrepr\nof\nNumpyExtensionArray\nto account for NEP51 (\nGH 61085\n)\nThe\nSeries.str.decode()\nhas gained the argument\ndtype\nto control the dtype of the result (\nGH 60940\n)\nThe\ncumsum()\n,\ncummin()\n, and\ncummax()\nreductions are now implemented for\nStringDtype\ncolumns (\nGH 60633\n)\nThe\nsum()\nreduction is now implemented for\nStringDtype\ncolumns (\nGH 59853\n)\nNotable bug fixes\n#\nThese are bug fixes that might have notable behavior changes.\nnotable_bug_fix1\n#\nIn previous versions, comparing\nSeries\nof different string dtypes (e.g.\npd.StringDtype(\"pyarrow\",\nna_value=pd.NA)\nagainst\npd.StringDtype(\"python\",\nna_value=np.nan)\n) would result in inconsistent resulting dtype or incorrectly raise. pandas will now use the hierarchy\nIncreased minimum version for Python\n#\nin determining the result dtype when there are different string dtypes compared. Some examples:\nWhen\npd.StringDtype(\"pyarrow\",\nna_value=pd.NA)\nis compared against any other string dtype, the result will always be\nboolean[pyarrow]\n.\nWhen\npd.StringDtype(\"python\",\nna_value=pd.NA)\nis compared against\npd.StringDtype(\"pyarrow\",\nna_value=np.nan)\n, the result will be\nboolean\n, the NumPy-backed nullable extension array.\nWhen\npd.StringDtype(\"python\",\nna_value=pd.NA)\nis compared against\npd.StringDtype(\"python\",\nna_value=np.nan)\n, the result will be\nboolean\n, the NumPy-backed nullable extension array.\nAPI changes\n#\nWhen enabling the\nfuture.infer_string\noption,\nIndex\nset operations (like\nunion or intersection) will now ignore the dtype of an empty\nRangeIndex\nor\nempty\nIndex\nwith\nobject\ndtype when determining the dtype of the resulting\nIndex (\nGH 60797\n)\nDeprecations\n#\nDeprecated allowing non-\nbool\nvalues for\nna\nin\nstr.contains()\n,\nstr.startswith()\n, and\nstr.endswith()\nfor dtypes that do not already disallow these (\nGH 59615\n)\nDeprecated the\n\"pyarrow_numpy\"\nstorage option for\nStringDtype\n(\nGH 60152\n)\nThe deprecation of setting the argument\ninclude_groups\nto\nTrue\nin\nDataFrameGroupBy.apply()\nhas been promoted from a\nDeprecationWarning\nto\nFutureWarning\n; only\nFalse\nwill be allowed (\nGH 7155\n)\nBug fixes\n#\nNumeric\n#\nBug in\nSeries.mode()\nand\nDataFrame.mode()\nwith\ndropna=False\nwhere not all dtypes would sort in the presence of\nNA\nvalues (\nGH 60702\n)\nBug in\nSeries.round()\nwhere a\nTypeError\nwould always raise with\nobject\ndtype (\nGH 61206\n)\nStrings\n#\nBug in\nDataFrameGroupBy.min()\n,\nDataFrameGroupBy.max()\n,\nResampler.min()\n,\nResampler.max()\nwhere all NA values of string dtype would return float instead of string dtype (\nGH 60810\n)\nBug in\nDataFrame.sum()\nwith\naxis=1\n,\nDataFrameGroupBy.sum()\nor\nSeriesGroupBy.sum()\nwith\nskipna=True\n, and\nResampler.sum()\nwith all NA values of\nStringDtype\nresulted in\n0\ninstead of the empty string\n\"\"\n(\nGH 60229\n)\nBug in\nSeries.__pos__()\nand\nDataFrame.__pos__()\nwhere an\nException\nwas not raised for\nStringDtype\nwith\nstorage=\"pyarrow\"\n(\nGH 60710\n)\nBug in\nSeries.rank()\nfor\nStringDtype\nwith\nstorage=\"pyarrow\"\nthat incorrectly returned integer results with\nmethod=\"average\"\nand raised an error if it would truncate results (\nGH 59768\n)\nBug in\nSeries.replace()\nwith\nStringDtype\nwhen replacing with a non-string value was not upcasting to\nobject\ndtype (\nGH 60282\n)\nBug in\nSeries.str.center()\nwith\nStringDtype\nwith\nstorage=\"pyarrow\"\nnot matching the python behavior in corner cases with an odd number of fill characters (\nGH 54792\n)\nBug in\nSeries.str.replace()\nwhen\nn\n<\n0\nfor\nStringDtype\nwith\nstorage=\"pyarrow\"\n(\nGH 59628\n)\nBug in\nSeries.str.slice()\nwith negative\nstep\nwith\nArrowDtype\nand\nStringDtype\nwith\nstorage=\"pyarrow\"\ngiving incorrect results (\nGH 59710\n)\nIndexing\n#\nBug in\nIndex.get_indexer()\nround-tripping through string dtype when\ninfer_string\nis enabled (\nGH 55834\n)\nI/O\n#\nBug in\nDataFrame.to_excel()\nwhich stored decimals as strings instead of numbers (\nGH 49598\n)\nOther\n#\nFixed usage of\ninspect\nwhen the optional dependencies\npyarrow\nor\njinja2\nare not installed (\nGH 60196\n)\nContributors\n#\nA total of 24 people contributed patches to this release.  People with a\nâ+â by their names contributed a patch for the first time.\nChiLin Chiu +\nIrv Lustig\nIsuru Fernando +\nJake Thomas Trevallion +\nJoris Van den Bossche\nKevin Amparado +\nLOCHAN PAUDEL +\nLumberbot (aka Jack)\nMarc Mueller +\nMarco Edward Gorelli\nMatthew Roeschke\nPandas Development Team\nPatrick Hoefler\nRichard Shadrach\nSALCAN +\nSebastian Berg\nSimon Hawkins\nThomas Li\nWill Ayd\nWilliam Andrea\nWilliam Ayd\ndependabot[bot]\njbrockmendel\ntasfia8 +\nprevious\nRelease notes\nnext\nWhatâs new in 2.2.3 (September 20, 2024)\nOn this page\nEnhancements\nOther enhancements\nNotable bug fixes\nnotable_bug_fix1\nIncreased minimum version for Python\nAPI changes\nDeprecations\nBug fixes\nNumeric\nStrings\nIndexing\nI/O\nOther\nContributors\nShow Source",
    "crawl_status": "success"
  },
  {
    "library_name": "Pandas",
    "url": "https://pandas.pydata.org/pandas-docs/version/2.2.0/whatsnew/v2.2.0.html",
    "version": "v2.2.0.html",
    "title": "Whatâs new in 2.2.0 (January 19, 2024) — pandas 2.2.0 documentation",
    "release_date": "Unknown release date",
    "content": "Release notes\nWhatâs new...\nWhatâs new in 2.2.0 (January 19, 2024)\n#\nThese are the changes in pandas 2.2.0. See\nRelease notes\nfor a full changelog\nincluding other versions of pandas.\nUpcoming changes in pandas 3.0\n#\npandas 3.0 will bring two bigger changes to the default behavior of pandas.\nCopy-on-Write\n#\nThe currently optional mode Copy-on-Write will be enabled by default in pandas 3.0. There\nwonât be an option to keep the current behavior enabled. The new behavioral semantics are\nexplained in the\nuser guide about Copy-on-Write\n.\nThe new behavior can be enabled since pandas 2.0 with the following option:\npd\n.\noptions\n.\nmode\n.\ncopy_on_write\n=\nTrue\nThis change brings different changes in behavior in how pandas operates with respect to\ncopies and views. Some of these changes allow a clear deprecation, like the changes in\nchained assignment. Other changes are more subtle and thus, the warnings are hidden behind\nan option that can be enabled in pandas 2.2.\npd\n.\noptions\n.\nmode\n.\ncopy_on_write\n=\n\"warn\"\nThis mode will warn in many different scenarios that arenât actually relevant to\nmost queries. We recommend exploring this mode, but it is not necessary to get rid\nof all of these warnings. The\nmigration guide\nexplains the upgrade process in more detail.\nDedicated string data type (backed by Arrow) by default\n#\nHistorically, pandas represented string columns with NumPy object data type. This\nrepresentation has numerous problems, including slow performance and a large memory\nfootprint. This will change in pandas 3.0. pandas will start inferring string columns\nas a new\nstring\ndata type, backed by Arrow, which represents strings contiguous in memory. This brings\na huge performance and memory improvement.\nOld behavior:\nIn [1]:\nser\n=\npd\n.\nSeries\n([\n\"a\"\n,\n\"b\"\n])\nOut[1]:\n0    a\n1    b\ndtype: object\nNew behavior:\nIn [1]:\nser\n=\npd\n.\nSeries\n([\n\"a\"\n,\n\"b\"\n])\nOut[1]:\n0    a\n1    b\ndtype: string\nThe string data type that is used in these scenarios will mostly behave as NumPy\nobject would, including missing value semantics and general operations on these\ncolumns.\nThis change includes a few additional changes across the API:\nCurrently, specifying\ndtype=\"string\"\ncreates a dtype that is backed by Python strings\nwhich are stored in a NumPy array. This will change in pandas 3.0, this dtype\nwill create an Arrow backed string column.\nThe column names and the Index will also be backed by Arrow strings.\nPyArrow will become a required dependency with pandas 3.0 to accommodate this change.\nThis future dtype inference logic can be enabled with:\npd\n.\noptions\n.\nfuture\n.\ninfer_string\n=\nTrue\nEnhancements\n#\nADBC Driver support in to_sql and read_sql\n#\nread_sql()\nand\nto_sql()\nnow work with\nApache Arrow ADBC\ndrivers. Compared to\ntraditional drivers used via SQLAlchemy, ADBC drivers should provide\nsignificant performance improvements, better type support and cleaner\nnullability handling.\nimport\nadbc_driver_postgresql.dbapi\nas\npg_dbapi\ndf\n=\npd\n.\nDataFrame\n(\n[\n[\n1\n,\n2\n,\n3\n],\n[\n4\n,\n5\n,\n6\n],\n],\ncolumns\n=\n[\n'a'\n,\n'b'\n,\n'c'\n]\n)\nuri\n=\n\"postgresql://postgres:postgres@localhost/postgres\"\nwith\npg_dbapi\n.\nconnect\n(\nuri\n)\nas\nconn\n:\ndf\n.\nto_sql\n(\n\"pandas_table\"\n,\nconn\n,\nindex\n=\nFalse\n)\n# for round-tripping\nwith\npg_dbapi\n.\nconnect\n(\nuri\n)\nas\nconn\n:\ndf2\n=\npd\n.\nread_sql\n(\n\"pandas_table\"\n,\nconn\n)\nThe Arrow type system offers a wider array of types that can more closely match\nwhat databases like PostgreSQL can offer. To illustrate, note this (non-exhaustive)\nlisting of types available in different databases and pandas backends:\nnumpy/pandas\narrow\npostgres\nsqlite\nint16/Int16\nint16\nSMALLINT\nINTEGER\nint32/Int32\nint32\nINTEGER\nINTEGER\nint64/Int64\nint64\nBIGINT\nINTEGER\nfloat32\nfloat32\nREAL\nREAL\nfloat64\nfloat64\nDOUBLE PRECISION\nREAL\nobject\nstring\nTEXT\nTEXT\nbool\nbool_\nBOOLEAN\ndatetime64[ns]\ntimestamp(us)\nTIMESTAMP\ndatetime64[ns,tz]\ntimestamp(us,tz)\nTIMESTAMPTZ\ndate32\nDATE\nmonth_day_nano_interval\nINTERVAL\nbinary\nBINARY\nBLOB\ndecimal128\nDECIMAL\n[\n1\n]\nlist\nARRAY\n[\n1\n]\nstruct\nCOMPOSITE TYPE\n[\n1\n]\nFootnotes\n[\n1\n]\n(\n1\n,\n2\n,\n3\n)\nNot implemented as of writing, but theoretically possible\nIf you are interested in preserving database types as best as possible\nthroughout the lifecycle of your DataFrame, users are encouraged to\nleverage the\ndtype_backend=\"pyarrow\"\nargument of\nread_sql()\n# for round-tripping\nwith\npg_dbapi\n.\nconnect\n(\nuri\n)\nas\nconn\n:\ndf2\n=\npd\n.\nread_sql\n(\n\"pandas_table\"\n,\nconn\n,\ndtype_backend\n=\n\"pyarrow\"\n)\nThis will prevent your data from being converted to the traditional pandas/NumPy\ntype system, which often converts SQL types in ways that make them impossible to\nround-trip.\nFor a full list of ADBC drivers and their development status, see the\nADBC Driver\nImplementation Status\ndocumentation.\nCreate a pandas Series based on one or more conditions\n#\nThe\nSeries.case_when()\nfunction has been added to create a Series object based on one or more conditions. (\nGH 39154\n)\nIn [1]:\nimport\npandas\nas\npd\nIn [2]:\ndf\n=\npd\n.\nDataFrame\n(\ndict\n(\na\n=\n[\n1\n,\n2\n,\n3\n],\nb\n=\n[\n4\n,\n5\n,\n6\n]))\nIn [3]:\ndefault\n=\npd\n.\nSeries\n(\n'default'\n,\nindex\n=\ndf\n.\nindex\n)\nIn [4]:\ndefault\n.\ncase_when\n(\n...:\ncaselist\n=\n[\n...:\n(\ndf\n.\na\n==\n1\n,\n'first'\n),\n# condition, replacement\n...:\n(\ndf\n.\na\n.\ngt\n(\n1\n)\n&\ndf\n.\nb\n.\neq\n(\n5\n),\n'second'\n),\n# condition, replacement\n...:\n],\n...:\n)\n...:\nOut[4]:\n0      first\n1     second\n2    default\ndtype: object\nto_numpy\nfor NumPy nullable and Arrow types converts to suitable NumPy dtype\n#\nto_numpy\nfor NumPy nullable and Arrow types will now convert to a\nsuitable NumPy dtype instead of\nobject\ndtype for nullable and PyArrow backed extension dtypes.\nOld behavior:\nIn [1]:\nser\n=\npd\n.\nSeries\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\n\"Int64\"\n)\nIn [2]:\nser\n.\nto_numpy\n()\nOut[2]:\narray([1, 2, 3], dtype=object)\nNew behavior:\nIn [5]:\nser\n=\npd\n.\nSeries\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\n\"Int64\"\n)\nIn [6]:\nser\n.\nto_numpy\n()\nOut[6]:\narray([1, 2, 3])\nIn [7]:\nser\n=\npd\n.\nSeries\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\n\"timestamp[ns][pyarrow]\"\n)\nIn [8]:\nser\n.\nto_numpy\n()\nOut[8]:\narray(['1970-01-01T00:00:00.000000001', '1970-01-01T00:00:00.000000002',\n'1970-01-01T00:00:00.000000003'], dtype='datetime64[ns]')\nThe default NumPy dtype (without any arguments) is determined as follows:\nfloat dtypes are cast to NumPy floats\ninteger dtypes without missing values are cast to NumPy integer dtypes\ninteger dtypes with missing values are cast to NumPy float dtypes and\nNaN\nis used as missing value indicator\nboolean dtypes without missing values are cast to NumPy bool dtype\nboolean dtypes with missing values keep object dtype\ndatetime and timedelta types are cast to Numpy datetime64 and timedelta64 types respectively and\nNaT\nis used as missing value indicator\nSeries.struct accessor for PyArrow structured data\n#\nThe\nSeries.struct\naccessor provides attributes and methods for processing\ndata with\nstruct[pyarrow]\ndtype Series. For example,\nSeries.struct.explode()\nconverts PyArrow structured data to a pandas\nDataFrame. (\nGH 54938\n)\nIn [9]:\nimport\npyarrow\nas\npa\nIn [10]:\nseries\n=\npd\n.\nSeries\n(\n....:\n[\n....:\n{\n\"project\"\n:\n\"pandas\"\n,\n\"version\"\n:\n\"2.2.0\"\n},\n....:\n{\n\"project\"\n:\n\"numpy\"\n,\n\"version\"\n:\n\"1.25.2\"\n},\n....:\n{\n\"project\"\n:\n\"pyarrow\"\n,\n\"version\"\n:\n\"13.0.0\"\n},\n....:\n],\n....:\ndtype\n=\npd\n.\nArrowDtype\n(\n....:\npa\n.\nstruct\n([\n....:\n(\n\"project\"\n,\npa\n.\nstring\n()),\n....:\n(\n\"version\"\n,\npa\n.\nstring\n()),\n....:\n])\n....:\n),\n....:\n)\n....:\nIn [11]:\nseries\n.\nstruct\n.\nexplode\n()\nOut[11]:\nproject version\n0   pandas   2.2.0\n1    numpy  1.25.2\n2  pyarrow  13.0.0\nUse\nSeries.struct.field()\nto index into a (possible nested)\nstruct field.\nIn [12]:\nseries\n.\nstruct\n.\nfield\n(\n\"project\"\n)\nOut[12]:\n0     pandas\n1      numpy\n2    pyarrow\nName: project, dtype: string[pyarrow]\nSeries.list accessor for PyArrow list data\n#\nThe\nSeries.list\naccessor provides attributes and methods for processing\ndata with\nlist[pyarrow]\ndtype Series. For example,\nSeries.list.__getitem__()\nallows indexing pyarrow lists in\na Series. (\nGH 55323\n)\nIn [13]:\nimport\npyarrow\nas\npa\nIn [14]:\nseries\n=\npd\n.\nSeries\n(\n....:\n[\n....:\n[\n1\n,\n2\n,\n3\n],\n....:\n[\n4\n,\n5\n],\n....:\n[\n6\n],\n....:\n],\n....:\ndtype\n=\npd\n.\nArrowDtype\n(\n....:\npa\n.\nlist_\n(\npa\n.\nint64\n())\n....:\n),\n....:\n)\n....:\nIn [15]:\nseries\n.\nlist\n[\n0\n]\nOut[15]:\n0    1\n1    4\n2    6\ndtype: int64[pyarrow]\nCalamine engine for\nread_excel()\n#\nThe\ncalamine\nengine was added to\nread_excel()\n.\nIt uses\npython-calamine\n, which provides Python bindings for the Rust library\ncalamine\n.\nThis engine supports Excel files (\n.xlsx\n,\n.xlsm\n,\n.xls\n,\n.xlsb\n) and OpenDocument spreadsheets (\n.ods\n) (\nGH 50395\n).\nThere are two advantages of this engine:\nCalamine is often faster than other engines, some benchmarks show results up to 5x faster than âopenpyxlâ, 20x - âodfâ, 4x - âpyxlsbâ, and 1.5x - âxlrdâ.\nBut, âopenpyxlâ and âpyxlsbâ are faster in reading a few rows from large files because of lazy iteration over rows.\nCalamine supports the recognition of datetime in\n.xlsb\nfiles, unlike âpyxlsbâ which is the only other engine in pandas that can read\n.xlsb\nfiles.\npd\n.\nread_excel\n(\n\"path_to_file.xlsb\"\n,\nengine\n=\n\"calamine\"\n)\nFor more, see\nCalamine (Excel and ODS files)\nin the user guide on IO tools.\nOther enhancements\n#\nto_sql()\nwith method parameter set to\nmulti\nworks with Oracle on the backend\nSeries.attrs\n/\nDataFrame.attrs\nnow uses a deepcopy for propagating\nattrs\n(\nGH 54134\n).\nget_dummies()\nnow returning  extension dtypes\nboolean\nor\nbool[pyarrow]\nthat are compatible with the input dtype (\nGH 56273\n)\nread_csv()\nnow supports\non_bad_lines\nparameter with\nengine=\"pyarrow\"\n(\nGH 54480\n)\nread_sas()\nreturns\ndatetime64\ndtypes with resolutions better matching those stored natively in SAS, and avoids returning object-dtype in cases that cannot be stored with\ndatetime64[ns]\ndtype (\nGH 56127\n)\nread_spss()\nnow returns a\nDataFrame\nthat stores the metadata in\nDataFrame.attrs\n(\nGH 54264\n)\ntseries.api.guess_datetime_format()\nis now part of the public API (\nGH 54727\n)\nDataFrame.apply()\nnow allows the usage of numba (via\nengine=\"numba\"\n) to JIT compile the passed function, allowing for potential speedups (\nGH 54666\n)\nExtensionArray._explode()\ninterface method added to allow extension type implementations of the\nexplode\nmethod (\nGH 54833\n)\nExtensionArray.duplicated()\nadded to allow extension type implementations of the\nduplicated\nmethod (\nGH 55255\n)\nSeries.ffill()\n,\nSeries.bfill()\n,\nDataFrame.ffill()\n, and\nDataFrame.bfill()\nhave gained the argument\nlimit_area\n; 3rd party\nExtensionArray\nauthors need to add this argument to the method\n_pad_or_backfill\n(\nGH 56492\n)\nAllow passing\nread_only\n,\ndata_only\nand\nkeep_links\narguments to openpyxl using\nengine_kwargs\nof\nread_excel()\n(\nGH 55027\n)\nImplement\nSeries.interpolate()\nand\nDataFrame.interpolate()\nfor\nArrowDtype\nand masked dtypes (\nGH 56267\n)\nImplement masked algorithms for\nSeries.value_counts()\n(\nGH 54984\n)\nImplemented\nSeries.dt()\nmethods and attributes for\nArrowDtype\nwith\npyarrow.duration\ntype (\nGH 52284\n)\nImplemented\nSeries.str.extract()\nfor\nArrowDtype\n(\nGH 56268\n)\nImproved error message that appears in\nDatetimeIndex.to_period()\nwith frequencies which are not supported as period frequencies, such as\n\"BMS\"\n(\nGH 56243\n)\nImproved error message when constructing\nPeriod\nwith invalid offsets such as\n\"QS\"\n(\nGH 55785\n)\nThe dtypes\nstring[pyarrow]\nand\nstring[pyarrow_numpy]\nnow both utilize the\nlarge_string\ntype from PyArrow to avoid overflow for long columns (\nGH 56259\n)\nNotable bug fixes\n#\nThese are bug fixes that might have notable behavior changes.\nmerge()\nand\nDataFrame.join()\nnow consistently follow documented sort behavior\n#\nIn previous versions of pandas,\nmerge()\nand\nDataFrame.join()\ndid not\nalways return a result that followed the documented sort behavior. pandas now\nfollows the documented sort behavior in merge and join operations (\nGH 54611\n,\nGH 56426\n,\nGH 56443\n).\nAs documented,\nsort=True\nsorts the join keys lexicographically in the resulting\nDataFrame\n. With\nsort=False\n, the order of the join keys depends on the\njoin type (\nhow\nkeyword):\nhow=\"left\"\n: preserve the order of the left keys\nhow=\"right\"\n: preserve the order of the right keys\nhow=\"inner\"\n: preserve the order of the left keys\nhow=\"outer\"\n: sort keys lexicographically\nOne example with changing behavior is inner joins with non-unique left join keys\nand\nsort=False\n:\nIn [16]:\nleft\n=\npd\n.\nDataFrame\n({\n\"a\"\n:\n[\n1\n,\n2\n,\n1\n]})\nIn [17]:\nright\n=\npd\n.\nDataFrame\n({\n\"a\"\n:\n[\n1\n,\n2\n]})\nIn [18]:\nresult\n=\npd\n.\nmerge\n(\nleft\n,\nright\n,\nhow\n=\n\"inner\"\n,\non\n=\n\"a\"\n,\nsort\n=\nFalse\n)\nOld Behavior\nIn [5]:\nresult\nOut[5]:\na\n0  1\n1  1\n2  2\nNew Behavior\nIn [19]:\nresult\nOut[19]:\na\n0  1\n1  2\n2  1\nmerge()\nand\nDataFrame.join()\nno longer reorder levels when levels differ\n#\nIn previous versions of pandas,\nmerge()\nand\nDataFrame.join()\nwould reorder\nindex levels when joining on two indexes with different levels (\nGH 34133\n).\nIn [20]:\nleft\n=\npd\n.\nDataFrame\n({\n\"left\"\n:\n1\n},\nindex\n=\npd\n.\nMultiIndex\n.\nfrom_tuples\n([(\n\"x\"\n,\n1\n),\n(\n\"x\"\n,\n2\n)],\nnames\n=\n[\n\"A\"\n,\n\"B\"\n]))\nIn [21]:\nright\n=\npd\n.\nDataFrame\n({\n\"right\"\n:\n2\n},\nindex\n=\npd\n.\nMultiIndex\n.\nfrom_tuples\n([(\n1\n,\n1\n),\n(\n2\n,\n2\n)],\nnames\n=\n[\n\"B\"\n,\n\"C\"\n]))\nIn [22]:\nleft\nOut[22]:\nleft\nA B\nx 1     1\n2     1\nIn [23]:\nright\nOut[23]:\nright\nB C\n1 1      2\n2 2      2\nIn [24]:\nresult\n=\nleft\n.\njoin\n(\nright\n)\nOld Behavior\nIn [5]:\nresult\nOut[5]:\nleft  right\nB A C\n1 x 1     1      2\n2 x 2     1      2\nNew Behavior\nIn [25]:\nresult\nOut[25]:\nleft  right\nA B C\nx 1 1     1      2\n2 2     1      2\nIncreased minimum versions for dependencies\n#\nFor\noptional dependencies\nthe general recommendation is to use the latest version.\nOptional dependencies below the lowest tested version may still work but are not considered supported.\nThe following table lists the optional dependencies that have had their minimum tested version increased.\nPackage\nNew Minimum Version\nbeautifulsoup4\n4.11.2\nblosc\n1.21.3\nbottleneck\n1.3.6\nfastparquet\n2022.12.0\nfsspec\n2022.11.0\ngcsfs\n2022.11.0\nlxml\n4.9.2\nmatplotlib\n3.6.3\nnumba\n0.56.4\nnumexpr\n2.8.4\nqtpy\n2.3.0\nopenpyxl\n3.1.0\npsycopg2\n2.9.6\npyreadstat\n1.2.0\npytables\n3.8.0\npyxlsb\n1.0.10\ns3fs\n2022.11.0\nscipy\n1.10.0\nsqlalchemy\n2.0.0\ntabulate\n0.9.0\nxarray\n2022.12.0\nxlsxwriter\n3.0.5\nzstandard\n0.19.0\npyqt5\n5.15.8\ntzdata\n2022.7\nSee\nDependencies\nand\nOptional dependencies\nfor more.\nOther API changes\n#\nThe hash values of nullable extension dtypes changed to improve the performance of the hashing operation (\nGH 56507\n)\ncheck_exact\nnow only takes effect for floating-point dtypes in\ntesting.assert_frame_equal()\nand\ntesting.assert_series_equal()\n. In particular, integer dtypes are always checked exactly (\nGH 55882\n)\nDeprecations\n#\nChained assignment\n#\nIn preparation of larger upcoming changes to the copy / view behaviour in pandas 3.0\n(\nCopy-on-Write (CoW)\n, PDEP-7), we started deprecating\nchained assignment\n.\nChained assignment occurs when you try to update a pandas DataFrame or Series through\ntwo subsequent indexing operations. Depending on the type and order of those operations\nthis currently does or does not work.\nA typical example is as follows:\ndf\n=\npd\n.\nDataFrame\n({\n\"foo\"\n:\n[\n1\n,\n2\n,\n3\n],\n\"bar\"\n:\n[\n4\n,\n5\n,\n6\n]})\n# first selecting rows with a mask, then assigning values to a column\n# -> this has never worked and raises a SettingWithCopyWarning\ndf\n[\ndf\n[\n\"bar\"\n]\n>\n5\n][\n\"foo\"\n]\n=\n100\n# first selecting the column, and then assigning to a subset of that column\n# -> this currently works\ndf\n[\n\"foo\"\n][\ndf\n[\n\"bar\"\n]\n>\n5\n]\n=\n100\nThis second example of chained assignment currently works to update the original\ndf\n.\nThis will no longer work in pandas 3.0, and therefore we started deprecating this:\n>>>\ndf\n[\n\"foo\"\n][\ndf\n[\n\"bar\"\n]\n>\n5\n]\n=\n100\nFutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\ndf[\"col\"][row_indexer] = value\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nYou can fix this warning and ensure your code is ready for pandas 3.0 by removing\nthe usage of chained assignment. Typically, this can be done by doing the assignment\nin a single step using for example\n.loc\n. For the example above, we can do:\ndf\n.\nloc\n[\ndf\n[\n\"bar\"\n]\n>\n5\n,\n\"foo\"\n]\n=\n100\nThe same deprecation applies to inplace methods that are done in a chained manner, such as:\n>>>\ndf\n[\n\"foo\"\n]\n.\nfillna\n(\n0\n,\ninplace\n=\nTrue\n)\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\nWhen the goal is to update the column in the DataFrame\ndf\n, the alternative here is\nto call the method on\ndf\nitself, such as\ndf.fillna({\"foo\":\n0},\ninplace=True)\n.\nSee more details in the\nmigration guide\n.\nDeprecate aliases\nM\n,\nQ\n,\nY\n, etc. in favour of\nME\n,\nQE\n,\nYE\n, etc. for offsets\n#\nDeprecated the following frequency aliases (\nGH 9586\n):\noffsets\ndeprecated aliases\nnew aliases\nMonthEnd\nM\nME\nBusinessMonthEnd\nBM\nBME\nSemiMonthEnd\nSM\nSME\nCustomBusinessMonthEnd\nCBM\nCBME\nQuarterEnd\nQ\nQE\nBQuarterEnd\nBQ\nBQE\nYearEnd\nY\nYE\nBYearEnd\nBY\nBYE\nFor example:\nPrevious behavior\n:\nIn [8]:\npd\n.\ndate_range\n(\n'2020-01-01'\n,\nperiods\n=\n3\n,\nfreq\n=\n'Q-NOV'\n)\nOut[8]:\nDatetimeIndex(['2020-02-29', '2020-05-31', '2020-08-31'],\ndtype='datetime64[ns]', freq='Q-NOV')\nFuture behavior\n:\nIn [26]:\npd\n.\ndate_range\n(\n'2020-01-01'\n,\nperiods\n=\n3\n,\nfreq\n=\n'QE-NOV'\n)\nOut[26]:\nDatetimeIndex(['2020-02-29', '2020-05-31', '2020-08-31'], dtype='datetime64[ns]', freq='QE-NOV')\nDeprecated automatic downcasting\n#\nDeprecated the automatic downcasting of object dtype results in a number of\nmethods. These would silently change the dtype in a hard to predict manner since the\nbehavior was value dependent. Additionally, pandas is moving away from silent dtype\nchanges (\nGH 54710\n,\nGH 54261\n).\nThese methods are:\nSeries.replace()\nand\nDataFrame.replace()\nDataFrame.fillna()\n,\nSeries.fillna()\nDataFrame.ffill()\n,\nSeries.ffill()\nDataFrame.bfill()\n,\nSeries.bfill()\nDataFrame.mask()\n,\nSeries.mask()\nDataFrame.where()\n,\nSeries.where()\nDataFrame.clip()\n,\nSeries.clip()\nExplicitly call\nDataFrame.infer_objects()\nto replicate the current behavior in the future.\nresult\n=\nresult\n.\ninfer_objects\n(\ncopy\n=\nFalse\n)\nOr explicitly cast all-round floats to ints using\nastype\n.\nSet the following option to opt into the future behavior:\nIn [9]:\npd\n.\nset_option\n(\n\"future.no_silent_downcasting\"\n,\nTrue\n)\nOther Deprecations\n#\nChanged\nTimedelta.resolution_string()\nto return\nh\n,\nmin\n,\ns\n,\nms\n,\nus\n, and\nns\ninstead of\nH\n,\nT\n,\nS\n,\nL\n,\nU\n, and\nN\n, for compatibility with respective deprecations in frequency aliases (\nGH 52536\n)\nDeprecated\noffsets.Day.delta\n,\noffsets.Hour.delta\n,\noffsets.Minute.delta\n,\noffsets.Second.delta\n,\noffsets.Milli.delta\n,\noffsets.Micro.delta\n,\noffsets.Nano.delta\n, use\npd.Timedelta(obj)\ninstead (\nGH 55498\n)\nDeprecated\npandas.api.types.is_interval()\nand\npandas.api.types.is_period()\n, use\nisinstance(obj,\npd.Interval)\nand\nisinstance(obj,\npd.Period)\ninstead (\nGH 55264\n)\nDeprecated\nread_gbq()\nand\nDataFrame.to_gbq()\n. Use\npandas_gbq.read_gbq\nand\npandas_gbq.to_gbq\ninstead\nhttps://pandas-gbq.readthedocs.io/en/latest/api.html\n(\nGH 55525\n)\nDeprecated\nDataFrameGroupBy.fillna()\nand\nSeriesGroupBy.fillna()\n; use\nDataFrameGroupBy.ffill()\n,\nDataFrameGroupBy.bfill()\nfor forward and backward filling or\nDataFrame.fillna()\nto fill with a single value (or the Series equivalents) (\nGH 55718\n)\nDeprecated\nDateOffset.is_anchored()\n, use\nobj.n\n==\n1\nfor non-Tick subclasses (for Tick this was always False) (\nGH 55388\n)\nDeprecated\nDatetimeArray.__init__()\nand\nTimedeltaArray.__init__()\n, use\narray()\ninstead (\nGH 55623\n)\nDeprecated\nIndex.format()\n, use\nindex.astype(str)\nor\nindex.map(formatter)\ninstead (\nGH 55413\n)\nDeprecated\nSeries.ravel()\n, the underlying array is already 1D, so ravel is not necessary (\nGH 52511\n)\nDeprecated\nSeries.resample()\nand\nDataFrame.resample()\nwith a\nPeriodIndex\n(and the âconventionâ keyword), convert to\nDatetimeIndex\n(with\n.to_timestamp()\n) before resampling instead (\nGH 53481\n)\nDeprecated\nSeries.view()\n, use\nSeries.astype()\ninstead to change the dtype (\nGH 20251\n)\nDeprecated\noffsets.Tick.is_anchored()\n, use\nFalse\ninstead (\nGH 55388\n)\nDeprecated\ncore.internals\nmembers\nBlock\n,\nExtensionBlock\n, and\nDatetimeTZBlock\n, use public APIs instead (\nGH 55139\n)\nDeprecated\nyear\n,\nmonth\n,\nquarter\n,\nday\n,\nhour\n,\nminute\n, and\nsecond\nkeywords in the\nPeriodIndex\nconstructor, use\nPeriodIndex.from_fields()\ninstead (\nGH 55960\n)\nDeprecated accepting a type as an argument in\nIndex.view()\n, call without any arguments instead (\nGH 55709\n)\nDeprecated allowing non-integer\nperiods\nargument in\ndate_range()\n,\ntimedelta_range()\n,\nperiod_range()\n, and\ninterval_range()\n(\nGH 56036\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_clipboard()\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_csv()\nexcept\npath_or_buf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_dict()\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_excel()\nexcept\nexcel_writer\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_gbq()\nexcept\ndestination_table\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_hdf()\nexcept\npath_or_buf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_html()\nexcept\nbuf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_json()\nexcept\npath_or_buf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_latex()\nexcept\nbuf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_markdown()\nexcept\nbuf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_parquet()\nexcept\npath\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_pickle()\nexcept\npath\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_string()\nexcept\nbuf\n(\nGH 54229\n)\nDeprecated allowing non-keyword arguments in\nDataFrame.to_xml()\nexcept\npath_or_buffer\n(\nGH 54229\n)\nDeprecated allowing passing\nBlockManager\nobjects to\nDataFrame\nor\nSingleBlockManager\nobjects to\nSeries\n(\nGH 52419\n)\nDeprecated behavior of\nIndex.insert()\nwith an object-dtype index silently performing type inference on the result, explicitly call\nresult.infer_objects(copy=False)\nfor the old behavior instead (\nGH 51363\n)\nDeprecated casting non-datetimelike values (mainly strings) in\nSeries.isin()\nand\nIndex.isin()\nwith\ndatetime64\n,\ntimedelta64\n, and\nPeriodDtype\ndtypes (\nGH 53111\n)\nDeprecated dtype inference in\nIndex\n,\nSeries\nand\nDataFrame\nconstructors when giving a pandas input, call\n.infer_objects\non the input to keep the current behavior (\nGH 56012\n)\nDeprecated dtype inference when setting a\nIndex\ninto a\nDataFrame\n, cast explicitly instead (\nGH 56102\n)\nDeprecated including the groups in computations when using\nDataFrameGroupBy.apply()\nand\nDataFrameGroupBy.resample()\n; pass\ninclude_groups=False\nto exclude the groups (\nGH 7155\n)\nDeprecated indexing an\nIndex\nwith a boolean indexer of length zero (\nGH 55820\n)\nDeprecated not passing a tuple to\nDataFrameGroupBy.get_group\nor\nSeriesGroupBy.get_group\nwhen grouping by a length-1 list-like (\nGH 25971\n)\nDeprecated string\nAS\ndenoting frequency in\nYearBegin\nand strings\nAS-DEC\n,\nAS-JAN\n, etc. denoting annual frequencies with various fiscal year starts (\nGH 54275\n)\nDeprecated string\nA\ndenoting frequency in\nYearEnd\nand strings\nA-DEC\n,\nA-JAN\n, etc. denoting annual frequencies with various fiscal year ends (\nGH 54275\n)\nDeprecated string\nBAS\ndenoting frequency in\nBYearBegin\nand strings\nBAS-DEC\n,\nBAS-JAN\n, etc. denoting annual frequencies with various fiscal year starts (\nGH 54275\n)\nDeprecated string\nBA\ndenoting frequency in\nBYearEnd\nand strings\nBA-DEC\n,\nBA-JAN\n, etc. denoting annual frequencies with various fiscal year ends (\nGH 54275\n)\nDeprecated strings\nH\n,\nBH\n, and\nCBH\ndenoting frequencies in\nHour\n,\nBusinessHour\n,\nCustomBusinessHour\n(\nGH 52536\n)\nDeprecated strings\nH\n,\nS\n,\nU\n, and\nN\ndenoting units in\nto_timedelta()\n(\nGH 52536\n)\nDeprecated strings\nH\n,\nT\n,\nS\n,\nL\n,\nU\n, and\nN\ndenoting units in\nTimedelta\n(\nGH 52536\n)\nDeprecated strings\nT\n,\nS\n,\nL\n,\nU\n, and\nN\ndenoting frequencies in\nMinute\n,\nSecond\n,\nMilli\n,\nMicro\n,\nNano\n(\nGH 52536\n)\nDeprecated support for combining parsed datetime columns in\nread_csv()\nalong with the\nkeep_date_col\nkeyword (\nGH 55569\n)\nDeprecated the\nDataFrameGroupBy.grouper\nand\nSeriesGroupBy.grouper\n; these attributes will be removed in a future version of pandas (\nGH 56521\n)\nDeprecated the\nGrouping\nattributes\ngroup_index\n,\nresult_index\n, and\ngroup_arraylike\n; these will be removed in a future version of pandas (\nGH 56148\n)\nDeprecated the\ndelim_whitespace\nkeyword in\nread_csv()\nand\nread_table()\n, use\nsep=\"\\\\s+\"\ninstead (\nGH 55569\n)\nDeprecated the\nerrors=\"ignore\"\noption in\nto_datetime()\n,\nto_timedelta()\n, and\nto_numeric()\n; explicitly catch exceptions instead (\nGH 54467\n)\nDeprecated the\nfastpath\nkeyword in the\nSeries\nconstructor (\nGH 20110\n)\nDeprecated the\nkind\nkeyword in\nSeries.resample()\nand\nDataFrame.resample()\n, explicitly cast the objectâs\nindex\ninstead (\nGH 55895\n)\nDeprecated the\nordinal\nkeyword in\nPeriodIndex\n, use\nPeriodIndex.from_ordinals()\ninstead (\nGH 55960\n)\nDeprecated the\nunit\nkeyword in\nTimedeltaIndex\nconstruction, use\nto_timedelta()\ninstead (\nGH 55499\n)\nDeprecated the\nverbose\nkeyword in\nread_csv()\nand\nread_table()\n(\nGH 55569\n)\nDeprecated the behavior of\nDataFrame.replace()\nand\nSeries.replace()\nwith\nCategoricalDtype\n; in a future version replace will change the values while preserving the categories. To change the categories, use\nser.cat.rename_categories\ninstead (\nGH 55147\n)\nDeprecated the behavior of\nSeries.value_counts()\nand\nIndex.value_counts()\nwith object dtype; in a future version these will not perform dtype inference on the resulting\nIndex\n, do\nresult.index\n=\nresult.index.infer_objects()\nto retain the old behavior (\nGH 56161\n)\nDeprecated the default of\nobserved=False\nin\nDataFrame.pivot_table()\n; will be\nTrue\nin a future version (\nGH 56236\n)\nDeprecated the extension test classes\nBaseNoReduceTests\n,\nBaseBooleanReduceTests\n, and\nBaseNumericReduceTests\n, use\nBaseReduceTests\ninstead (\nGH 54663\n)\nDeprecated the option\nmode.data_manager\nand the\nArrayManager\n; only the\nBlockManager\nwill be available in future versions (\nGH 55043\n)\nDeprecated the previous implementation of\nDataFrame.stack\n; specify\nfuture_stack=True\nto adopt the future version (\nGH 53515\n)\nPerformance improvements\n#\nPerformance improvement in\ntesting.assert_frame_equal()\nand\ntesting.assert_series_equal()\n(\nGH 55949\n,\nGH 55971\n)\nPerformance improvement in\nconcat()\nwith\naxis=1\nand objects with unaligned indexes (\nGH 55084\n)\nPerformance improvement in\nget_dummies()\n(\nGH 56089\n)\nPerformance improvement in\nmerge()\nand\nmerge_ordered()\nwhen joining on sorted ascending keys (\nGH 56115\n)\nPerformance improvement in\nmerge_asof()\nwhen\nby\nis not\nNone\n(\nGH 55580\n,\nGH 55678\n)\nPerformance improvement in\nread_stata()\nfor files with many variables (\nGH 55515\n)\nPerformance improvement in\nDataFrame.groupby()\nwhen aggregating pyarrow timestamp and duration dtypes (\nGH 55031\n)\nPerformance improvement in\nDataFrame.join()\nwhen joining on unordered categorical indexes (\nGH 56345\n)\nPerformance improvement in\nDataFrame.loc()\nand\nSeries.loc()\nwhen indexing with a\nMultiIndex\n(\nGH 56062\n)\nPerformance improvement in\nDataFrame.sort_index()\nand\nSeries.sort_index()\nwhen indexed by a\nMultiIndex\n(\nGH 54835\n)\nPerformance improvement in\nDataFrame.to_dict()\non converting DataFrame to dictionary (\nGH 50990\n)\nPerformance improvement in\nIndex.difference()\n(\nGH 55108\n)\nPerformance improvement in\nIndex.sort_values()\nwhen index is already sorted (\nGH 56128\n)\nPerformance improvement in\nMultiIndex.get_indexer()\nwhen\nmethod\nis not\nNone\n(\nGH 55839\n)\nPerformance improvement in\nSeries.duplicated()\nfor pyarrow dtypes (\nGH 55255\n)\nPerformance improvement in\nSeries.str.get_dummies()\nwhen dtype is\n\"string[pyarrow]\"\nor\n\"string[pyarrow_numpy]\"\n(\nGH 56110\n)\nPerformance improvement in\nSeries.str()\nmethods (\nGH 55736\n)\nPerformance improvement in\nSeries.value_counts()\nand\nSeries.mode()\nfor masked dtypes (\nGH 54984\n,\nGH 55340\n)\nPerformance improvement in\nDataFrameGroupBy.nunique()\nand\nSeriesGroupBy.nunique()\n(\nGH 55972\n)\nPerformance improvement in\nSeriesGroupBy.idxmax()\n,\nSeriesGroupBy.idxmin()\n,\nDataFrameGroupBy.idxmax()\n,\nDataFrameGroupBy.idxmin()\n(\nGH 54234\n)\nPerformance improvement when hashing a nullable extension array (\nGH 56507\n)\nPerformance improvement when indexing into a non-unique index (\nGH 55816\n)\nPerformance improvement when indexing with more than 4 keys (\nGH 54550\n)\nPerformance improvement when localizing time to UTC (\nGH 55241\n)\nBug fixes\n#\nCategorical\n#\nCategorical.isin()\nraising\nInvalidIndexError\nfor categorical containing overlapping\nInterval\nvalues (\nGH 34974\n)\nBug in\nCategoricalDtype.__eq__()\nreturning\nFalse\nfor unordered categorical data with mixed types (\nGH 55468\n)\nBug when casting\npa.dictionary\nto\nCategoricalDtype\nusing a\npa.DictionaryArray\nas categories (\nGH 56672\n)\nDatetimelike\n#\nBug in\nDatetimeIndex\nconstruction when passing both a\ntz\nand either\ndayfirst\nor\nyearfirst\nignoring dayfirst/yearfirst (\nGH 55813\n)\nBug in\nDatetimeIndex\nwhen passing an object-dtype ndarray of float objects and a\ntz\nincorrectly localizing the result (\nGH 55780\n)\nBug in\nSeries.isin()\nwith\nDatetimeTZDtype\ndtype and comparison values that are all\nNaT\nincorrectly returning all-\nFalse\neven if the series contains\nNaT\nentries (\nGH 56427\n)\nBug in\nconcat()\nraising\nAttributeError\nwhen concatenating all-NA DataFrame with\nDatetimeTZDtype\ndtype DataFrame (\nGH 52093\n)\nBug in\ntesting.assert_extension_array_equal()\nthat could use the wrong unit when comparing resolutions (\nGH 55730\n)\nBug in\nto_datetime()\nand\nDatetimeIndex\nwhen passing a list of mixed-string-and-numeric types incorrectly raising (\nGH 55780\n)\nBug in\nto_datetime()\nand\nDatetimeIndex\nwhen passing mixed-type objects with a mix of timezones or mix of timezone-awareness failing to raise\nValueError\n(\nGH 55693\n)\nBug in\nTick.delta()\nwith very large ticks raising\nOverflowError\ninstead of\nOutOfBoundsTimedelta\n(\nGH 55503\n)\nBug in\nDatetimeIndex.shift()\nwith non-nanosecond resolution incorrectly returning with nanosecond resolution (\nGH 56117\n)\nBug in\nDatetimeIndex.union()\nreturning object dtype for tz-aware indexes with the same timezone but different units (\nGH 55238\n)\nBug in\nIndex.is_monotonic_increasing()\nand\nIndex.is_monotonic_decreasing()\nalways caching\nIndex.is_unique()\nas\nTrue\nwhen first value in index is\nNaT\n(\nGH 55755\n)\nBug in\nIndex.view()\nto a datetime64 dtype with non-supported resolution incorrectly raising (\nGH 55710\n)\nBug in\nSeries.dt.round()\nwith non-nanosecond resolution and\nNaT\nentries incorrectly raising\nOverflowError\n(\nGH 56158\n)\nBug in\nSeries.fillna()\nwith non-nanosecond resolution dtypes and higher-resolution vector values returning incorrect (internally-corrupted) results (\nGH 56410\n)\nBug in\nTimestamp.unit()\nbeing inferred incorrectly from an ISO8601 format string with minute or hour resolution and a timezone offset (\nGH 56208\n)\nBug in\n.astype\nconverting from a higher-resolution\ndatetime64\ndtype to a lower-resolution\ndatetime64\ndtype (e.g.\ndatetime64[us]->datetime64[ms]\n) silently overflowing with values near the lower implementation bound (\nGH 55979\n)\nBug in adding or subtracting a\nWeek\noffset to a\ndatetime64\nSeries\n,\nIndex\n, or\nDataFrame\ncolumn with non-nanosecond resolution returning incorrect results (\nGH 55583\n)\nBug in addition or subtraction of\nBusinessDay\noffset with\noffset\nattribute to non-nanosecond\nIndex\n,\nSeries\n, or\nDataFrame\ncolumn giving incorrect results (\nGH 55608\n)\nBug in addition or subtraction of\nDateOffset\nobjects with microsecond components to\ndatetime64\nIndex\n,\nSeries\n, or\nDataFrame\ncolumns with non-nanosecond resolution (\nGH 55595\n)\nBug in addition or subtraction of very large\nTick\nobjects with\nTimestamp\nor\nTimedelta\nobjects raising\nOverflowError\ninstead of\nOutOfBoundsTimedelta\n(\nGH 55503\n)\nBug in creating a\nIndex\n,\nSeries\n, or\nDataFrame\nwith a non-nanosecond\nDatetimeTZDtype\nand inputs that would be out of bounds with nanosecond resolution incorrectly raising\nOutOfBoundsDatetime\n(\nGH 54620\n)\nBug in creating a\nIndex\n,\nSeries\n, or\nDataFrame\nwith a non-nanosecond\ndatetime64\n(or\nDatetimeTZDtype\n) from mixed-numeric inputs treating those as nanoseconds instead of as multiples of the dtypeâs unit (which would happen with non-mixed numeric inputs) (\nGH 56004\n)\nBug in creating a\nIndex\n,\nSeries\n, or\nDataFrame\nwith a non-nanosecond\ndatetime64\ndtype and inputs that would be out of bounds for a\ndatetime64[ns]\nincorrectly raising\nOutOfBoundsDatetime\n(\nGH 55756\n)\nBug in parsing datetime strings with nanosecond resolution with non-ISO8601 formats incorrectly truncating sub-microsecond components (\nGH 56051\n)\nBug in parsing datetime strings with sub-second resolution and trailing zeros incorrectly inferring second or millisecond resolution (\nGH 55737\n)\nBug in the results of\nto_datetime()\nwith an floating-dtype argument with\nunit\nnot matching the pointwise results of\nTimestamp\n(\nGH 56037\n)\nFixed regression where\nconcat()\nwould raise an error when concatenating\ndatetime64\ncolumns with differing resolutions (\nGH 53641\n)\nTimedelta\n#\nBug in\nTimedelta\nconstruction raising\nOverflowError\ninstead of\nOutOfBoundsTimedelta\n(\nGH 55503\n)\nBug in rendering (\n__repr__\n) of\nTimedeltaIndex\nand\nSeries\nwith timedelta64 values with non-nanosecond resolution entries that are all multiples of 24 hours failing to use the compact representation used in the nanosecond cases (\nGH 55405\n)\nTimezones\n#\nBug in\nAbstractHolidayCalendar\nwhere timezone data was not propagated when computing holiday observances (\nGH 54580\n)\nBug in\nTimestamp\nconstruction with an ambiguous value and a\npytz\ntimezone failing to raise\npytz.AmbiguousTimeError\n(\nGH 55657\n)\nBug in\nTimestamp.tz_localize()\nwith\nnonexistent=\"shift_forward\naround UTC+0 during DST (\nGH 51501\n)\nNumeric\n#\nBug in\nread_csv()\nwith\nengine=\"pyarrow\"\ncausing rounding errors for large integers (\nGH 52505\n)\nBug in\nSeries.__floordiv__()\nand\nSeries.__truediv__()\nfor\nArrowDtype\nwith integral dtypes raising for large divisors (\nGH 56706\n)\nBug in\nSeries.__floordiv__()\nfor\nArrowDtype\nwith integral dtypes raising for large values (\nGH 56645\n)\nBug in\nSeries.pow()\nnot filling missing values correctly (\nGH 55512\n)\nBug in\nSeries.replace()\nand\nDataFrame.replace()\nmatching float\n0.0\nwith\nFalse\nand vice versa (\nGH 55398\n)\nBug in\nSeries.round()\nraising for nullable boolean dtype (\nGH 55936\n)\nConversion\n#\nBug in\nDataFrame.astype()\nwhen called with\nstr\non unpickled array - the array might change in-place (\nGH 54654\n)\nBug in\nDataFrame.astype()\nwhere\nerrors=\"ignore\"\nhad no effect for extension types (\nGH 54654\n)\nBug in\nSeries.convert_dtypes()\nnot converting all NA column to\nnull[pyarrow]\n(\nGH 55346\n)\nBug in :meth:\nDataFrame.loc\nwas not throwing âincompatible dtype warningâ (see\nPDEP6\n) when assigning a\nSeries\nwith a different dtype using a full column setter (e.g.\ndf.loc[:,\n'a']\n=\nincompatible_value\n) (\nGH 39584\n)\nStrings\n#\nBug in\npandas.api.types.is_string_dtype()\nwhile checking object array with no elements is of the string dtype (\nGH 54661\n)\nBug in\nDataFrame.apply()\nfailing when\nengine=\"numba\"\nand columns or index have\nStringDtype\n(\nGH 56189\n)\nBug in\nDataFrame.reindex()\nnot matching\nIndex\nwith\nstring[pyarrow_numpy]\ndtype (\nGH 56106\n)\nBug in\nIndex.str.cat()\nalways casting result to object dtype (\nGH 56157\n)\nBug in\nSeries.__mul__()\nfor\nArrowDtype\nwith\npyarrow.string\ndtype and\nstring[pyarrow]\nfor the pyarrow backend (\nGH 51970\n)\nBug in\nSeries.str.find()\nwhen\nstart\n<\n0\nfor\nArrowDtype\nwith\npyarrow.string\n(\nGH 56411\n)\nBug in\nSeries.str.fullmatch()\nwhen\ndtype=pandas.ArrowDtype(pyarrow.string()))\nallows partial matches when regex ends in literal //$ (\nGH 56652\n)\nBug in\nSeries.str.replace()\nwhen\nn\n<\n0\nfor\nArrowDtype\nwith\npyarrow.string\n(\nGH 56404\n)\nBug in\nSeries.str.startswith()\nand\nSeries.str.endswith()\nwith arguments of type\ntuple[str,\n...]\nfor\nArrowDtype\nwith\npyarrow.string\ndtype (\nGH 56579\n)\nBug in\nSeries.str.startswith()\nand\nSeries.str.endswith()\nwith arguments of type\ntuple[str,\n...]\nfor\nstring[pyarrow]\n(\nGH 54942\n)\nBug in comparison operations for\ndtype=\"string[pyarrow_numpy]\"\nraising if dtypes canât be compared (\nGH 56008\n)\nInterval\n#\nBug in\nInterval\n__repr__\nnot displaying UTC offsets for\nTimestamp\nbounds. Additionally the hour, minute and second components will now be shown (\nGH 55015\n)\nBug in\nIntervalIndex.factorize()\nand\nSeries.factorize()\nwith\nIntervalDtype\nwith datetime64 or timedelta64 intervals not preserving non-nanosecond units (\nGH 56099\n)\nBug in\nIntervalIndex.from_arrays()\nwhen passed\ndatetime64\nor\ntimedelta64\narrays with mismatched resolutions constructing an invalid\nIntervalArray\nobject (\nGH 55714\n)\nBug in\nIntervalIndex.from_tuples()\nraising if subtype is a nullable extension dtype (\nGH 56765\n)\nBug in\nIntervalIndex.get_indexer()\nwith datetime or timedelta intervals incorrectly matching on integer targets (\nGH 47772\n)\nBug in\nIntervalIndex.get_indexer()\nwith timezone-aware datetime intervals incorrectly matching on a sequence of timezone-naive targets (\nGH 47772\n)\nBug in setting values on a\nSeries\nwith an\nIntervalIndex\nusing a slice incorrectly raising (\nGH 54722\n)\nIndexing\n#\nBug in\nDataFrame.loc()\nmutating a boolean indexer when\nDataFrame\nhas a\nMultiIndex\n(\nGH 56635\n)\nBug in\nDataFrame.loc()\nwhen setting\nSeries\nwith extension dtype into NumPy dtype (\nGH 55604\n)\nBug in\nIndex.difference()\nnot returning a unique set of values when\nother\nis empty or\nother\nis considered non-comparable (\nGH 55113\n)\nBug in setting\nCategorical\nvalues into a\nDataFrame\nwith numpy dtypes raising\nRecursionError\n(\nGH 52927\n)\nFixed bug when creating new column with missing values when setting a single string value (\nGH 56204\n)\nMissing\n#\nBug in\nDataFrame.update()\nwasnât updating in-place for tz-aware datetime64 dtypes (\nGH 56227\n)\nMultiIndex\n#\nBug in\nMultiIndex.get_indexer()\nnot raising\nValueError\nwhen\nmethod\nprovided and index is non-monotonic (\nGH 53452\n)\nI/O\n#\nBug in\nread_csv()\nwhere\nengine=\"python\"\ndid not respect\nchunksize\narg when\nskiprows\nwas specified (\nGH 56323\n)\nBug in\nread_csv()\nwhere\nengine=\"python\"\nwas causing a\nTypeError\nwhen a callable\nskiprows\nand a chunk size was specified (\nGH 55677\n)\nBug in\nread_csv()\nwhere\non_bad_lines=\"warn\"\nwould write to\nstderr\ninstead of raising a Python warning; this now yields a\nerrors.ParserWarning\n(\nGH 54296\n)\nBug in\nread_csv()\nwith\nengine=\"pyarrow\"\nwhere\nquotechar\nwas ignored (\nGH 52266\n)\nBug in\nread_csv()\nwith\nengine=\"pyarrow\"\nwhere\nusecols\nwasnât working with a CSV with no headers (\nGH 54459\n)\nBug in\nread_excel()\n, with\nengine=\"xlrd\"\n(\nxls\nfiles) erroring when the file contains\nNaN\nor\nInf\n(\nGH 54564\n)\nBug in\nread_json()\nnot handling dtype conversion properly if\ninfer_string\nis set (\nGH 56195\n)\nBug in\nDataFrame.to_excel()\n, with\nOdsWriter\n(\nods\nfiles) writing Boolean/string value (\nGH 54994\n)\nBug in\nDataFrame.to_hdf()\nand\nread_hdf()\nwith\ndatetime64\ndtypes with non-nanosecond resolution failing to round-trip correctly (\nGH 55622\n)\nBug in\nDataFrame.to_stata()\nraising for extension dtypes (\nGH 54671\n)\nBug in\nread_excel()\nwith\nengine=\"odf\"\n(\nods\nfiles) when a string cell contains an annotation (\nGH 55200\n)\nBug in\nread_excel()\nwith an ODS file without cached formatted cell for float values (\nGH 55219\n)\nBug where\nDataFrame.to_json()\nwould raise an\nOverflowError\ninstead of a\nTypeError\nwith unsupported NumPy types (\nGH 55403\n)\nPeriod\n#\nBug in\nPeriodIndex\nconstruction when more than one of\ndata\n,\nordinal\nand\n**fields\nare passed failing to raise\nValueError\n(\nGH 55961\n)\nBug in\nPeriod\naddition silently wrapping around instead of raising\nOverflowError\n(\nGH 55503\n)\nBug in casting from\nPeriodDtype\nwith\nastype\nto\ndatetime64\nor\nDatetimeTZDtype\nwith non-nanosecond unit incorrectly returning with nanosecond unit (\nGH 55958\n)\nPlotting\n#\nBug in\nDataFrame.plot.box()\nwith\nvert=False\nand a Matplotlib\nAxes\ncreated with\nsharey=True\n(\nGH 54941\n)\nBug in\nDataFrame.plot.scatter()\ndiscarding string columns (\nGH 56142\n)\nBug in\nSeries.plot()\nwhen reusing an\nax\nobject failing to raise when a\nhow\nkeyword is passed (\nGH 55953\n)\nGroupby/resample/rolling\n#\nBug in\nDataFrameGroupBy.idxmin()\n,\nDataFrameGroupBy.idxmax()\n,\nSeriesGroupBy.idxmin()\n, and\nSeriesGroupBy.idxmax()\nwould not retain\nCategorical\ndtype when the index was a\nCategoricalIndex\nthat contained NA values (\nGH 54234\n)\nBug in\nDataFrameGroupBy.transform()\nand\nSeriesGroupBy.transform()\nwhen\nobserved=False\nand\nf=\"idxmin\"\nor\nf=\"idxmax\"\nwould incorrectly raise on unobserved categories (\nGH 54234\n)\nBug in\nDataFrameGroupBy.value_counts()\nand\nSeriesGroupBy.value_counts()\ncould result in incorrect sorting if the columns of the DataFrame or name of the Series are integers (\nGH 55951\n)\nBug in\nDataFrameGroupBy.value_counts()\nand\nSeriesGroupBy.value_counts()\nwould not respect\nsort=False\nin\nDataFrame.groupby()\nand\nSeries.groupby()\n(\nGH 55951\n)\nBug in\nDataFrameGroupBy.value_counts()\nand\nSeriesGroupBy.value_counts()\nwould sort by proportions rather than frequencies when\nsort=True\nand\nnormalize=True\n(\nGH 55951\n)\nBug in\nDataFrame.asfreq()\nand\nSeries.asfreq()\nwith a\nDatetimeIndex\nwith non-nanosecond resolution incorrectly converting to nanosecond resolution (\nGH 55958\n)\nBug in\nDataFrame.ewm()\nwhen passed\ntimes\nwith non-nanosecond\ndatetime64\nor\nDatetimeTZDtype\ndtype (\nGH 56262\n)\nBug in\nDataFrame.groupby()\nand\nSeries.groupby()\nwhere grouping by a combination of\nDecimal\nand NA values would fail when\nsort=True\n(\nGH 54847\n)\nBug in\nDataFrame.groupby()\nfor DataFrame subclasses when selecting a subset of columns to apply the function to (\nGH 56761\n)\nBug in\nDataFrame.resample()\nnot respecting\nclosed\nand\nlabel\narguments for\nBusinessDay\n(\nGH 55282\n)\nBug in\nDataFrame.resample()\nwhen resampling on a\nArrowDtype\nof\npyarrow.timestamp\nor\npyarrow.duration\ntype (\nGH 55989\n)\nBug in\nDataFrame.resample()\nwhere bin edges were not correct for\nBusinessDay\n(\nGH 55281\n)\nBug in\nDataFrame.resample()\nwhere bin edges were not correct for\nMonthBegin\n(\nGH 55271\n)\nBug in\nDataFrame.rolling()\nand\nSeries.rolling()\nwhere duplicate datetimelike indexes are treated as consecutive rather than equal with\nclosed='left'\nand\nclosed='neither'\n(\nGH 20712\n)\nBug in\nDataFrame.rolling()\nand\nSeries.rolling()\nwhere either the\nindex\nor\non\ncolumn was\nArrowDtype\nwith\npyarrow.timestamp\ntype (\nGH 55849\n)\nReshaping\n#\nBug in\nconcat()\nignoring\nsort\nparameter when passed\nDatetimeIndex\nindexes (\nGH 54769\n)\nBug in\nconcat()\nrenaming\nSeries\nwhen\nignore_index=False\n(\nGH 15047\n)\nBug in\nmerge_asof()\nraising\nTypeError\nwhen\nby\ndtype is not\nobject\n,\nint64\n, or\nuint64\n(\nGH 22794\n)\nBug in\nmerge_asof()\nraising incorrect error for string dtype (\nGH 56444\n)\nBug in\nmerge_asof()\nwhen using a\nTimedelta\ntolerance on a\nArrowDtype\ncolumn (\nGH 56486\n)\nBug in\nmerge()\nnot raising when merging datetime columns with timedelta columns (\nGH 56455\n)\nBug in\nmerge()\nnot raising when merging string columns with numeric columns (\nGH 56441\n)\nBug in\nmerge()\nnot sorting for new string dtype (\nGH 56442\n)\nBug in\nmerge()\nreturning columns in incorrect order when left and/or right is empty (\nGH 51929\n)\nBug in\nDataFrame.melt()\nwhere an exception was raised if\nvar_name\nwas not a string (\nGH 55948\n)\nBug in\nDataFrame.melt()\nwhere it would not preserve the datetime (\nGH 55254\n)\nBug in\nDataFrame.pivot_table()\nwhere the row margin is incorrect when the columns have numeric names (\nGH 26568\n)\nBug in\nDataFrame.pivot()\nwith numeric columns and extension dtype for data (\nGH 56528\n)\nBug in\nDataFrame.stack()\nwith\nfuture_stack=True\nwould not preserve NA values in the index (\nGH 56573\n)\nSparse\n#\nBug in\narrays.SparseArray.take()\nwhen using a different fill value than the arrayâs fill value (\nGH 55181\n)\nOther\n#\nDataFrame.__dataframe__()\ndid not support pyarrow large strings (\nGH 56702\n)\nBug in\nDataFrame.describe()\nwhen formatting percentiles in the resulting percentile 99.999% is rounded to 100% (\nGH 55765\n)\nBug in\napi.interchange.from_dataframe()\nwhere it raised\nNotImplementedError\nwhen handling empty string columns (\nGH 56703\n)\nBug in\ncut()\nand\nqcut()\nwith\ndatetime64\ndtype values with non-nanosecond units incorrectly returning nanosecond-unit bins (\nGH 56101\n)\nBug in\ncut()\nincorrectly allowing cutting of timezone-aware datetimes with timezone-naive bins (\nGH 54964\n)\nBug in\ninfer_freq()\nand\nDatetimeIndex.inferred_freq()\nwith weekly frequencies and non-nanosecond resolutions (\nGH 55609\n)\nBug in\nDataFrame.apply()\nwhere passing\nraw=True\nignored\nargs\npassed to the applied function (\nGH 55009\n)\nBug in\nDataFrame.from_dict()\nwhich would always sort the rows of the created\nDataFrame\n.  (\nGH 55683\n)\nBug in\nDataFrame.sort_index()\nwhen passing\naxis=\"columns\"\nand\nignore_index=True\nraising a\nValueError\n(\nGH 56478\n)\nBug in rendering\ninf\nvalues inside a\nDataFrame\nwith the\nuse_inf_as_na\noption enabled (\nGH 55483\n)\nBug in rendering a\nSeries\nwith a\nMultiIndex\nwhen one of the index levelâs names is 0 not having that name displayed (\nGH 55415\n)\nBug in the error message when assigning an empty\nDataFrame\nto a column (\nGH 55956\n)\nBug when time-like strings were being cast to\nArrowDtype\nwith\npyarrow.time64\ntype (\nGH 56463\n)\nFixed a spurious deprecation warning from\nnumba\n>= 0.58.0 when passing a numpy ufunc in\ncore.window.Rolling.apply\nwith\nengine=\"numba\"\n(\nGH 55247\n)\nContributors\n#\nA total of 162 people contributed patches to this release.  People with a\nâ+â by their names contributed a patch for the first time.\nAG\nAaron Rahman +\nAbdullah Ihsan Secer +\nAbhijit Deo +\nAdrian DâAlessandro\nAhmad Mustafa Anis +\nAmanda Bizzinotto\nAmith KK +\nAniket Patil +\nAntonio Fonseca +\nArtur Barseghyan\nBen Greiner\nBill Blum +\nBoyd Kane\nDamian Kula\nDan King +\nDaniel Weindl +\nDaniele Nicolodi\nDavid Poznik\nDavid Toneian +\nDea MarÃ­a LÃ©on\nDeepak George +\nDmitriy +\nDominique Garmier +\nDonald Thevalingam +\nDoug Davis +\nDukastlik +\nElahe Sharifi +\nEric Han +\nFangchen Li\nFrancisco Alfaro +\nGadea Autric +\nGuillaume Lemaitre\nHadi Abdi Khojasteh\nHedeer El Showk +\nHuanghz2001 +\nIsaac Virshup\nIssam +\nItay Azolay +\nItayazolay +\nJaca +\nJack McIvor +\nJackCollins91 +\nJames Spencer +\nJay\nJessica Greene\nJirka Borovec +\nJohannaTrost +\nJohn C +\nJoris Van den Bossche\nJosÃ© Lucas Mayer +\nJosÃ© Lucas Silva Mayer +\nJoÃ£o Andrade +\nKai MÃ¼hlbauer\nKatharina Tielking, MD +\nKazuto Haruguchi +\nKevin\nLawrence Mitchell\nLinus +\nLinus Sommer +\nLouis-Ãmile Robitaille +\nLuke Manley\nLumberbot (aka Jack)\nMaggie Liu +\nMainHanzo +\nMarc Garcia\nMarco Edward Gorelli\nMarcoGorelli\nMartin Å Ã­cho +\nMateusz SokÃ³Å\nMatheus Felipe +\nMatthew Roeschke\nMatthias Bussonnier\nMaxwell Bileschi +\nMichael Tiemann\nMichaÅ GÃ³rny\nMolly Bowers +\nMoritz Schubert +\nNNLNR +\nNatalia Mokeeva\nNils MÃ¼ller-Wendt +\nOmar Elbaz\nPandas Development Team\nParas Gupta +\nParthi\nPatrick Hoefler\nPaul Pellissier +\nPaul Uhlenbruck +\nPhilip Meier\nPhilippe THOMY +\nQuang Nguyá»n\nRaghav\nRajat Subhra Mukherjee\nRalf Gommers\nRandolf Scholz +\nRichard Shadrach\nRob +\nRohan Jain +\nRyan Gibson +\nSai-Suraj-27 +\nSamuel Oranyeli +\nSara Bonati +\nSebastian Berg\nSergey Zakharov +\nShyamala Venkatakrishnan +\nStEmGeo +\nStefanie Molin\nStijn de Gooijer +\nThiago Gariani +\nThomas A Caswell\nThomas Baumann +\nThomas Guillet +\nThomas Lazarus +\nThomas Li\nTim Hoffmann\nTim Swast\nTom Augspurger\nToro +\nTorsten WÃ¶rtwein\nVille Aikas +\nVinita Parasrampuria +\nVyas Ramasubramani +\nWilliam Andrea\nWilliam Ayd\nWillian Wang +\nXiao Yuan\nYao Xiao\nYves Delley\nZemux1613 +\nZiad Kermadi +\naaron-robeson-8451 +\naram-cinnamon +\ncaneff +\nccccjone +\nchris-caballero +\ncobalt\ncolor455nm +\ndenisrei +\ndependabot[bot]\njbrockmendel\njfadia +\njohanna.trost +\nkgmuzungu +\nmecopur +\nmhb143 +\nmorotti +\nmvirts +\nomar-elbaz\npaulreece\npre-commit-ci[bot]\nraj-thapa\nrebecca-palmer\nrmhowe425\nrohanjain101\nshiersansi +\nsmij720\nsrkds +\ntaytzehao\ntorext\nvboxuser +\nxzmeng +\nyashb +\nprevious\nRelease notes\nnext\nWhatâs new in 2.1.4 (December 8, 2023)\nOn this page\nUpcoming changes in pandas 3.0\nCopy-on-Write\nDedicated string data type (backed by Arrow) by default\nEnhancements\nADBC Driver support in to_sql and read_sql\nCreate a pandas Series based on one or more conditions\nto_numpy\nfor NumPy nullable and Arrow types converts to suitable NumPy dtype\nSeries.struct accessor for PyArrow structured data\nSeries.list accessor for PyArrow list data\nCalamine engine for\nread_excel()\nOther enhancements\nNotable bug fixes\nmerge()\nand\nDataFrame.join()\nnow consistently follow documented sort behavior\nmerge()\nand\nDataFrame.join()\nno longer reorder levels when levels differ\nIncreased minimum versions for dependencies\nOther API changes\nDeprecations\nChained assignment\nDeprecate aliases\nM\n,\nQ\n,\nY\n, etc. in favour of\nME\n,\nQE\n,\nYE\n, etc. for offsets\nDeprecated automatic downcasting\nOther Deprecations\nPerformance improvements\nBug fixes\nCategorical\nDatetimelike\nTimedelta\nTimezones\nNumeric\nConversion\nStrings\nInterval\nIndexing\nMissing\nMultiIndex\nI/O\nPeriod\nPlotting\nGroupby/resample/rolling\nReshaping\nSparse\nOther\nContributors\nShow Source",
    "crawl_status": "success"
  },
  {
    "library_name": "Pandas",
    "url": "https://pandas.pydata.org/pandas-docs/version/2.0/whatsnew/v2.0.0.html",
    "version": "v2.0.0.html",
    "title": "Whatâs new in 2.0.0 (April 3, 2023) — pandas 2.0.3 documentation",
    "release_date": "Unknown release date",
    "content": "Whatâs new in 2.0.0 (April 3, 2023)\n#\nThese are the changes in pandas 2.0.0. See\nRelease notes\nfor a full changelog\nincluding other versions of pandas.\nEnhancements\n#\nInstalling optional dependencies with pip extras\n#\nWhen installing pandas using pip, sets of optional dependencies can also be installed by specifying extras.\npip\ninstall\n\"pandas[performance, aws]>=2.0.0\"\nThe available extras, found in the\ninstallation guide\n, are\n[all,\nperformance,\ncomputation,\nfss,\naws,\ngcp,\nexcel,\nparquet,\nfeather,\nhdf5,\nspss,\npostgresql,\nmysql,\nsql-other,\nhtml,\nxml,\nplot,\noutput_formatting,\nclipboard,\ncompression,\ntest]\n(\nGH39164\n).\nIndex\ncan now hold numpy numeric dtypes\n#\nIt is now possible to use any numpy numeric dtype in a\nIndex\n(\nGH42717\n).\nPreviously it was only possible to use\nint64\n,\nuint64\n&\nfloat64\ndtypes:\nIn [1]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nint8\n)\nOut[1]:\nInt64Index([1, 2, 3], dtype=\"int64\")\nIn [2]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nuint16\n)\nOut[2]:\nUInt64Index([1, 2, 3], dtype=\"uint64\")\nIn [3]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nfloat32\n)\nOut[3]:\nFloat64Index([1.0, 2.0, 3.0], dtype=\"float64\")\nInt64Index\n,\nUInt64Index\n&\nFloat64Index\nwere deprecated in pandas\nversion 1.4 and have now been removed. Instead\nIndex\nshould be used directly, and\ncan it now take all numpy numeric dtypes, i.e.\nint8\n/\nint16\n/\nint32\n/\nint64\n/\nuint8\n/\nuint16\n/\nuint32\n/\nuint64\n/\nfloat32\n/\nfloat64\ndtypes:\nIn [1]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nint8\n)\nOut[1]:\nIndex([1, 2, 3], dtype='int8')\nIn [2]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nuint16\n)\nOut[2]:\nIndex([1, 2, 3], dtype='uint16')\nIn [3]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nfloat32\n)\nOut[3]:\nIndex([1.0, 2.0, 3.0], dtype='float32')\nThe ability for\nIndex\nto hold the numpy numeric dtypes has meant some changes in Pandas\nfunctionality. In particular, operations that previously were forced to create 64-bit indexes,\ncan now create indexes with lower bit sizes, e.g. 32-bit indexes.\nBelow is a possibly non-exhaustive list of changes:\nInstantiating using a numpy numeric array now follows the dtype of the numpy array.\nPreviously, all indexes created from numpy numeric arrays were forced to 64-bit. Now,\nfor example,\nIndex(np.array([1,\n2,\n3]))\nwill be\nint32\non 32-bit systems, where\nit previously would have been\nint64\neven on 32-bit systems.\nInstantiating\nIndex\nusing a list of numbers will still return 64bit dtypes,\ne.g.\nIndex([1,\n2,\n3])\nwill have a\nint64\ndtype, which is the same as previously.\nThe various numeric datetime attributes of\nDatetimeIndex\n(\nday\n,\nmonth\n,\nyear\netc.) were previously in of\ndtype\nint64\n, while they were\nint32\nfor\narrays.DatetimeArray\n. They are now\nint32\non\nDatetimeIndex\nalso:\nIn [4]:\nidx\n=\npd\n.\ndate_range\n(\nstart\n=\n'1/1/2018'\n,\nperiods\n=\n3\n,\nfreq\n=\n'M'\n)\nIn [5]:\nidx\n.\narray\n.\nyear\nOut[5]:\narray([2018, 2018, 2018], dtype=int32)\nIn [6]:\nidx\n.\nyear\nOut[6]:\nIndex([2018, 2018, 2018], dtype='int32')\nLevel dtypes on Indexes from\nSeries.sparse.from_coo()\nare now of dtype\nint32\n,\nthe same as they are on the\nrows\n/\ncols\non a scipy sparse matrix. Previously they\nwere of dtype\nint64\n.\nIn [7]:\nfrom\nscipy\nimport\nsparse\nIn [8]:\nA\n=\nsparse\n.\ncoo_matrix\n(\n...:\n([\n3.0\n,\n1.0\n,\n2.0\n],\n([\n1\n,\n0\n,\n0\n],\n[\n0\n,\n2\n,\n3\n])),\nshape\n=\n(\n3\n,\n4\n)\n...:\n)\n...:\nIn [9]:\nser\n=\npd\n.\nSeries\n.\nsparse\n.\nfrom_coo\n(\nA\n)\nIn [10]:\nser\n.\nindex\n.\ndtypes\nOut[10]:\nlevel_0    int32\nlevel_1    int32\ndtype: object\nIndex\ncannot be instantiated using a float16 dtype. Previously instantiating\nan\nIndex\nusing dtype\nfloat16\nresulted in a\nFloat64Index\nwith a\nfloat64\ndtype. It now raises a\nNotImplementedError\n:\nIn [11]:\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nfloat16\n)\n---------------------------------------------------------------------------\nNotImplementedError\nTraceback (most recent call last)\nCell\nIn\n[\n11\n],\nline\n1\n---->\n1\npd\n.\nIndex\n([\n1\n,\n2\n,\n3\n],\ndtype\n=\nnp\n.\nfloat16\n)\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:562,\nin\nIndex.__new__\n(cls, data, dtype, copy, name, tupleize_cols)\n558\narr\n=\nensure_wrapped_if_datetimelike\n(\narr\n)\n560\nklass\n=\ncls\n.\n_dtype_to_subclass\n(\narr\n.\ndtype\n)\n-->\n562\narr\n=\nklass\n.\n_ensure_array\n(\narr\n,\narr\n.\ndtype\n,\ncopy\n=\nFalse\n)\n563\nreturn\nklass\n.\n_simple_new\n(\narr\n,\nname\n,\nrefs\n=\nrefs\n)\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:575,\nin\nIndex._ensure_array\n(cls, data, dtype, copy)\n572\nraise\nValueError\n(\n\"Index data must be 1-dimensional\"\n)\n573\nelif\ndtype\n==\nnp\n.\nfloat16\n:\n574\n# float16 not supported (no indexing engine)\n-->\n575\nraise\nNotImplementedError\n(\n\"float16 indexes are not supported\"\n)\n577\nif\ncopy\n:\n578\n# asarray_tuplesafe does not always copy underlying data,\n579\n#  so need to make sure that this happens\n580\ndata\n=\ndata\n.\ncopy\n()\nNotImplementedError\n: float16 indexes are not supported\nArgument\ndtype_backend\n, to return pyarrow-backed or numpy-backed nullable dtypes\n#\nThe following functions gained a new keyword\ndtype_backend\n(\nGH36712\n)\nread_csv()\nread_clipboard()\nread_fwf()\nread_excel()\nread_html()\nread_xml()\nread_json()\nread_sql()\nread_sql_query()\nread_sql_table()\nread_parquet()\nread_orc()\nread_feather()\nread_spss()\nto_numeric()\nDataFrame.convert_dtypes()\nSeries.convert_dtypes()\nWhen this option is set to\n\"numpy_nullable\"\nit will return a\nDataFrame\nthat is\nbacked by nullable dtypes.\nWhen this keyword is set to\n\"pyarrow\"\n, then these functions will return pyarrow-backed nullable\nArrowDtype\nDataFrames (\nGH48957\n,\nGH49997\n):\nread_csv()\nread_clipboard()\nread_fwf()\nread_excel()\nread_html()\nread_xml()\nread_json()\nread_sql()\nread_sql_query()\nread_sql_table()\nread_parquet()\nread_orc()\nread_feather()\nread_spss()\nto_numeric()\nDataFrame.convert_dtypes()\nSeries.convert_dtypes()\nIn [12]:\nimport\nio\nIn [13]:\ndata\n=\nio\n.\nStringIO\n(\n\"\"\"a,b,c,d,e,f,g,h,i\n....:\n1,2.5,True,a,,,,,\n....:\n3,4.5,False,b,6,7.5,True,a,\n....:\n\"\"\"\n)\n....:\nIn [14]:\ndf\n=\npd\n.\nread_csv\n(\ndata\n,\ndtype_backend\n=\n\"pyarrow\"\n)\nIn [15]:\ndf\n.\ndtypes\nOut[15]:\na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni      null[pyarrow]\ndtype: object\nIn [16]:\ndata\n.\nseek\n(\n0\n)\nOut[16]:\n0\nIn [17]:\ndf_pyarrow\n=\npd\n.\nread_csv\n(\ndata\n,\ndtype_backend\n=\n\"pyarrow\"\n,\nengine\n=\n\"pyarrow\"\n)\nIn [18]:\ndf_pyarrow\n.\ndtypes\nOut[18]:\na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni      null[pyarrow]\ndtype: object\nCopy-on-Write improvements\n#\nA new lazy copy mechanism that defers the copy until the object in question is modified\nwas added to the methods listed in\nCopy-on-Write optimizations\n.\nThese methods return views when Copy-on-Write is enabled, which provides a significant\nperformance improvement compared to the regular execution (\nGH49473\n).\nAccessing a single column of a DataFrame as a Series (e.g.\ndf[\"col\"]\n) now always\nreturns a new object every time it is constructed when Copy-on-Write is enabled (not\nreturning multiple times an identical, cached Series object). This ensures that those\nSeries objects correctly follow the Copy-on-Write rules (\nGH49450\n)\nThe\nSeries\nconstructor will now create a lazy copy (deferring the copy until\na modification to the data happens) when constructing a Series from an existing\nSeries with the default of\ncopy=False\n(\nGH50471\n)\nThe\nDataFrame\nconstructor will now create a lazy copy (deferring the copy until\na modification to the data happens) when constructing from an existing\nDataFrame\nwith the default of\ncopy=False\n(\nGH51239\n)\nThe\nDataFrame\nconstructor, when constructing a DataFrame from a dictionary\nof Series objects and specifying\ncopy=False\n, will now use a lazy copy\nof those Series objects for the columns of the DataFrame (\nGH50777\n)\nThe\nDataFrame\nconstructor, when constructing a DataFrame from a\nSeries\nor\nIndex\nand specifying\ncopy=False\n, will\nnow respect Copy-on-Write.\nThe\nDataFrame\nand\nSeries\nconstructors, when constructing from\na NumPy array, will now copy the array by default to avoid mutating\nthe\nDataFrame\n/\nSeries\nwhen mutating the array. Specify\ncopy=False\nto get the old behavior.\nWhen setting\ncopy=False\npandas does not guarantee correct Copy-on-Write\nbehavior when the NumPy array is modified after creation of the\nDataFrame\n/\nSeries\n.\nThe\nDataFrame.from_records()\nwill now respect Copy-on-Write when called\nwith a\nDataFrame\n.\nTrying to set values using chained assignment (for example,\ndf[\"a\"][1:3]\n=\n0\n)\nwill now always raise a warning when Copy-on-Write is enabled. In this mode,\nchained assignment can never work because we are always setting into a temporary\nobject that is the result of an indexing operation (getitem), which under\nCopy-on-Write always behaves as a copy. Thus, assigning through a chain\ncan never update the original Series or DataFrame. Therefore, an informative\nwarning is raised to the user to avoid silently doing nothing (\nGH49467\n)\nDataFrame.replace()\nwill now respect the Copy-on-Write mechanism\nwhen\ninplace=True\n.\nDataFrame.transpose()\nwill now respect the Copy-on-Write mechanism.\nArithmetic operations that can be inplace, e.g.\nser\n*=\n2\nwill now respect the\nCopy-on-Write mechanism.\nDataFrame.__getitem__()\nwill now respect the Copy-on-Write mechanism when the\nDataFrame\nhas\nMultiIndex\ncolumns.\nSeries.__getitem__()\nwill now respect the Copy-on-Write mechanism when the\nSeries\nhas a\nMultiIndex\n.\nSeries.view()\nwill now respect the Copy-on-Write mechanism.\nCopy-on-Write can be enabled through one of\npd\n.\nset_option\n(\n\"mode.copy_on_write\"\n,\nTrue\n)\npd\n.\noptions\n.\nmode\n.\ncopy_on_write\n=\nTrue\nAlternatively, copy on write can be enabled locally through:\nwith\npd\n.\noption_context\n(\n\"mode.copy_on_write\"\n,\nTrue\n):\n...\nOther enhancements\n#\nAdded support for\nstr\naccessor methods when using\nArrowDtype\nwith a\npyarrow.string\ntype (\nGH50325\n)\nAdded support for\ndt\naccessor methods when using\nArrowDtype\nwith a\npyarrow.timestamp\ntype (\nGH50954\n)\nread_sas()\nnow supports using\nencoding='infer'\nto correctly read and use the encoding specified by the sas file. (\nGH48048\n)\nDataFrameGroupBy.quantile()\n,\nSeriesGroupBy.quantile()\nand\nDataFrameGroupBy.std()\nnow preserve nullable dtypes instead of casting to numpy dtypes (\nGH37493\n)\nDataFrameGroupBy.std()\n,\nSeriesGroupBy.std()\nnow support datetime64, timedelta64, and\nDatetimeTZDtype\ndtypes (\nGH48481\n)\nSeries.add_suffix()\n,\nDataFrame.add_suffix()\n,\nSeries.add_prefix()\nand\nDataFrame.add_prefix()\nsupport an\naxis\nargument. If\naxis\nis set, the default behaviour of which axis to consider can be overwritten (\nGH47819\n)\ntesting.assert_frame_equal()\nnow shows the first element where the DataFrames differ, analogously to\npytest\nâs output (\nGH47910\n)\nAdded\nindex\nparameter to\nDataFrame.to_dict()\n(\nGH46398\n)\nAdded support for extension array dtypes in\nmerge()\n(\nGH44240\n)\nAdded metadata propagation for binary operators on\nDataFrame\n(\nGH28283\n)\nAdded\ncumsum\n,\ncumprod\n,\ncummin\nand\ncummax\nto the\nExtensionArray\ninterface via\n_accumulate\n(\nGH28385\n)\nCategoricalConversionWarning\n,\nInvalidComparison\n,\nInvalidVersion\n,\nLossySetitemError\n, and\nNoBufferPresent\nare now exposed in\npandas.errors\n(\nGH27656\n)\nFix\ntest\noptional_extra by adding missing test package\npytest-asyncio\n(\nGH48361\n)\nDataFrame.astype()\nexception message thrown improved to include column name when type conversion is not possible. (\nGH47571\n)\ndate_range()\nnow supports a\nunit\nkeyword (âsâ, âmsâ, âusâ, or ânsâ) to specify the desired resolution of the output index (\nGH49106\n)\ntimedelta_range()\nnow supports a\nunit\nkeyword (âsâ, âmsâ, âusâ, or ânsâ) to specify the desired resolution of the output index (\nGH49824\n)\nDataFrame.to_json()\nnow supports a\nmode\nkeyword with supported inputs âwâ and âaâ. Defaulting to âwâ, âaâ can be used when lines=True and orient=ârecordsâ to append record oriented json lines to an existing json file. (\nGH35849\n)\nAdded\nname\nparameter to\nIntervalIndex.from_breaks()\n,\nIntervalIndex.from_arrays()\nand\nIntervalIndex.from_tuples()\n(\nGH48911\n)\nImprove exception message when using\ntesting.assert_frame_equal()\non a\nDataFrame\nto include the column that is compared (\nGH50323\n)\nImproved error message for\nmerge_asof()\nwhen join-columns were duplicated (\nGH50102\n)\nAdded support for extension array dtypes to\nget_dummies()\n(\nGH32430\n)\nAdded\nIndex.infer_objects()\nanalogous to\nSeries.infer_objects()\n(\nGH50034\n)\nAdded\ncopy\nparameter to\nSeries.infer_objects()\nand\nDataFrame.infer_objects()\n, passing\nFalse\nwill avoid making copies for series or columns that are already non-object or where no better dtype can be inferred (\nGH50096\n)\nDataFrame.plot.hist()\nnow recognizes\nxlabel\nand\nylabel\narguments (\nGH49793\n)\nSeries.drop_duplicates()\nhas gained\nignore_index\nkeyword to reset index (\nGH48304\n)\nSeries.dropna()\nand\nDataFrame.dropna()\nhas gained\nignore_index\nkeyword to reset index (\nGH31725\n)\nImproved error message in\nto_datetime()\nfor non-ISO8601 formats, informing users about the position of the first error (\nGH50361\n)\nImproved error message when trying to align\nDataFrame\nobjects (for example, in\nDataFrame.compare()\n) to clarify that âidentically labelledâ refers to both index and columns (\nGH50083\n)\nAdded support for\nIndex.min()\nand\nIndex.max()\nfor pyarrow string dtypes (\nGH51397\n)\nAdded\nDatetimeIndex.as_unit()\nand\nTimedeltaIndex.as_unit()\nto convert to different resolutions; supported resolutions are âsâ, âmsâ, âusâ, and ânsâ (\nGH50616\n)\nAdded\nSeries.dt.unit()\nand\nSeries.dt.as_unit()\nto convert to different resolutions; supported resolutions are âsâ, âmsâ, âusâ, and ânsâ (\nGH51223\n)\nAdded new argument\ndtype\nto\nread_sql()\nto be consistent with\nread_sql_query()\n(\nGH50797\n)\nread_csv()\n,\nread_table()\n,\nread_fwf()\nand\nread_excel()\nnow accept\ndate_format\n(\nGH50601\n)\nto_datetime()\nnow accepts\n\"ISO8601\"\nas an argument to\nformat\n, which will match any ISO8601 string (but possibly not identically-formatted) (\nGH50411\n)\nto_datetime()\nnow accepts\n\"mixed\"\nas an argument to\nformat\n, which will infer the format for each element individually (\nGH50972\n)\nAdded new argument\nengine\nto\nread_json()\nto support parsing JSON with pyarrow by specifying\nengine=\"pyarrow\"\n(\nGH48893\n)\nAdded support for SQLAlchemy 2.0 (\nGH40686\n)\nAdded support for\ndecimal\nparameter when\nengine=\"pyarrow\"\nin\nread_csv()\n(\nGH51302\n)\nIndex\nset operations\nIndex.union()\n,\nIndex.intersection()\n,\nIndex.difference()\n, and\nIndex.symmetric_difference()\nnow support\nsort=True\n, which will always return a sorted result, unlike the default\nsort=None\nwhich does not sort in some cases (\nGH25151\n)\nNotable bug fixes\n#\nThese are bug fixes that might have notable behavior changes.\nDataFrameGroupBy.cumsum()\nand\nDataFrameGroupBy.cumprod()\noverflow instead of lossy casting to float\n#\nIn previous versions we cast to float when applying\ncumsum\nand\ncumprod\nwhich\nlead to incorrect results even if the result could be hold by\nint64\ndtype.\nAdditionally, the aggregation overflows consistent with numpy and the regular\nDataFrame.cumprod()\nand\nDataFrame.cumsum()\nmethods when the limit of\nint64\nis reached (\nGH37493\n).\nOld Behavior\nIn [1]:\ndf\n=\npd\n.\nDataFrame\n({\n\"key\"\n:\n[\n\"b\"\n]\n*\n7\n,\n\"value\"\n:\n625\n})\nIn [2]:\ndf\n.\ngroupby\n(\n\"key\"\n)[\n\"value\"\n]\n.\ncumprod\n()[\n5\n]\nOut[2]:\n5.960464477539062e+16\nWe return incorrect results with the 6th value.\nNew Behavior\nIn [19]:\ndf\n=\npd\n.\nDataFrame\n({\n\"key\"\n:\n[\n\"b\"\n]\n*\n7\n,\n\"value\"\n:\n625\n})\nIn [20]:\ndf\n.\ngroupby\n(\n\"key\"\n)[\n\"value\"\n]\n.\ncumprod\n()\nOut[20]:\n0                   625\n1                390625\n2             244140625\n3          152587890625\n4        95367431640625\n5     59604644775390625\n6    359414837200037393\nName: value, dtype: int64\nWe overflow with the 7th value, but the 6th value is still correct.\nDataFrameGroupBy.nth()\nand\nSeriesGroupBy.nth()\nnow behave as filtrations\n#\nIn previous versions of pandas,\nDataFrameGroupBy.nth()\nand\nSeriesGroupBy.nth()\nacted as if they were aggregations. However, for most\ninputs\nn\n, they may return either zero or multiple rows per group. This means\nthat they are filtrations, similar to e.g.\nDataFrameGroupBy.head()\n. pandas\nnow treats them as filtrations (\nGH13666\n).\nIn [21]:\ndf\n=\npd\n.\nDataFrame\n({\n\"a\"\n:\n[\n1\n,\n1\n,\n2\n,\n1\n,\n2\n],\n\"b\"\n:\n[\nnp\n.\nnan\n,\n2.0\n,\n3.0\n,\n4.0\n,\n5.0\n]})\nIn [22]:\ngb\n=\ndf\n.\ngroupby\n(\n\"a\"\n)\nOld Behavior\nIn [5]:\ngb\n.\nnth\n(\nn\n=\n1\n)\nOut[5]:\nA    B\n1  1  2.0\n4  2  5.0\nNew Behavior\nIn [23]:\ngb\n.\nnth\n(\nn\n=\n1\n)\nOut[23]:\na    b\n1  1  2.0\n4  2  5.0\nIn particular, the index of the result is derived from the input by selecting\nthe appropriate rows. Also, when\nn\nis larger than the group, no rows instead of\nNaN\nis returned.\nOld Behavior\nIn [5]:\ngb\n.\nnth\n(\nn\n=\n3\n,\ndropna\n=\n\"any\"\n)\nOut[5]:\nB\nA\n1 NaN\n2 NaN\nNew Behavior\nIn [24]:\ngb\n.\nnth\n(\nn\n=\n3\n,\ndropna\n=\n\"any\"\n)\nOut[24]:\nEmpty DataFrame\nColumns: [a, b]\nIndex: []\nBackwards incompatible API changes\n#\nConstruction with datetime64 or timedelta64 dtype with unsupported resolution\n#\nIn past versions, when constructing a\nSeries\nor\nDataFrame\nand\npassing a âdatetime64â or âtimedelta64â dtype with unsupported resolution\n(i.e. anything other than ânsâ), pandas would silently replace the given dtype\nwith its nanosecond analogue:\nPrevious behavior\n:\nIn [5]:\npd\n.\nSeries\n([\n\"2016-01-01\"\n],\ndtype\n=\n\"datetime64[s]\"\n)\nOut[5]:\n0   2016-01-01\ndtype: datetime64[ns]\nIn [6] pd.Series([\"2016-01-01\"], dtype=\"datetime64[D]\")\nOut[6]:\n0   2016-01-01\ndtype: datetime64[ns]\nIn pandas 2.0 we support resolutions âsâ, âmsâ, âusâ, and ânsâ. When passing\na supported dtype (e.g. âdatetime64[s]â), the result now has exactly\nthe requested dtype:\nNew behavior\n:\nIn [25]:\npd\n.\nSeries\n([\n\"2016-01-01\"\n],\ndtype\n=\n\"datetime64[s]\"\n)\nOut[25]:\n0   2016-01-01\ndtype: datetime64[s]\nWith an un-supported dtype, pandas now raises instead of silently swapping in\na supported dtype:\nNew behavior\n:\nIn [26]:\npd\n.\nSeries\n([\n\"2016-01-01\"\n],\ndtype\n=\n\"datetime64[D]\"\n)\n---------------------------------------------------------------------------\nTypeError\nTraceback (most recent call last)\nCell\nIn\n[\n26\n],\nline\n1\n---->\n1\npd\n.\nSeries\n([\n\"2016-01-01\"\n],\ndtype\n=\n\"datetime64[D]\"\n)\nFile ~/work/pandas/pandas/pandas/core/series.py:509,\nin\nSeries.__init__\n(self, data, index, dtype, name, copy, fastpath)\n507\ndata\n=\ndata\n.\ncopy\n()\n508\nelse\n:\n-->\n509\ndata\n=\nsanitize_array\n(\ndata\n,\nindex\n,\ndtype\n,\ncopy\n)\n511\nmanager\n=\nget_option\n(\n\"mode.data_manager\"\n)\n512\nif\nmanager\n==\n\"block\"\n:\nFile ~/work/pandas/pandas/pandas/core/construction.py:599,\nin\nsanitize_array\n(data, index, dtype, copy, allow_2d)\n596\nsubarr\n=\nnp\n.\narray\n([],\ndtype\n=\nnp\n.\nfloat64\n)\n598\nelif\ndtype\nis\nnot\nNone\n:\n-->\n599\nsubarr\n=\n_try_cast\n(\ndata\n,\ndtype\n,\ncopy\n)\n601\nelse\n:\n602\nsubarr\n=\nmaybe_convert_platform\n(\ndata\n)\nFile ~/work/pandas/pandas/pandas/core/construction.py:756,\nin\n_try_cast\n(arr, dtype, copy)\n751\nreturn\nlib\n.\nensure_string_array\n(\narr\n,\nconvert_na_value\n=\nFalse\n,\ncopy\n=\ncopy\n)\n.\nreshape\n(\n752\nshape\n753\n)\n755\nelif\ndtype\n.\nkind\nin\n[\n\"m\"\n,\n\"M\"\n]:\n-->\n756\nreturn\nmaybe_cast_to_datetime\n(\narr\n,\ndtype\n)\n758\n# GH#15832: Check if we are requesting a numeric dtype and\n759\n# that we can convert the data to the requested dtype.\n760\nelif\nis_integer_dtype\n(\ndtype\n):\n761\n# this will raise if we have e.g. floats\nFile ~/work/pandas/pandas/pandas/core/dtypes/cast.py:1236,\nin\nmaybe_cast_to_datetime\n(value, dtype)\n1232\nraise\nTypeError\n(\n\"value must be listlike\"\n)\n1234\n# TODO: _from_sequence would raise ValueError in cases where\n1235\n#  _ensure_nanosecond_dtype raises TypeError\n->\n1236\n_ensure_nanosecond_dtype\n(\ndtype\n)\n1238\nif\nis_timedelta64_dtype\n(\ndtype\n):\n1239\nres\n=\nTimedeltaArray\n.\n_from_sequence\n(\nvalue\n,\ndtype\n=\ndtype\n)\nFile ~/work/pandas/pandas/pandas/core/dtypes/cast.py:1294,\nin\n_ensure_nanosecond_dtype\n(dtype)\n1291\nraise\nValueError\n(\nmsg\n)\n1292\n# TODO: ValueError or TypeError? existing test\n1293\n#  test_constructor_generic_timestamp_bad_frequency expects TypeError\n->\n1294\nraise\nTypeError\n(\n1295\nf\n\"dtype=\n{\ndtype\n}\nis not supported. Supported resolutions are 's', \"\n1296\n\"'ms', 'us', and 'ns'\"\n1297\n)\nTypeError\n: dtype=datetime64[D] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'\nValue counts sets the resulting name to\ncount\n#\nIn past versions, when running\nSeries.value_counts()\n, the result would inherit\nthe original objectâs name, and the result index would be nameless. This would cause\nconfusion when resetting the index, and the column names would not correspond with the\ncolumn values.\nNow, the result name will be\n'count'\n(or\n'proportion'\nif\nnormalize=True\nwas passed),\nand the index will be named after the original object (\nGH49497\n).\nPrevious behavior\n:\nIn [8]:\npd\n.\nSeries\n([\n'quetzal'\n,\n'quetzal'\n,\n'elk'\n],\nname\n=\n'animal'\n)\n.\nvalue_counts\n()\nOut[2]:\nquetzal    2\nelk        1\nName: animal, dtype: int64\nNew behavior\n:\nIn [27]:\npd\n.\nSeries\n([\n'quetzal'\n,\n'quetzal'\n,\n'elk'\n],\nname\n=\n'animal'\n)\n.\nvalue_counts\n()\nOut[27]:\nanimal\nquetzal    2\nelk        1\nName: count, dtype: int64\nLikewise for other\nvalue_counts\nmethods (for example,\nDataFrame.value_counts()\n).\nDisallow astype conversion to non-supported datetime64/timedelta64 dtypes\n#\nIn previous versions, converting a\nSeries\nor\nDataFrame\nfrom\ndatetime64[ns]\nto a different\ndatetime64[X]\ndtype would return\nwith\ndatetime64[ns]\ndtype instead of the requested dtype. In pandas 2.0,\nsupport is added for âdatetime64[s]â, âdatetime64[ms]â, and âdatetime64[us]â dtypes,\nso converting to those dtypes gives exactly the requested dtype:\nPrevious behavior\n:\nIn [28]:\nidx\n=\npd\n.\ndate_range\n(\n\"2016-01-01\"\n,\nperiods\n=\n3\n)\nIn [29]:\nser\n=\npd\n.\nSeries\n(\nidx\n)\nPrevious behavior\n:\nIn [4]:\nser\n.\nastype\n(\n\"datetime64[s]\"\n)\nOut[4]:\n0   2016-01-01\n1   2016-01-02\n2   2016-01-03\ndtype: datetime64[ns]\nWith the new behavior, we get exactly the requested dtype:\nNew behavior\n:\nIn [30]:\nser\n.\nastype\n(\n\"datetime64[s]\"\n)\nOut[30]:\n0   2016-01-01\n1   2016-01-02\n2   2016-01-03\ndtype: datetime64[s]\nFor non-supported resolutions e.g. âdatetime64[D]â, we raise instead of silently\nignoring the requested dtype:\nNew behavior\n:\nIn [31]:\nser\n.\nastype\n(\n\"datetime64[D]\"\n)\n---------------------------------------------------------------------------\nTypeError\nTraceback (most recent call last)\nCell\nIn\n[\n31\n],\nline\n1\n---->\n1\nser\n.\nastype\n(\n\"datetime64[D]\"\n)\nFile ~/work/pandas/pandas/pandas/core/generic.py:6324,\nin\nNDFrame.astype\n(self, dtype, copy, errors)\n6317\nresults\n=\n[\n6318\nself\n.\niloc\n[:,\ni\n]\n.\nastype\n(\ndtype\n,\ncopy\n=\ncopy\n)\n6319         for i\nin\nrange(len\n(self.columns))\n6320\n]\n6322\nelse\n:\n6323\n# else, only a single dtype is given\n->\n6324\nnew_data\n=\nself\n.\n_mgr\n.\nastype\n(\ndtype\n=\ndtype\n,\ncopy\n=\ncopy\n,\nerrors\n=\nerrors\n)\n6325\nreturn\nself\n.\n_constructor\n(\nnew_data\n)\n.\n__finalize__\n(\nself\n,\nmethod\n=\n\"astype\"\n)\n6327\n# GH 33113: handle empty frame or series\nFile ~/work/pandas/pandas/pandas/core/internals/managers.py:451,\nin\nBaseBlockManager.astype\n(self, dtype, copy, errors)\n448\nelif\nusing_copy_on_write\n():\n449\ncopy\n=\nFalse\n-->\n451\nreturn\nself\n.\napply\n(\n452\n\"astype\"\n,\n453\ndtype\n=\ndtype\n,\n454\ncopy\n=\ncopy\n,\n455\nerrors\n=\nerrors\n,\n456\nusing_cow\n=\nusing_copy_on_write\n(),\n457\n)\nFile ~/work/pandas/pandas/pandas/core/internals/managers.py:352,\nin\nBaseBlockManager.apply\n(self, f, align_keys, **kwargs)\n350\napplied\n=\nb\n.\napply\n(\nf\n,\n**\nkwargs\n)\n351\nelse\n:\n-->\n352\napplied\n=\ngetattr\n(\nb\n,\nf\n)(\n**\nkwargs\n)\n353\nresult_blocks\n=\nextend_blocks\n(\napplied\n,\nresult_blocks\n)\n355\nout\n=\ntype\n(\nself\n)\n.\nfrom_blocks\n(\nresult_blocks\n,\nself\n.\naxes\n)\nFile ~/work/pandas/pandas/pandas/core/internals/blocks.py:511,\nin\nBlock.astype\n(self, dtype, copy, errors, using_cow)\n491\n\"\"\"\n492\nCoerce to the new dtype.\n493\n(...)\n507\nBlock\n508\n\"\"\"\n509\nvalues\n=\nself\n.\nvalues\n-->\n511\nnew_values\n=\nastype_array_safe\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n,\nerrors\n=\nerrors\n)\n513\nnew_values\n=\nmaybe_coerce_values\n(\nnew_values\n)\n515\nrefs\n=\nNone\nFile ~/work/pandas/pandas/pandas/core/dtypes/astype.py:242,\nin\nastype_array_safe\n(values, dtype, copy, errors)\n239\ndtype\n=\ndtype\n.\nnumpy_dtype\n241\ntry\n:\n-->\n242\nnew_values\n=\nastype_array\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n)\n243\nexcept\n(\nValueError\n,\nTypeError\n):\n244\n# e.g. _astype_nansafe can fail on object-dtype of strings\n245\n#  trying to convert to float\n246\nif\nerrors\n==\n\"ignore\"\n:\nFile ~/work/pandas/pandas/pandas/core/dtypes/astype.py:184,\nin\nastype_array\n(values, dtype, copy)\n180\nreturn\nvalues\n182\nif\nnot\nisinstance\n(\nvalues\n,\nnp\n.\nndarray\n):\n183\n# i.e. ExtensionArray\n-->\n184\nvalues\n=\nvalues\n.\nastype\n(\ndtype\n,\ncopy\n=\ncopy\n)\n186\nelse\n:\n187\nvalues\n=\n_astype_nansafe\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n)\nFile ~/work/pandas/pandas/pandas/core/arrays/datetimes.py:701,\nin\nDatetimeArray.astype\n(self, dtype, copy)\n699\nelif\nis_period_dtype\n(\ndtype\n):\n700\nreturn\nself\n.\nto_period\n(\nfreq\n=\ndtype\n.\nfreq\n)\n-->\n701\nreturn\ndtl\n.\nDatetimeLikeArrayMixin\n.\nastype\n(\nself\n,\ndtype\n,\ncopy\n)\nFile ~/work/pandas/pandas/pandas/core/arrays/datetimelike.py:487,\nin\nDatetimeLikeArrayMixin.astype\n(self, dtype, copy)\n480\nelif\n(\n481\nis_datetime_or_timedelta_dtype\n(\ndtype\n)\n482\nand\nnot\nis_dtype_equal\n(\nself\n.\ndtype\n,\ndtype\n)\n483\n)\nor\nis_float_dtype\n(\ndtype\n):\n484\n# disallow conversion between datetime/timedelta,\n485\n# and conversions for any datetimelike to float\n486\nmsg\n=\nf\n\"Cannot cast\n{\ntype\n(\nself\n)\n.\n__name__\n}\nto dtype\n{\ndtype\n}\n\"\n-->\n487\nraise\nTypeError\n(\nmsg\n)\n488\nelse\n:\n489\nreturn\nnp\n.\nasarray\n(\nself\n,\ndtype\n=\ndtype\n)\nTypeError\n: Cannot cast DatetimeArray to dtype datetime64[D]\nFor conversion from\ntimedelta64[ns]\ndtypes, the old behavior converted\nto a floating point format.\nPrevious behavior\n:\nIn [32]:\nidx\n=\npd\n.\ntimedelta_range\n(\n\"1 Day\"\n,\nperiods\n=\n3\n)\nIn [33]:\nser\n=\npd\n.\nSeries\n(\nidx\n)\nPrevious behavior\n:\nIn [7]:\nser\n.\nastype\n(\n\"timedelta64[s]\"\n)\nOut[7]:\n0     86400.0\n1    172800.0\n2    259200.0\ndtype: float64\nIn [8]:\nser\n.\nastype\n(\n\"timedelta64[D]\"\n)\nOut[8]:\n0    1.0\n1    2.0\n2    3.0\ndtype: float64\nThe new behavior, as for datetime64, either gives exactly the requested dtype or raises:\nNew behavior\n:\nIn [34]:\nser\n.\nastype\n(\n\"timedelta64[s]\"\n)\nOut[34]:\n0   1 days 00:00:00\n1   2 days 00:00:00\n2   3 days 00:00:00\ndtype: timedelta64[s]\nIn [35]:\nser\n.\nastype\n(\n\"timedelta64[D]\"\n)\n---------------------------------------------------------------------------\nValueError\nTraceback (most recent call last)\nCell\nIn\n[\n35\n],\nline\n1\n---->\n1\nser\n.\nastype\n(\n\"timedelta64[D]\"\n)\nFile ~/work/pandas/pandas/pandas/core/generic.py:6324,\nin\nNDFrame.astype\n(self, dtype, copy, errors)\n6317\nresults\n=\n[\n6318\nself\n.\niloc\n[:,\ni\n]\n.\nastype\n(\ndtype\n,\ncopy\n=\ncopy\n)\n6319         for i\nin\nrange(len\n(self.columns))\n6320\n]\n6322\nelse\n:\n6323\n# else, only a single dtype is given\n->\n6324\nnew_data\n=\nself\n.\n_mgr\n.\nastype\n(\ndtype\n=\ndtype\n,\ncopy\n=\ncopy\n,\nerrors\n=\nerrors\n)\n6325\nreturn\nself\n.\n_constructor\n(\nnew_data\n)\n.\n__finalize__\n(\nself\n,\nmethod\n=\n\"astype\"\n)\n6327\n# GH 33113: handle empty frame or series\nFile ~/work/pandas/pandas/pandas/core/internals/managers.py:451,\nin\nBaseBlockManager.astype\n(self, dtype, copy, errors)\n448\nelif\nusing_copy_on_write\n():\n449\ncopy\n=\nFalse\n-->\n451\nreturn\nself\n.\napply\n(\n452\n\"astype\"\n,\n453\ndtype\n=\ndtype\n,\n454\ncopy\n=\ncopy\n,\n455\nerrors\n=\nerrors\n,\n456\nusing_cow\n=\nusing_copy_on_write\n(),\n457\n)\nFile ~/work/pandas/pandas/pandas/core/internals/managers.py:352,\nin\nBaseBlockManager.apply\n(self, f, align_keys, **kwargs)\n350\napplied\n=\nb\n.\napply\n(\nf\n,\n**\nkwargs\n)\n351\nelse\n:\n-->\n352\napplied\n=\ngetattr\n(\nb\n,\nf\n)(\n**\nkwargs\n)\n353\nresult_blocks\n=\nextend_blocks\n(\napplied\n,\nresult_blocks\n)\n355\nout\n=\ntype\n(\nself\n)\n.\nfrom_blocks\n(\nresult_blocks\n,\nself\n.\naxes\n)\nFile ~/work/pandas/pandas/pandas/core/internals/blocks.py:511,\nin\nBlock.astype\n(self, dtype, copy, errors, using_cow)\n491\n\"\"\"\n492\nCoerce to the new dtype.\n493\n(...)\n507\nBlock\n508\n\"\"\"\n509\nvalues\n=\nself\n.\nvalues\n-->\n511\nnew_values\n=\nastype_array_safe\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n,\nerrors\n=\nerrors\n)\n513\nnew_values\n=\nmaybe_coerce_values\n(\nnew_values\n)\n515\nrefs\n=\nNone\nFile ~/work/pandas/pandas/pandas/core/dtypes/astype.py:242,\nin\nastype_array_safe\n(values, dtype, copy, errors)\n239\ndtype\n=\ndtype\n.\nnumpy_dtype\n241\ntry\n:\n-->\n242\nnew_values\n=\nastype_array\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n)\n243\nexcept\n(\nValueError\n,\nTypeError\n):\n244\n# e.g. _astype_nansafe can fail on object-dtype of strings\n245\n#  trying to convert to float\n246\nif\nerrors\n==\n\"ignore\"\n:\nFile ~/work/pandas/pandas/pandas/core/dtypes/astype.py:184,\nin\nastype_array\n(values, dtype, copy)\n180\nreturn\nvalues\n182\nif\nnot\nisinstance\n(\nvalues\n,\nnp\n.\nndarray\n):\n183\n# i.e. ExtensionArray\n-->\n184\nvalues\n=\nvalues\n.\nastype\n(\ndtype\n,\ncopy\n=\ncopy\n)\n186\nelse\n:\n187\nvalues\n=\n_astype_nansafe\n(\nvalues\n,\ndtype\n,\ncopy\n=\ncopy\n)\nFile ~/work/pandas/pandas/pandas/core/arrays/timedeltas.py:363,\nin\nTimedeltaArray.astype\n(self, dtype, copy)\n359\nreturn\ntype\n(\nself\n)\n.\n_simple_new\n(\n360\nres_values\n,\ndtype\n=\nres_values\n.\ndtype\n,\nfreq\n=\nself\n.\nfreq\n361\n)\n362\nelse\n:\n-->\n363\nraise\nValueError\n(\n364\nf\n\"Cannot convert from\n{\nself\n.\ndtype\n}\nto\n{\ndtype\n}\n. \"\n365\n\"Supported resolutions are 's', 'ms', 'us', 'ns'\"\n366\n)\n368\nreturn\ndtl\n.\nDatetimeLikeArrayMixin\n.\nastype\n(\nself\n,\ndtype\n,\ncopy\n=\ncopy\n)\nValueError\n: Cannot convert from timedelta64[ns] to timedelta64[D]. Supported resolutions are 's', 'ms', 'us', 'ns'\nUTC and fixed-offset timezones default to standard-library tzinfo objects\n#\nIn previous versions, the default\ntzinfo\nobject used to represent UTC\nwas\npytz.UTC\n. In pandas 2.0, we default to\ndatetime.timezone.utc\ninstead.\nSimilarly, for timezones represent fixed UTC offsets, we use\ndatetime.timezone\nobjects instead of\npytz.FixedOffset\nobjects. See (\nGH34916\n)\nPrevious behavior\n:\nIn [2]:\nts\n=\npd\n.\nTimestamp\n(\n\"2016-01-01\"\n,\ntz\n=\n\"UTC\"\n)\nIn [3]:\ntype\n(\nts\n.\ntzinfo\n)\nOut[3]:\npytz.UTC\nIn [4]:\nts2\n=\npd\n.\nTimestamp\n(\n\"2016-01-01 04:05:06-07:00\"\n)\nIn [3]:\ntype\n(\nts2\n.\ntzinfo\n)\nOut[5]:\npytz._FixedOffset\nNew behavior\n:\nIn [36]:\nts\n=\npd\n.\nTimestamp\n(\n\"2016-01-01\"\n,\ntz\n=\n\"UTC\"\n)\nIn [37]:\ntype\n(\nts\n.\ntzinfo\n)\nOut[37]:\ndatetime.timezone\nIn [38]:\nts2\n=\npd\n.\nTimestamp\n(\n\"2016-01-01 04:05:06-07:00\"\n)\nIn [39]:\ntype\n(\nts2\n.\ntzinfo\n)\nOut[39]:\ndatetime.timezone\nFor timezones that are neither UTC nor fixed offsets, e.g. âUS/Pacificâ, we\ncontinue to default to\npytz\nobjects.\nEmpty DataFrames/Series will now default to have a\nRangeIndex\n#\nBefore, constructing an empty (where\ndata\nis\nNone\nor an empty list-like argument)\nSeries\nor\nDataFrame\nwithout\nspecifying the axes (\nindex=None\n,\ncolumns=None\n) would return the axes as empty\nIndex\nwith object dtype.\nNow, the axes return an empty\nRangeIndex\n(\nGH49572\n).\nPrevious behavior\n:\nIn [8]:\npd\n.\nSeries\n()\n.\nindex\nOut[8]:\nIndex([], dtype='object')\nIn [9] pd.DataFrame().axes\nOut[9]:\n[Index([], dtype='object'), Index([], dtype='object')]\nNew behavior\n:\nIn [40]:\npd\n.\nSeries\n()\n.\nindex\nOut[40]:\nRangeIndex(start=0, stop=0, step=1)\nIn [41]:\npd\n.\nDataFrame\n()\n.\naxes\nOut[41]:\n[RangeIndex(start=0, stop=0, step=1), RangeIndex(start=0, stop=0, step=1)]\nDataFrame to LaTeX has a new render engine\n#\nThe existing\nDataFrame.to_latex()\nhas been restructured to utilise the\nextended implementation previously available under\nStyler.to_latex()\n.\nThe arguments signature is similar, albeit\ncol_space\nhas been removed since\nit is ignored by LaTeX engines. This render engine also requires\njinja2\nas a\ndependency which needs to be installed, since rendering is based upon jinja2 templates.\nThe pandas latex options below are no longer used and have been removed. The generic\nmax rows and columns arguments remain but for this functionality should be replaced\nby the Styler equivalents.\nThe alternative options giving similar functionality are indicated below:\ndisplay.latex.escape\n: replaced with\nstyler.format.escape\n,\ndisplay.latex.longtable\n: replaced with\nstyler.latex.environment\n,\ndisplay.latex.multicolumn\n,\ndisplay.latex.multicolumn_format\nand\ndisplay.latex.multirow\n: replaced with\nstyler.sparse.rows\n,\nstyler.sparse.columns\n,\nstyler.latex.multirow_align\nand\nstyler.latex.multicol_align\n,\ndisplay.latex.repr\n: replaced with\nstyler.render.repr\n,\ndisplay.max_rows\nand\ndisplay.max_columns\n: replace with\nstyler.render.max_rows\n,\nstyler.render.max_columns\nand\nstyler.render.max_elements\n.\nNote that due to this change some defaults have also changed:\nmultirow\nnow defaults to\nTrue\n.\nmultirow_align\ndefaults to\nârâ\ninstead of\nâlâ\n.\nmulticol_align\ndefaults to\nârâ\ninstead of\nâlâ\n.\nescape\nnow defaults to\nFalse\n.\nNote that the behaviour of\n_repr_latex_\nis also changed. Previously\nsetting\ndisplay.latex.repr\nwould generate LaTeX only when using nbconvert for a\nJupyterNotebook, and not when the user is running the notebook. Now the\nstyler.render.repr\noption allows control of the specific output\nwithin JupyterNotebooks for operations (not just on nbconvert). See\nGH39911\n.\nIncreased minimum versions for dependencies\n#\nSome minimum supported versions of dependencies were updated.\nIf installed, we now require:\nPackage\nMinimum Version\nRequired\nChanged\nmypy (dev)\n1.0\nX\npytest (dev)\n7.0.0\nX\npytest-xdist (dev)\n2.2.0\nX\nhypothesis (dev)\n6.34.2\nX\npython-dateutil\n2.8.2\nX\nX\ntzdata\n2022.1\nX\nX\nFor\noptional libraries\nthe general recommendation is to use the latest version.\nThe following table lists the lowest version per library that is currently being tested throughout the development of pandas.\nOptional libraries below the lowest tested version may still work, but are not considered supported.\nPackage\nMinimum Version\nChanged\npyarrow\n7.0.0\nX\nmatplotlib\n3.6.1\nX\nfastparquet\n0.6.3\nX\nxarray\n0.21.0\nX\nSee\nDependencies\nand\nOptional dependencies\nfor more.\nDatetimes are now parsed with a consistent format\n#\nIn the past,\nto_datetime()\nguessed the format for each element independently. This was appropriate for some cases where elements had mixed date formats - however, it would regularly cause problems when users expected a consistent format but the function would switch formats between elements. As of version 2.0.0, parsing will use a consistent format, determined by the first non-NA value (unless the user specifies a format, in which case that is used).\nOld behavior\n:\nIn [1]:\nser\n=\npd\n.\nSeries\n([\n'13-01-2000'\n,\n'12-01-2000'\n])\nIn [2]:\npd\n.\nto_datetime\n(\nser\n)\nOut[2]:\n0   2000-01-13\n1   2000-12-01\ndtype: datetime64[ns]\nNew behavior\n:\nIn [42]:\nser\n=\npd\n.\nSeries\n([\n'13-01-2000'\n,\n'12-01-2000'\n])\nIn [43]:\npd\n.\nto_datetime\n(\nser\n)\nOut[43]:\n0   2000-01-13\n1   2000-01-12\ndtype: datetime64[ns]\nNote that this affects\nread_csv()\nas well.\nIf you still need to parse dates with inconsistent formats, you can use\nformat='mixed'\n(possibly alongside\ndayfirst\n)\nser\n=\npd\n.\nSeries\n([\n'13-01-2000'\n,\n'12 January 2000'\n])\npd\n.\nto_datetime\n(\nser\n,\nformat\n=\n'mixed'\n,\ndayfirst\n=\nTrue\n)\nor, if your formats are all ISO8601 (but possibly not identically-formatted)\nser\n=\npd\n.\nSeries\n([\n'2020-01-01'\n,\n'2020-01-01 03:00'\n])\npd\n.\nto_datetime\n(\nser\n,\nformat\n=\n'ISO8601'\n)\nOther API changes\n#\nThe\nfreq\n,\ntz\n,\nnanosecond\n, and\nunit\nkeywords in the\nTimestamp\nconstructor are now keyword-only (\nGH45307\n,\nGH32526\n)\nPassing\nnanoseconds\ngreater than 999 or less than 0 in\nTimestamp\nnow raises a\nValueError\n(\nGH48538\n,\nGH48255\n)\nread_csv()\n: specifying an incorrect number of columns with\nindex_col\nof now raises\nParserError\ninstead of\nIndexError\nwhen using the c parser.\nDefault value of\ndtype\nin\nget_dummies()\nis changed to\nbool\nfrom\nuint8\n(\nGH45848\n)\nDataFrame.astype()\n,\nSeries.astype()\n, and\nDatetimeIndex.astype()\ncasting datetime64 data to any of âdatetime64[s]â, âdatetime64[ms]â, âdatetime64[us]â will return an object with the given resolution instead of coercing back to âdatetime64[ns]â (\nGH48928\n)\nDataFrame.astype()\n,\nSeries.astype()\n, and\nDatetimeIndex.astype()\ncasting timedelta64 data to any of âtimedelta64[s]â, âtimedelta64[ms]â, âtimedelta64[us]â will return an object with the given resolution instead of coercing to âfloat64â dtype (\nGH48963\n)\nDatetimeIndex.astype()\n,\nTimedeltaIndex.astype()\n,\nPeriodIndex.astype()\nSeries.astype()\n,\nDataFrame.astype()\nwith\ndatetime64\n,\ntimedelta64\nor\nPeriodDtype\ndtypes no longer allow converting to integer dtypes other than âint64â, do\nobj.astype('int64',\ncopy=False).astype(dtype)\ninstead (\nGH49715\n)\nIndex.astype()\nnow allows casting from\nfloat64\ndtype to datetime-like dtypes, matching\nSeries\nbehavior (\nGH49660\n)\nPassing data with dtype of âtimedelta64[s]â, âtimedelta64[ms]â, or âtimedelta64[us]â to\nTimedeltaIndex\n,\nSeries\n, or\nDataFrame\nconstructors will now retain that dtype instead of casting to âtimedelta64[ns]â; timedelta64 data with lower resolution will be cast to the lowest supported resolution âtimedelta64[s]â (\nGH49014\n)\nPassing\ndtype\nof âtimedelta64[s]â, âtimedelta64[ms]â, or âtimedelta64[us]â to\nTimedeltaIndex\n,\nSeries\n, or\nDataFrame\nconstructors will now retain that dtype instead of casting to âtimedelta64[ns]â; passing a dtype with lower resolution for\nSeries\nor\nDataFrame\nwill be cast to the lowest supported resolution âtimedelta64[s]â (\nGH49014\n)\nPassing a\nnp.datetime64\nobject with non-nanosecond resolution to\nTimestamp\nwill retain the input resolution if it is âsâ, âmsâ, âusâ, or ânsâ; otherwise it will be cast to the closest supported resolution (\nGH49008\n)\nPassing\ndatetime64\nvalues with resolution other than nanosecond to\nto_datetime()\nwill retain the input resolution if it is âsâ, âmsâ, âusâ, or ânsâ; otherwise it will be cast to the closest supported resolution (\nGH50369\n)\nPassing integer values and a non-nanosecond datetime64 dtype (e.g. âdatetime64[s]â)\nDataFrame\n,\nSeries\n, or\nIndex\nwill treat the values as multiples of the dtypeâs unit, matching the behavior of e.g.\nSeries(np.array(values,\ndtype=\"M8[s]\"))\n(\nGH51092\n)\nPassing a string in ISO-8601 format to\nTimestamp\nwill retain the resolution of the parsed input if it is âsâ, âmsâ, âusâ, or ânsâ; otherwise it will be cast to the closest supported resolution (\nGH49737\n)\nThe\nother\nargument in\nDataFrame.mask()\nand\nSeries.mask()\nnow defaults to\nno_default\ninstead of\nnp.nan\nconsistent with\nDataFrame.where()\nand\nSeries.where()\n. Entries will be filled with the corresponding NULL value (\nnp.nan\nfor numpy dtypes,\npd.NA\nfor extension dtypes). (\nGH49111\n)\nChanged behavior of\nSeries.quantile()\nand\nDataFrame.quantile()\nwith\nSparseDtype\nto retain sparse dtype (\nGH49583\n)\nWhen creating a\nSeries\nwith a object-dtype\nIndex\nof datetime objects, pandas no longer silently converts the index to a\nDatetimeIndex\n(\nGH39307\n,\nGH23598\n)\npandas.testing.assert_index_equal()\nwith parameter\nexact=\"equiv\"\nnow considers two indexes equal when both are either a\nRangeIndex\nor\nIndex\nwith an\nint64\ndtype. Previously it meant either a\nRangeIndex\nor a\nInt64Index\n(\nGH51098\n)\nSeries.unique()\nwith dtype âtimedelta64[ns]â or âdatetime64[ns]â now returns\nTimedeltaArray\nor\nDatetimeArray\ninstead of\nnumpy.ndarray\n(\nGH49176\n)\nto_datetime()\nand\nDatetimeIndex\nnow allow sequences containing both\ndatetime\nobjects and numeric entries, matching\nSeries\nbehavior (\nGH49037\n,\nGH50453\n)\npandas.api.types.is_string_dtype()\nnow only returns\nTrue\nfor array-likes with\ndtype=object\nwhen the elements are inferred to be strings (\nGH15585\n)\nPassing a sequence containing\ndatetime\nobjects and\ndate\nobjects to\nSeries\nconstructor will return with\nobject\ndtype instead of\ndatetime64[ns]\ndtype, consistent with\nIndex\nbehavior (\nGH49341\n)\nPassing strings that cannot be parsed as datetimes to\nSeries\nor\nDataFrame\nwith\ndtype=\"datetime64[ns]\"\nwill raise instead of silently ignoring the keyword and returning\nobject\ndtype (\nGH24435\n)\nPassing a sequence containing a type that cannot be converted to\nTimedelta\nto\nto_timedelta()\nor to the\nSeries\nor\nDataFrame\nconstructor with\ndtype=\"timedelta64[ns]\"\nor to\nTimedeltaIndex\nnow raises\nTypeError\ninstead of\nValueError\n(\nGH49525\n)\nChanged behavior of\nIndex\nconstructor with sequence containing at least one\nNaT\nand everything else either\nNone\nor\nNaN\nto infer\ndatetime64[ns]\ndtype instead of\nobject\n, matching\nSeries\nbehavior (\nGH49340\n)\nread_stata()\nwith parameter\nindex_col\nset to\nNone\n(the default) will now set the index on the returned\nDataFrame\nto a\nRangeIndex\ninstead of a\nInt64Index\n(\nGH49745\n)\nChanged behavior of\nIndex\n,\nSeries\n, and\nDataFrame\narithmetic methods when working with object-dtypes, the results no longer do type inference on the result of the array operations, use\nresult.infer_objects(copy=False)\nto do type inference on the result (\nGH49999\n,\nGH49714\n)\nChanged behavior of\nIndex\nconstructor with an object-dtype\nnumpy.ndarray\ncontaining all-\nbool\nvalues or all-complex values, this will now retain object dtype, consistent with the\nSeries\nbehavior (\nGH49594\n)\nChanged behavior of\nSeries.astype()\nfrom object-dtype containing\nbytes\nobjects to string dtypes; this now does\nval.decode()\non bytes objects instead of\nstr(val)\n, matching\nIndex.astype()\nbehavior (\nGH45326\n)\nAdded\n\"None\"\nto default\nna_values\nin\nread_csv()\n(\nGH50286\n)\nChanged behavior of\nSeries\nand\nDataFrame\nconstructors when given an integer dtype and floating-point data that is not round numbers, this now raises\nValueError\ninstead of silently retaining the float dtype; do\nSeries(data)\nor\nDataFrame(data)\nto get the old behavior, and\nSeries(data).astype(dtype)\nor\nDataFrame(data).astype(dtype)\nto get the specified dtype (\nGH49599\n)\nChanged behavior of\nDataFrame.shift()\nwith\naxis=1\n, an integer\nfill_value\n, and homogeneous datetime-like dtype, this now fills new columns with integer dtypes instead of casting to datetimelike (\nGH49842\n)\nFiles are now closed when encountering an exception in\nread_json()\n(\nGH49921\n)\nChanged behavior of\nread_csv()\n,\nread_json()\n&\nread_fwf()\n, where the index will now always be a\nRangeIndex\n, when no index is specified. Previously the index would be a\nIndex\nwith dtype\nobject\nif the new DataFrame/Series has length 0 (\nGH49572\n)\nDataFrame.values()\n,\nDataFrame.to_numpy()\n,\nDataFrame.xs()\n,\nDataFrame.reindex()\n,\nDataFrame.fillna()\n, and\nDataFrame.replace()\nno longer silently consolidate the underlying arrays; do\ndf\n=\ndf.copy()\nto ensure consolidation (\nGH49356\n)\nCreating a new DataFrame using a full slice on both axes with\nloc\nor\niloc\n(thus,\ndf.loc[:,\n:]\nor\ndf.iloc[:,\n:]\n) now returns a\nnew DataFrame (shallow copy) instead of the original DataFrame, consistent with other\nmethods to get a full slice (for example\ndf.loc[:]\nor\ndf[:]\n) (\nGH49469\n)\nThe\nSeries\nand\nDataFrame\nconstructors will now return a shallow copy\n(i.e. share data, but not attributes) when passed a Series and DataFrame,\nrespectively, and with the default of\ncopy=False\n(and if no other keyword triggers\na copy). Previously, the new Series or DataFrame would share the index attribute (e.g.\ndf.index\n=\n...\nwould also update the index of the parent or child) (\nGH49523\n)\nDisallow computing\ncumprod\nfor\nTimedelta\nobject; previously this returned incorrect values (\nGH50246\n)\nDataFrame\nobjects read from a\nHDFStore\nfile without an index now have a\nRangeIndex\ninstead of an\nint64\nindex (\nGH51076\n)\nInstantiating an\nIndex\nwith an numeric numpy dtype with data containing\nNA\nand/or\nNaT\nnow raises a\nValueError\n. Previously a\nTypeError\nwas raised (\nGH51050\n)\nLoading a JSON file with duplicate columns using\nread_json(orient='split')\nrenames columns to avoid duplicates, as\nread_csv()\nand the other readers do (\nGH50370\n)\nThe levels of the index of the\nSeries\nreturned from\nSeries.sparse.from_coo\nnow always have dtype\nint32\n. Previously they had dtype\nint64\n(\nGH50926\n)\nto_datetime()\nwith\nunit\nof either âYâ or âMâ will now raise if a sequence contains a non-round\nfloat\nvalue, matching the\nTimestamp\nbehavior (\nGH50301\n)\nThe methods\nSeries.round()\n,\nDataFrame.__invert__()\n,\nSeries.__invert__()\n,\nDataFrame.swapaxes()\n,\nDataFrame.first()\n,\nDataFrame.last()\n,\nSeries.first()\n,\nSeries.last()\nand\nDataFrame.align()\nwill now always return new objects (\nGH51032\n)\nDataFrame\nand\nDataFrameGroupBy\naggregations (e.g. âsumâ) with object-dtype columns no longer infer non-object dtypes for their results, explicitly call\nresult.infer_objects(copy=False)\non the result to obtain the old behavior (\nGH51205\n,\nGH49603\n)\nDivision by zero with\nArrowDtype\ndtypes returns\n-inf\n,\nnan\n, or\ninf\ndepending on the numerator, instead of raising (\nGH51541\n)\nAdded\npandas.api.types.is_any_real_numeric_dtype()\nto check for real numeric dtypes (\nGH51152\n)\nvalue_counts()\nnow returns data with\nArrowDtype\nwith\npyarrow.int64\ntype instead of\n\"Int64\"\ntype (\nGH51462\n)\nArrowExtensionArray\ncomparison methods now return data with\nArrowDtype\nwith\npyarrow.bool_\ntype instead of\n\"boolean\"\ndtype (\nGH51643\n)\nfactorize()\nand\nunique()\npreserve the original dtype when passed numpy timedelta64 or datetime64 with non-nanosecond resolution (\nGH48670\n)\nNote\nA current PDEP proposes the deprecation and removal of the keywords\ninplace\nand\ncopy\nfor all but a small subset of methods from the pandas API. The current discussion takes place\nat\nhere\n. The keywords wonât be necessary\nanymore in the context of Copy-on-Write. If this proposal is accepted, both\nkeywords would be deprecated in the next release of pandas and removed in pandas 3.0.\nDeprecations\n#\nDeprecated parsing datetime strings with system-local timezone to\ntzlocal\n, pass a\ntz\nkeyword or explicitly call\ntz_localize\ninstead (\nGH50791\n)\nDeprecated argument\ninfer_datetime_format\nin\nto_datetime()\nand\nread_csv()\n, as a strict version of it is now the default (\nGH48621\n)\nDeprecated behavior of\nto_datetime()\nwith\nunit\nwhen parsing strings, in a future version these will be parsed as datetimes (matching unit-less behavior) instead of cast to floats. To retain the old behavior, cast strings to numeric types before calling\nto_datetime()\n(\nGH50735\n)\nDeprecated\npandas.io.sql.execute()\n(\nGH50185\n)\nIndex.is_boolean()\nhas been deprecated. Use\npandas.api.types.is_bool_dtype()\ninstead (\nGH50042\n)\nIndex.is_integer()\nhas been deprecated. Use\npandas.api.types.is_integer_dtype()\ninstead (\nGH50042\n)\nIndex.is_floating()\nhas been deprecated. Use\npandas.api.types.is_float_dtype()\ninstead (\nGH50042\n)\nIndex.holds_integer()\nhas been deprecated. Use\npandas.api.types.infer_dtype()\ninstead (\nGH50243\n)\nIndex.is_numeric()\nhas been deprecated. Use\npandas.api.types.is_any_real_numeric_dtype()\ninstead (\nGH50042\n,:issue:\n51152\n)\nIndex.is_categorical()\nhas been deprecated. Use\npandas.api.types.is_categorical_dtype()\ninstead (\nGH50042\n)\nIndex.is_object()\nhas been deprecated. Use\npandas.api.types.is_object_dtype()\ninstead (\nGH50042\n)\nIndex.is_interval()\nhas been deprecated. Use\npandas.api.types.is_interval_dtype()\ninstead (\nGH50042\n)\nDeprecated argument\ndate_parser\nin\nread_csv()\n,\nread_table()\n,\nread_fwf()\n, and\nread_excel()\nin favour of\ndate_format\n(\nGH50601\n)\nDeprecated\nall\nand\nany\nreductions with\ndatetime64\nand\nDatetimeTZDtype\ndtypes, use e.g.\n(obj\n!=\npd.Timestamp(0),\ntz=obj.tz).all()\ninstead (\nGH34479\n)\nDeprecated unused arguments\n*args\nand\n**kwargs\nin\nResampler\n(\nGH50977\n)\nDeprecated calling\nfloat\nor\nint\non a single element\nSeries\nto return a\nfloat\nor\nint\nrespectively. Extract the element before calling\nfloat\nor\nint\ninstead (\nGH51101\n)\nDeprecated\nGrouper.groups()\n, use\nGroupby.groups()\ninstead (\nGH51182\n)\nDeprecated\nGrouper.grouper()\n, use\nGroupby.grouper()\ninstead (\nGH51182\n)\nDeprecated\nGrouper.obj()\n, use\nGroupby.obj()\ninstead (\nGH51206\n)\nDeprecated\nGrouper.indexer()\n, use\nResampler.indexer()\ninstead (\nGH51206\n)\nDeprecated\nGrouper.ax()\n, use\nResampler.ax()\ninstead (\nGH51206\n)\nDeprecated keyword\nuse_nullable_dtypes\nin\nread_parquet()\n, use\ndtype_backend\ninstead (\nGH51853\n)\nDeprecated\nSeries.pad()\nin favor of\nSeries.ffill()\n(\nGH33396\n)\nDeprecated\nSeries.backfill()\nin favor of\nSeries.bfill()\n(\nGH33396\n)\nDeprecated\nDataFrame.pad()\nin favor of\nDataFrame.ffill()\n(\nGH33396\n)\nDeprecated\nDataFrame.backfill()\nin favor of\nDataFrame.bfill()\n(\nGH33396\n)\nDeprecated\nclose()\n. Use\nStataReader\nas a context manager instead (\nGH49228\n)\nRemoval of prior version deprecations/changes\n#\nRemoved\nInt64Index\n,\nUInt64Index\nand\nFloat64Index\n. See also\nhere\nfor more information (\nGH42717\n)\nRemoved deprecated\nTimestamp.freq\n,\nTimestamp.freqstr\nand argument\nfreq\nfrom the\nTimestamp\nconstructor and\nTimestamp.fromordinal()\n(\nGH14146\n)\nRemoved deprecated\nCategoricalBlock\n,\nBlock.is_categorical()\n, require datetime64 and timedelta64 values to be wrapped in\nDatetimeArray\nor\nTimedeltaArray\nbefore passing to\nBlock.make_block_same_class()\n, require\nDatetimeTZBlock.values\nto have the correct ndim when passing to the\nBlockManager\nconstructor, and removed the âfastpathâ keyword from the\nSingleBlockManager\nconstructor (\nGH40226\n,\nGH40571\n)\nRemoved deprecated global option\nuse_inf_as_null\nin favor of\nuse_inf_as_na\n(\nGH17126\n)\nRemoved deprecated module\npandas.core.index\n(\nGH30193\n)\nRemoved deprecated alias\npandas.core.tools.datetimes.to_time\n, import the function directly from\npandas.core.tools.times\ninstead (\nGH34145\n)\nRemoved deprecated alias\npandas.io.json.json_normalize\n, import the function directly from\npandas.json_normalize\ninstead (\nGH27615\n)\nRemoved deprecated\nCategorical.to_dense()\n, use\nnp.asarray(cat)\ninstead (\nGH32639\n)\nRemoved deprecated\nCategorical.take_nd()\n(\nGH27745\n)\nRemoved deprecated\nCategorical.mode()\n, use\nSeries(cat).mode()\ninstead (\nGH45033\n)\nRemoved deprecated\nCategorical.is_dtype_equal()\nand\nCategoricalIndex.is_dtype_equal()\n(\nGH37545\n)\nRemoved deprecated\nCategoricalIndex.take_nd()\n(\nGH30702\n)\nRemoved deprecated\nIndex.is_type_compatible()\n(\nGH42113\n)\nRemoved deprecated\nIndex.is_mixed()\n, check\nindex.inferred_type\ndirectly instead (\nGH32922\n)\nRemoved deprecated\npandas.api.types.is_categorical()\n; use\npandas.api.types.is_categorical_dtype()\ninstead  (\nGH33385\n)\nRemoved deprecated\nIndex.asi8()\n(\nGH37877\n)\nEnforced deprecation changing behavior when passing\ndatetime64[ns]\ndtype data and timezone-aware dtype to\nSeries\n, interpreting the values as wall-times instead of UTC times, matching\nDatetimeIndex\nbehavior (\nGH41662\n)\nEnforced deprecation changing behavior when applying a numpy ufunc on multiple non-aligned (on the index or columns)\nDataFrame\nthat will now align the inputs first (\nGH39239\n)\nRemoved deprecated\nDataFrame._AXIS_NUMBERS()\n,\nDataFrame._AXIS_NAMES()\n,\nSeries._AXIS_NUMBERS()\n,\nSeries._AXIS_NAMES()\n(\nGH33637\n)\nRemoved deprecated\nIndex.to_native_types()\n, use\nobj.astype(str)\ninstead (\nGH36418\n)\nRemoved deprecated\nSeries.iteritems()\n,\nDataFrame.iteritems()\n, use\nobj.items\ninstead (\nGH45321\n)\nRemoved deprecated\nDataFrame.lookup()\n(\nGH35224\n)\nRemoved deprecated\nSeries.append()\n,\nDataFrame.append()\n, use\nconcat()\ninstead (\nGH35407\n)\nRemoved deprecated\nSeries.iteritems()\n,\nDataFrame.iteritems()\nand\nHDFStore.iteritems()\nuse\nobj.items\ninstead (\nGH45321\n)\nRemoved deprecated\nDatetimeIndex.union_many()\n(\nGH45018\n)\nRemoved deprecated\nweekofyear\nand\nweek\nattributes of\nDatetimeArray\n,\nDatetimeIndex\nand\ndt\naccessor in favor of\nisocalendar().week\n(\nGH33595\n)\nRemoved deprecated\nRangeIndex._start()\n,\nRangeIndex._stop()\n,\nRangeIndex._step()\n, use\nstart\n,\nstop\n,\nstep\ninstead (\nGH30482\n)\nRemoved deprecated\nDatetimeIndex.to_perioddelta()\n, Use\ndtindex\n-\ndtindex.to_period(freq).to_timestamp()\ninstead (\nGH34853\n)\nRemoved deprecated\nStyler.hide_index()\nand\nStyler.hide_columns()\n(\nGH49397\n)\nRemoved deprecated\nStyler.set_na_rep()\nand\nStyler.set_precision()\n(\nGH49397\n)\nRemoved deprecated\nStyler.where()\n(\nGH49397\n)\nRemoved deprecated\nStyler.render()\n(\nGH49397\n)\nRemoved deprecated argument\ncol_space\nin\nDataFrame.to_latex()\n(\nGH47970\n)\nRemoved deprecated argument\nnull_color\nin\nStyler.highlight_null()\n(\nGH49397\n)\nRemoved deprecated argument\ncheck_less_precise\nin\ntesting.assert_frame_equal()\n,\ntesting.assert_extension_array_equal()\n,\ntesting.assert_series_equal()\n,\ntesting.assert_index_equal()\n(\nGH30562\n)\nRemoved deprecated\nnull_counts\nargument in\nDataFrame.info()\n. Use\nshow_counts\ninstead (\nGH37999\n)\nRemoved deprecated\nIndex.is_monotonic()\n, and\nSeries.is_monotonic()\n; use\nobj.is_monotonic_increasing\ninstead (\nGH45422\n)\nRemoved deprecated\nIndex.is_all_dates()\n(\nGH36697\n)\nEnforced deprecation disallowing passing a timezone-aware\nTimestamp\nand\ndtype=\"datetime64[ns]\"\nto\nSeries\nor\nDataFrame\nconstructors (\nGH41555\n)\nEnforced deprecation disallowing passing a sequence of timezone-aware values and\ndtype=\"datetime64[ns]\"\nto to\nSeries\nor\nDataFrame\nconstructors (\nGH41555\n)\nEnforced deprecation disallowing\nnumpy.ma.mrecords.MaskedRecords\nin the\nDataFrame\nconstructor; pass\n\"{name:\ndata[name]\nfor\nname\nin\ndata.dtype.names}\ninstead (\nGH40363\n)\nEnforced deprecation disallowing unit-less âdatetime64â dtype in\nSeries.astype()\nand\nDataFrame.astype()\n(\nGH47844\n)\nEnforced deprecation disallowing using\n.astype\nto convert a\ndatetime64[ns]\nSeries\n,\nDataFrame\n, or\nDatetimeIndex\nto timezone-aware dtype, use\nobj.tz_localize\nor\nser.dt.tz_localize\ninstead (\nGH39258\n)\nEnforced deprecation disallowing using\n.astype\nto convert a timezone-aware\nSeries\n,\nDataFrame\n, or\nDatetimeIndex\nto timezone-naive\ndatetime64[ns]\ndtype, use\nobj.tz_localize(None)\nor\nobj.tz_convert(\"UTC\").tz_localize(None)\ninstead (\nGH39258\n)\nEnforced deprecation disallowing passing non boolean argument to sort in\nconcat()\n(\nGH44629\n)\nRemoved Date parser functions\nparse_date_time()\n,\nparse_date_fields()\n,\nparse_all_fields()\nand\ngeneric_parser()\n(\nGH24518\n)\nRemoved argument\nindex\nfrom the\ncore.arrays.SparseArray\nconstructor (\nGH43523\n)\nRemove argument\nsqueeze\nfrom\nDataFrame.groupby()\nand\nSeries.groupby()\n(\nGH32380\n)\nRemoved deprecated\napply\n,\napply_index\n,\n__call__\n,\nonOffset\n, and\nisAnchored\nattributes from\nDateOffset\n(\nGH34171\n)\nRemoved\nkeep_tz\nargument in\nDatetimeIndex.to_series()\n(\nGH29731\n)\nRemove arguments\nnames\nand\ndtype\nfrom\nIndex.copy()\nand\nlevels\nand\ncodes\nfrom\nMultiIndex.copy()\n(\nGH35853\n,\nGH36685\n)\nRemove argument\ninplace\nfrom\nMultiIndex.set_levels()\nand\nMultiIndex.set_codes()\n(\nGH35626\n)\nRemoved arguments\nverbose\nand\nencoding\nfrom\nDataFrame.to_excel()\nand\nSeries.to_excel()\n(\nGH47912\n)\nRemoved argument\nline_terminator\nfrom\nDataFrame.to_csv()\nand\nSeries.to_csv()\n, use\nlineterminator\ninstead (\nGH45302\n)\nRemoved argument\ninplace\nfrom\nDataFrame.set_axis()\nand\nSeries.set_axis()\n, use\nobj\n=\nobj.set_axis(...,\ncopy=False)\ninstead (\nGH48130\n)\nDisallow passing positional arguments to\nMultiIndex.set_levels()\nand\nMultiIndex.set_codes()\n(\nGH41485\n)\nDisallow parsing to Timedelta strings with components with units âYâ, âyâ, or âMâ, as these do not represent unambiguous durations (\nGH36838\n)\nRemoved\nMultiIndex.is_lexsorted()\nand\nMultiIndex.lexsort_depth()\n(\nGH38701\n)\nRemoved argument\nhow\nfrom\nPeriodIndex.astype()\n, use\nPeriodIndex.to_timestamp()\ninstead (\nGH37982\n)\nRemoved argument\ntry_cast\nfrom\nDataFrame.mask()\n,\nDataFrame.where()\n,\nSeries.mask()\nand\nSeries.where()\n(\nGH38836\n)\nRemoved argument\ntz\nfrom\nPeriod.to_timestamp()\n, use\nobj.to_timestamp(...).tz_localize(tz)\ninstead (\nGH34522\n)\nRemoved argument\nsort_columns\nin\nDataFrame.plot()\nand\nSeries.plot()\n(\nGH47563\n)\nRemoved argument\nis_copy\nfrom\nDataFrame.take()\nand\nSeries.take()\n(\nGH30615\n)\nRemoved argument\nkind\nfrom\nIndex.get_slice_bound()\n,\nIndex.slice_indexer()\nand\nIndex.slice_locs()\n(\nGH41378\n)\nRemoved arguments\nprefix\n,\nsqueeze\n,\nerror_bad_lines\nand\nwarn_bad_lines\nfrom\nread_csv()\n(\nGH40413\n,\nGH43427\n)\nRemoved arguments\nsqueeze\nfrom\nread_excel()\n(\nGH43427\n)\nRemoved argument\ndatetime_is_numeric\nfrom\nDataFrame.describe()\nand\nSeries.describe()\nas datetime data will always be summarized as numeric data (\nGH34798\n)\nDisallow passing list\nkey\nto\nSeries.xs()\nand\nDataFrame.xs()\n, pass a tuple instead (\nGH41789\n)\nDisallow subclass-specific keywords (e.g. âfreqâ, âtzâ, ânamesâ, âclosedâ) in the\nIndex\nconstructor (\nGH38597\n)\nRemoved argument\ninplace\nfrom\nCategorical.remove_unused_categories()\n(\nGH37918\n)\nDisallow passing non-round floats to\nTimestamp\nwith\nunit=\"M\"\nor\nunit=\"Y\"\n(\nGH47266\n)\nRemove keywords\nconvert_float\nand\nmangle_dupe_cols\nfrom\nread_excel()\n(\nGH41176\n)\nRemove keyword\nmangle_dupe_cols\nfrom\nread_csv()\nand\nread_table()\n(\nGH48137\n)\nRemoved\nerrors\nkeyword from\nDataFrame.where()\n,\nSeries.where()\n,\nDataFrame.mask()\nand\nSeries.mask()\n(\nGH47728\n)\nDisallow passing non-keyword arguments to\nread_excel()\nexcept\nio\nand\nsheet_name\n(\nGH34418\n)\nDisallow passing non-keyword arguments to\nDataFrame.drop()\nand\nSeries.drop()\nexcept\nlabels\n(\nGH41486\n)\nDisallow passing non-keyword arguments to\nDataFrame.fillna()\nand\nSeries.fillna()\nexcept\nvalue\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\nStringMethods.split()\nand\nStringMethods.rsplit()\nexcept for\npat\n(\nGH47448\n)\nDisallow passing non-keyword arguments to\nDataFrame.set_index()\nexcept\nkeys\n(\nGH41495\n)\nDisallow passing non-keyword arguments to\nResampler.interpolate()\nexcept\nmethod\n(\nGH41699\n)\nDisallow passing non-keyword arguments to\nDataFrame.reset_index()\nand\nSeries.reset_index()\nexcept\nlevel\n(\nGH41496\n)\nDisallow passing non-keyword arguments to\nDataFrame.dropna()\nand\nSeries.dropna()\n(\nGH41504\n)\nDisallow passing non-keyword arguments to\nExtensionArray.argsort()\n(\nGH46134\n)\nDisallow passing non-keyword arguments to\nCategorical.sort_values()\n(\nGH47618\n)\nDisallow passing non-keyword arguments to\nIndex.drop_duplicates()\nand\nSeries.drop_duplicates()\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\nDataFrame.drop_duplicates()\nexcept for\nsubset\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\nDataFrame.sort_index()\nand\nSeries.sort_index()\n(\nGH41506\n)\nDisallow passing non-keyword arguments to\nDataFrame.interpolate()\nand\nSeries.interpolate()\nexcept for\nmethod\n(\nGH41510\n)\nDisallow passing non-keyword arguments to\nDataFrame.any()\nand\nSeries.any()\n(\nGH44896\n)\nDisallow passing non-keyword arguments to\nIndex.set_names()\nexcept for\nnames\n(\nGH41551\n)\nDisallow passing non-keyword arguments to\nIndex.join()\nexcept for\nother\n(\nGH46518\n)\nDisallow passing non-keyword arguments to\nconcat()\nexcept for\nobjs\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\npivot()\nexcept for\ndata\n(\nGH48301\n)\nDisallow passing non-keyword arguments to\nDataFrame.pivot()\n(\nGH48301\n)\nDisallow passing non-keyword arguments to\nread_html()\nexcept for\nio\n(\nGH27573\n)\nDisallow passing non-keyword arguments to\nread_json()\nexcept for\npath_or_buf\n(\nGH27573\n)\nDisallow passing non-keyword arguments to\nread_sas()\nexcept for\nfilepath_or_buffer\n(\nGH47154\n)\nDisallow passing non-keyword arguments to\nread_stata()\nexcept for\nfilepath_or_buffer\n(\nGH48128\n)\nDisallow passing non-keyword arguments to\nread_csv()\nexcept\nfilepath_or_buffer\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\nread_table()\nexcept\nfilepath_or_buffer\n(\nGH41485\n)\nDisallow passing non-keyword arguments to\nread_fwf()\nexcept\nfilepath_or_buffer\n(\nGH44710\n)\nDisallow passing non-keyword arguments to\nread_xml()\nexcept for\npath_or_buffer\n(\nGH45133\n)\nDisallow passing non-keyword arguments to\nSeries.mask()\nand\nDataFrame.mask()\nexcept\ncond\nand\nother\n(\nGH41580\n)\nDisallow passing non-keyword arguments to\nDataFrame.to_stata()\nexcept for\npath\n(\nGH48128\n)\nDisallow passing non-keyword arguments to\nDataFrame.where()\nand\nSeries.where()\nexcept for\ncond\nand\nother\n(\nGH41523\n)\nDisallow passing non-keyword arguments to\nSeries.set_axis()\nand\nDataFrame.set_axis()\nexcept for\nlabels\n(\nGH41491\n)\nDisallow passing non-keyword arguments to\nSeries.rename_axis()\nand\nDataFrame.rename_axis()\nexcept for\nmapper\n(\nGH47587\n)\nDisallow passing non-keyword arguments to\nSeries.clip()\nand\nDataFrame.clip()\nexcept\nlower\nand\nupper\n(\nGH41511\n)\nDisallow passing non-keyword arguments to\nSeries.bfill()\n,\nSeries.ffill()\n,\nDataFrame.bfill()\nand\nDataFrame.ffill()\n(\nGH41508\n)\nDisallow passing non-keyword arguments to\nDataFrame.replace()\n,\nSeries.replace()\nexcept for\nto_replace\nand\nvalue\n(\nGH47587\n)\nDisallow passing non-keyword arguments to\nDataFrame.sort_values()\nexcept for\nby\n(\nGH41505\n)\nDisallow passing non-keyword arguments to\nSeries.sort_values()\n(\nGH41505\n)\nDisallow passing non-keyword arguments to\nDataFrame.reindex()\nexcept for\nlabels\n(\nGH17966\n)\nDisallow\nIndex.reindex()\nwith non-unique\nIndex\nobjects (\nGH42568\n)\nDisallowed constructing\nCategorical\nwith scalar\ndata\n(\nGH38433\n)\nDisallowed constructing\nCategoricalIndex\nwithout passing\ndata\n(\nGH38944\n)\nRemoved\nRolling.validate()\n,\nExpanding.validate()\n, and\nExponentialMovingWindow.validate()\n(\nGH43665\n)\nRemoved\nRolling.win_type\nreturning\n\"freq\"\n(\nGH38963\n)\nRemoved\nRolling.is_datetimelike\n(\nGH38963\n)\nRemoved the\nlevel\nkeyword in\nDataFrame\nand\nSeries\naggregations; use\ngroupby\ninstead (\nGH39983\n)\nRemoved deprecated\nTimedelta.delta()\n,\nTimedelta.is_populated()\n, and\nTimedelta.freq\n(\nGH46430\n,\nGH46476\n)\nRemoved deprecated\nNaT.freq\n(\nGH45071\n)\nRemoved deprecated\nCategorical.replace()\n, use\nSeries.replace()\ninstead (\nGH44929\n)\nRemoved the\nnumeric_only\nkeyword from\nCategorical.min()\nand\nCategorical.max()\nin favor of\nskipna\n(\nGH48821\n)\nChanged behavior of\nDataFrame.median()\nand\nDataFrame.mean()\nwith\nnumeric_only=None\nto not exclude datetime-like columns THIS NOTE WILL BE IRRELEVANT ONCE\nnumeric_only=None\nDEPRECATION IS ENFORCED (\nGH29941\n)\nRemoved\nis_extension_type()\nin favor of\nis_extension_array_dtype()\n(\nGH29457\n)\nRemoved\n.ExponentialMovingWindow.vol\n(\nGH39220\n)\nRemoved\nIndex.get_value()\nand\nIndex.set_value()\n(\nGH33907\n,\nGH28621\n)\nRemoved\nSeries.slice_shift()\nand\nDataFrame.slice_shift()\n(\nGH37601\n)\nRemove\nDataFrameGroupBy.pad()\nand\nDataFrameGroupBy.backfill()\n(\nGH45076\n)\nRemove\nnumpy\nargument from\nread_json()\n(\nGH30636\n)\nDisallow passing abbreviations for\norient\nin\nDataFrame.to_dict()\n(\nGH32516\n)\nDisallow partial slicing on an non-monotonic\nDatetimeIndex\nwith keys which are not in Index. This now raises a\nKeyError\n(\nGH18531\n)\nRemoved\nget_offset\nin favor of\nto_offset()\n(\nGH30340\n)\nRemoved the\nwarn\nkeyword in\ninfer_freq()\n(\nGH45947\n)\nRemoved the\ninclude_start\nand\ninclude_end\narguments in\nDataFrame.between_time()\nin favor of\ninclusive\n(\nGH43248\n)\nRemoved the\nclosed\nargument in\ndate_range()\nand\nbdate_range()\nin favor of\ninclusive\nargument (\nGH40245\n)\nRemoved the\ncenter\nkeyword in\nDataFrame.expanding()\n(\nGH20647\n)\nRemoved the\ntruediv\nkeyword from\neval()\n(\nGH29812\n)\nRemoved the\nmethod\nand\ntolerance\narguments in\nIndex.get_loc()\n. Use\nindex.get_indexer([label],\nmethod=...,\ntolerance=...)\ninstead (\nGH42269\n)\nRemoved the\npandas.datetime\nsubmodule (\nGH30489\n)\nRemoved the\npandas.np\nsubmodule (\nGH30296\n)\nRemoved\npandas.util.testing\nin favor of\npandas.testing\n(\nGH30745\n)\nRemoved\nSeries.str.__iter__()\n(\nGH28277\n)\nRemoved\npandas.SparseArray\nin favor of\narrays.SparseArray\n(\nGH30642\n)\nRemoved\npandas.SparseSeries\nand\npandas.SparseDataFrame\n, including pickle support. (\nGH30642\n)\nEnforced disallowing passing an integer\nfill_value\nto\nDataFrame.shift()\nand\nSeries.shift`()\nwith datetime64, timedelta64, or period dtypes (\nGH32591\n)\nEnforced disallowing a string column label into\ntimes\nin\nDataFrame.ewm()\n(\nGH43265\n)\nEnforced disallowing passing\nTrue\nand\nFalse\ninto\ninclusive\nin\nSeries.between()\nin favor of\n\"both\"\nand\n\"neither\"\nrespectively (\nGH40628\n)\nEnforced disallowing using\nusecols\nwith out of bounds indices for\nread_csv\nwith\nengine=\"c\"\n(\nGH25623\n)\nEnforced disallowing the use of\n**kwargs\nin\nExcelWriter\n; use the keyword argument\nengine_kwargs\ninstead (\nGH40430\n)\nEnforced disallowing a tuple of column labels into\nDataFrameGroupBy.__getitem__()\n(\nGH30546\n)\nEnforced disallowing missing labels when indexing with a sequence of labels on a level of a\nMultiIndex\n. This now raises a\nKeyError\n(\nGH42351\n)\nEnforced disallowing setting values with\n.loc\nusing a positional slice. Use\n.loc\nwith labels or\n.iloc\nwith positions instead (\nGH31840\n)\nEnforced disallowing positional indexing with a\nfloat\nkey even if that key is a round number, manually cast to integer instead (\nGH34193\n)\nEnforced disallowing using a\nDataFrame\nindexer with\n.iloc\n, use\n.loc\ninstead for automatic alignment (\nGH39022\n)\nEnforced disallowing\nset\nor\ndict\nindexers in\n__getitem__\nand\n__setitem__\nmethods (\nGH42825\n)\nEnforced disallowing indexing on a\nIndex\nor positional indexing on a\nSeries\nproducing multi-dimensional objects e.g.\nobj[:,\nNone]\n, convert to numpy before indexing instead (\nGH35141\n)\nEnforced disallowing\ndict\nor\nset\nobjects in\nsuffixes\nin\nmerge()\n(\nGH34810\n)\nEnforced disallowing\nmerge()\nto produce duplicated columns through the\nsuffixes\nkeyword and already existing columns (\nGH22818\n)\nEnforced disallowing using\nmerge()\nor\njoin()\non a different number of levels (\nGH34862\n)\nEnforced disallowing\nvalue_name\nargument in\nDataFrame.melt()\nto match an element in the\nDataFrame\ncolumns (\nGH35003\n)\nEnforced disallowing passing\nshowindex\ninto\n**kwargs\nin\nDataFrame.to_markdown()\nand\nSeries.to_markdown()\nin favor of\nindex\n(\nGH33091\n)\nRemoved setting Categorical._codes directly (\nGH41429\n)\nRemoved setting Categorical.categories directly (\nGH47834\n)\nRemoved argument\ninplace\nfrom\nCategorical.add_categories()\n,\nCategorical.remove_categories()\n,\nCategorical.set_categories()\n,\nCategorical.rename_categories()\n,\nCategorical.reorder_categories()\n,\nCategorical.set_ordered()\n,\nCategorical.as_ordered()\n,\nCategorical.as_unordered()\n(\nGH37981\n,\nGH41118\n,\nGH41133\n,\nGH47834\n)\nEnforced\nRolling.count()\nwith\nmin_periods=None\nto default to the size of the window (\nGH31302\n)\nRenamed\nfname\nto\npath\nin\nDataFrame.to_parquet()\n,\nDataFrame.to_stata()\nand\nDataFrame.to_feather()\n(\nGH30338\n)\nEnforced disallowing indexing a\nSeries\nwith a single item list with a slice (e.g.\nser[[slice(0,\n2)]]\n). Either convert the list to tuple, or pass the slice directly instead (\nGH31333\n)\nChanged behavior indexing on a\nDataFrame\nwith a\nDatetimeIndex\nindex using a string indexer, previously this operated as a slice on rows, now it operates like any other column key; use\nframe.loc[key]\nfor the old behavior (\nGH36179\n)\nEnforced the\ndisplay.max_colwidth\noption to not accept negative integers (\nGH31569\n)\nRemoved the\ndisplay.column_space\noption in favor of\ndf.to_string(col_space=...)\n(\nGH47280\n)\nRemoved the deprecated method\nmad\nfrom pandas classes (\nGH11787\n)\nRemoved the deprecated method\ntshift\nfrom pandas classes (\nGH11631\n)\nChanged behavior of empty data passed into\nSeries\n; the default dtype will be\nobject\ninstead of\nfloat64\n(\nGH29405\n)\nChanged the behavior of\nDatetimeIndex.union()\n,\nDatetimeIndex.intersection()\n, and\nDatetimeIndex.symmetric_difference()\nwith mismatched timezones to convert to UTC instead of casting to object dtype (\nGH39328\n)\nChanged the behavior of\nto_datetime()\nwith argument ânowâ with\nutc=False\nto match\nTimestamp(\"now\")\n(\nGH18705\n)\nChanged the behavior of indexing on a timezone-aware\nDatetimeIndex\nwith a timezone-naive\ndatetime\nobject or vice-versa; these now behave like any other non-comparable type by raising\nKeyError\n(\nGH36148\n)\nChanged the behavior of\nIndex.reindex()\n,\nSeries.reindex()\n, and\nDataFrame.reindex()\nwith a\ndatetime64\ndtype and a\ndatetime.date\nobject for\nfill_value\n; these are no longer considered equivalent to\ndatetime.datetime\nobjects so the reindex casts to object dtype (\nGH39767\n)\nChanged behavior of\nSparseArray.astype()\nwhen given a dtype that is not explicitly\nSparseDtype\n, cast to the exact requested dtype rather than silently using a\nSparseDtype\ninstead (\nGH34457\n)\nChanged behavior of\nIndex.ravel()\nto return a view on the original\nIndex\ninstead of a\nnp.ndarray\n(\nGH36900\n)\nChanged behavior of\nSeries.to_frame()\nand\nIndex.to_frame()\nwith explicit\nname=None\nto use\nNone\nfor the column name instead of the indexâs name or default\n0\n(\nGH45523\n)\nChanged behavior of\nconcat()\nwith one array of\nbool\n-dtype and another of integer dtype, this now returns\nobject\ndtype instead of integer dtype; explicitly cast the bool object to integer before concatenating to get the old behavior (\nGH45101\n)\nChanged behavior of\nDataFrame\nconstructor given floating-point\ndata\nand an integer\ndtype\n, when the data cannot be cast losslessly, the floating point dtype is retained, matching\nSeries\nbehavior (\nGH41170\n)\nChanged behavior of\nIndex\nconstructor when given a\nnp.ndarray\nwith object-dtype containing numeric entries; this now retains object dtype rather than inferring a numeric dtype, consistent with\nSeries\nbehavior (\nGH42870\n)\nChanged behavior of\nIndex.__and__()\n,\nIndex.__or__()\nand\nIndex.__xor__()\nto behave as logical operations (matching\nSeries\nbehavior) instead of aliases for set operations (\nGH37374\n)\nChanged behavior of\nDataFrame\nconstructor when passed a list whose first element is a\nCategorical\n, this now treats the elements as rows casting to\nobject\ndtype, consistent with behavior for other types (\nGH38845\n)\nChanged behavior of\nDataFrame\nconstructor when passed a\ndtype\n(other than int) that the data cannot be cast to; it now raises instead of silently ignoring the dtype (\nGH41733\n)\nChanged the behavior of\nSeries\nconstructor, it will no longer infer a datetime64 or timedelta64 dtype from string entries (\nGH41731\n)\nChanged behavior of\nTimestamp\nconstructor with a\nnp.datetime64\nobject and a\ntz\npassed to interpret the input as a wall-time as opposed to a UTC time (\nGH42288\n)\nChanged behavior of\nTimestamp.utcfromtimestamp()\nto return a timezone-aware object satisfying\nTimestamp.utcfromtimestamp(val).timestamp()\n==\nval\n(\nGH45083\n)\nChanged behavior of\nIndex\nconstructor when passed a\nSparseArray\nor\nSparseDtype\nto retain that dtype instead of casting to\nnumpy.ndarray\n(\nGH43930\n)\nChanged behavior of setitem-like operations (\n__setitem__\n,\nfillna\n,\nwhere\n,\nmask\n,\nreplace\n,\ninsert\n, fill_value for\nshift\n) on an object with\nDatetimeTZDtype\nwhen using a value with a non-matching timezone, the value will be cast to the objectâs timezone instead of casting both to object-dtype (\nGH44243\n)\nChanged behavior of\nIndex\n,\nSeries\n,\nDataFrame\nconstructors with floating-dtype data and a\nDatetimeTZDtype\n, the data are now interpreted as UTC-times instead of wall-times, consistent with how integer-dtype data are treated (\nGH45573\n)\nChanged behavior of\nSeries\nand\nDataFrame\nconstructors with integer dtype and floating-point data containing\nNaN\n, this now raises\nIntCastingNaNError\n(\nGH40110\n)\nChanged behavior of\nSeries\nand\nDataFrame\nconstructors with an integer\ndtype\nand values that are too large to losslessly cast to this dtype, this now raises\nValueError\n(\nGH41734\n)\nChanged behavior of\nSeries\nand\nDataFrame\nconstructors with an integer\ndtype\nand values having either\ndatetime64\nor\ntimedelta64\ndtypes, this now raises\nTypeError\n, use\nvalues.view(\"int64\")\ninstead (\nGH41770\n)\nRemoved the deprecated\nbase\nand\nloffset\narguments from\npandas.DataFrame.resample()\n,\npandas.Series.resample()\nand\npandas.Grouper\n. Use\noffset\nor\norigin\ninstead (\nGH31809\n)\nChanged behavior of\nSeries.fillna()\nand\nDataFrame.fillna()\nwith\ntimedelta64[ns]\ndtype and an incompatible\nfill_value\n; this now casts to\nobject\ndtype instead of raising, consistent with the behavior with other dtypes (\nGH45746\n)\nChange the default argument of\nregex\nfor\nSeries.str.replace()\nfrom\nTrue\nto\nFalse\n. Additionally, a single character\npat\nwith\nregex=True\nis now treated as a regular expression instead of a string literal. (\nGH36695\n,\nGH24804\n)\nChanged behavior of\nDataFrame.any()\nand\nDataFrame.all()\nwith\nbool_only=True\n; object-dtype columns with all-bool values will no longer be included, manually cast to\nbool\ndtype first (\nGH46188\n)\nChanged behavior of\nDataFrame.max()\n,\nDataFrame.min\n,\nDataFrame.mean\n,\nDataFrame.median\n,\nDataFrame.skew\n,\nDataFrame.kurt\nwith\naxis=None\nto return a scalar applying the aggregation across both axes (\nGH45072\n)\nChanged behavior of comparison of a\nTimestamp\nwith a\ndatetime.date\nobject; these now compare as un-equal and raise on inequality comparisons, matching the\ndatetime.datetime\nbehavior (\nGH36131\n)\nChanged behavior of comparison of\nNaT\nwith a\ndatetime.date\nobject; these now raise on inequality comparisons (\nGH39196\n)\nEnforced deprecation of silently dropping columns that raised a\nTypeError\nin\nSeries.transform\nand\nDataFrame.transform\nwhen used with a list or dictionary (\nGH43740\n)\nChanged behavior of\nDataFrame.apply()\nwith list-like so that any partial failure will raise an error (\nGH43740\n)\nChanged behaviour of\nDataFrame.to_latex()\nto now use the Styler implementation via\nStyler.to_latex()\n(\nGH47970\n)\nChanged behavior of\nSeries.__setitem__()\nwith an integer key and a\nFloat64Index\nwhen the key is not present in the index; previously we treated the key as positional (behaving like\nseries.iloc[key]\n=\nval\n), now we treat it is a label (behaving like\nseries.loc[key]\n=\nval\n), consistent with\nSeries.__getitem__`()\nbehavior (\nGH33469\n)\nRemoved\nna_sentinel\nargument from\nfactorize()\n,\nIndex.factorize()\n, and\nExtensionArray.factorize()\n(\nGH47157\n)\nChanged behavior of\nSeries.diff()\nand\nDataFrame.diff()\nwith\nExtensionDtype\ndtypes whose arrays do not implement\ndiff\n, these now raise\nTypeError\nrather than casting to numpy (\nGH31025\n)\nEnforced deprecation of calling numpy âufuncâs on\nDataFrame\nwith\nmethod=\"outer\"\n; this now raises\nNotImplementedError\n(\nGH36955\n)\nEnforced deprecation disallowing passing\nnumeric_only=True\nto\nSeries\nreductions (\nrank\n,\nany\n,\nall\n, â¦) with non-numeric dtype (\nGH47500\n)\nChanged behavior of\nDataFrameGroupBy.apply()\nand\nSeriesGroupBy.apply()\nso that\ngroup_keys\nis respected even if a transformer is detected (\nGH34998\n)\nComparisons between a\nDataFrame\nand a\nSeries\nwhere the frameâs columns do not match the seriesâs index raise\nValueError\ninstead of automatically aligning, do\nleft,\nright\n=\nleft.align(right,\naxis=1,\ncopy=False)\nbefore comparing (\nGH36795\n)\nEnforced deprecation\nnumeric_only=None\n(the default) in DataFrame reductions that would silently drop columns that raised;\nnumeric_only\nnow defaults to\nFalse\n(\nGH41480\n)\nChanged default of\nnumeric_only\nto\nFalse\nin all DataFrame methods with that argument (\nGH46096\n,\nGH46906\n)\nChanged default of\nnumeric_only\nto\nFalse\nin\nSeries.rank()\n(\nGH47561\n)\nEnforced deprecation of silently dropping nuisance columns in groupby and resample operations when\nnumeric_only=False\n(\nGH41475\n)\nEnforced deprecation of silently dropping nuisance columns in\nRolling\n,\nExpanding\n, and\nExponentialMovingWindow\nops. This will now raise a\nerrors.DataError\n(\nGH42834\n)\nChanged behavior in setting values with\ndf.loc[:,\nfoo]\n=\nbar\nor\ndf.iloc[:,\nfoo]\n=\nbar\n, these now always attempt to set values inplace before falling back to casting (\nGH45333\n)\nChanged default of\nnumeric_only\nin various\nDataFrameGroupBy\nmethods; all methods now default to\nnumeric_only=False\n(\nGH46072\n)\nChanged default of\nnumeric_only\nto\nFalse\nin\nResampler\nmethods (\nGH47177\n)\nUsing the method\nDataFrameGroupBy.transform()\nwith a callable that returns DataFrames will align to the inputâs index (\nGH47244\n)\nWhen providing a list of columns of length one to\nDataFrame.groupby()\n, the keys that are returned by iterating over the resulting\nDataFrameGroupBy\nobject will now be tuples of length one (\nGH47761\n)\nRemoved deprecated methods\nExcelWriter.write_cells()\n,\nExcelWriter.save()\n,\nExcelWriter.cur_sheet()\n,\nExcelWriter.handles()\n,\nExcelWriter.path()\n(\nGH45795\n)\nThe\nExcelWriter\nattribute\nbook\ncan no longer be set; it is still available to be accessed and mutated (\nGH48943\n)\nRemoved unused\n*args\nand\n**kwargs\nin\nRolling\n,\nExpanding\n, and\nExponentialMovingWindow\nops (\nGH47851\n)\nRemoved the deprecated argument\nline_terminator\nfrom\nDataFrame.to_csv()\n(\nGH45302\n)\nRemoved the deprecated argument\nlabel\nfrom\nlreshape()\n(\nGH30219\n)\nArguments after\nexpr\nin\nDataFrame.eval()\nand\nDataFrame.query()\nare keyword-only (\nGH47587\n)\nRemoved\nIndex._get_attributes_dict()\n(\nGH50648\n)\nRemoved\nSeries.__array_wrap__()\n(\nGH50648\n)\nChanged behavior of\nDataFrame.value_counts()\nto return a\nSeries\nwith\nMultiIndex\nfor any list-like(one element or not) but an\nIndex\nfor a single label (\nGH50829\n)\nPerformance improvements\n#\nPerformance improvement in\nDataFrameGroupBy.median()\nand\nSeriesGroupBy.median()\nand\nDataFrameGroupBy.cumprod()\nfor nullable dtypes (\nGH37493\n)\nPerformance improvement in\nDataFrameGroupBy.all()\n,\nDataFrameGroupBy.any()\n,\nSeriesGroupBy.all()\n, and\nSeriesGroupBy.any()\nfor object dtype (\nGH50623\n)\nPerformance improvement in\nMultiIndex.argsort()\nand\nMultiIndex.sort_values()\n(\nGH48406\n)\nPerformance improvement in\nMultiIndex.size()\n(\nGH48723\n)\nPerformance improvement in\nMultiIndex.union()\nwithout missing values and without duplicates (\nGH48505\n,\nGH48752\n)\nPerformance improvement in\nMultiIndex.difference()\n(\nGH48606\n)\nPerformance improvement in\nMultiIndex\nset operations with sort=None (\nGH49010\n)\nPerformance improvement in\nDataFrameGroupBy.mean()\n,\nSeriesGroupBy.mean()\n,\nDataFrameGroupBy.var()\n, and\nSeriesGroupBy.var()\nfor extension array dtypes (\nGH37493\n)\nPerformance improvement in\nMultiIndex.isin()\nwhen\nlevel=None\n(\nGH48622\n,\nGH49577\n)\nPerformance improvement in\nMultiIndex.putmask()\n(\nGH49830\n)\nPerformance improvement in\nIndex.union()\nand\nMultiIndex.union()\nwhen index contains duplicates (\nGH48900\n)\nPerformance improvement in\nSeries.rank()\nfor pyarrow-backed dtypes (\nGH50264\n)\nPerformance improvement in\nSeries.searchsorted()\nfor pyarrow-backed dtypes (\nGH50447\n)\nPerformance improvement in\nSeries.fillna()\nfor extension array dtypes (\nGH49722\n,\nGH50078\n)\nPerformance improvement in\nIndex.join()\n,\nIndex.intersection()\nand\nIndex.union()\nfor masked and arrow dtypes when\nIndex\nis monotonic (\nGH50310\n,\nGH51365\n)\nPerformance improvement for\nSeries.value_counts()\nwith nullable dtype (\nGH48338\n)\nPerformance improvement for\nSeries\nconstructor passing integer numpy array with nullable dtype (\nGH48338\n)\nPerformance improvement for\nDatetimeIndex\nconstructor passing a list (\nGH48609\n)\nPerformance improvement in\nmerge()\nand\nDataFrame.join()\nwhen joining on a sorted\nMultiIndex\n(\nGH48504\n)\nPerformance improvement in\nto_datetime()\nwhen parsing strings with timezone offsets (\nGH50107\n)\nPerformance improvement in\nDataFrame.loc()\nand\nSeries.loc()\nfor tuple-based indexing of a\nMultiIndex\n(\nGH48384\n)\nPerformance improvement for\nSeries.replace()\nwith categorical dtype (\nGH49404\n)\nPerformance improvement for\nMultiIndex.unique()\n(\nGH48335\n)\nPerformance improvement for indexing operations with nullable and arrow dtypes (\nGH49420\n,\nGH51316\n)\nPerformance improvement for\nconcat()\nwith extension array backed indexes (\nGH49128\n,\nGH49178\n)\nPerformance improvement for\napi.types.infer_dtype()\n(\nGH51054\n)\nReduce memory usage of\nDataFrame.to_pickle()\n/\nSeries.to_pickle()\nwhen using BZ2 or LZMA (\nGH49068\n)\nPerformance improvement for\nStringArray\nconstructor passing a numpy array with type\nnp.str_\n(\nGH49109\n)\nPerformance improvement in\nfrom_tuples()\n(\nGH50620\n)\nPerformance improvement in\nfactorize()\n(\nGH49177\n)\nPerformance improvement in\n__setitem__()\n(\nGH50248\n,\nGH50632\n)\nPerformance improvement in\nArrowExtensionArray\ncomparison methods when array contains NA (\nGH50524\n)\nPerformance improvement in\nto_numpy()\n(\nGH49973\n,\nGH51227\n)\nPerformance improvement when parsing strings to\nBooleanDtype\n(\nGH50613\n)\nPerformance improvement in\nDataFrame.join()\nwhen joining on a subset of a\nMultiIndex\n(\nGH48611\n)\nPerformance improvement for\nMultiIndex.intersection()\n(\nGH48604\n)\nPerformance improvement in\nDataFrame.__setitem__()\n(\nGH46267\n)\nPerformance improvement in\nvar\nand\nstd\nfor nullable dtypes (\nGH48379\n).\nPerformance improvement when iterating over pyarrow and nullable dtypes (\nGH49825\n,\nGH49851\n)\nPerformance improvements to\nread_sas()\n(\nGH47403\n,\nGH47405\n,\nGH47656\n,\nGH48502\n)\nMemory improvement in\nRangeIndex.sort_values()\n(\nGH48801\n)\nPerformance improvement in\nSeries.to_numpy()\nif\ncopy=True\nby avoiding copying twice (\nGH24345\n)\nPerformance improvement in\nSeries.rename()\nwith\nMultiIndex\n(\nGH21055\n)\nPerformance improvement in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwhen\nby\nis a categorical type and\nsort=False\n(\nGH48976\n)\nPerformance improvement in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwhen\nby\nis a categorical type and\nobserved=False\n(\nGH49596\n)\nPerformance improvement in\nread_stata()\nwith parameter\nindex_col\nset to\nNone\n(the default). Now the index will be a\nRangeIndex\ninstead of\nInt64Index\n(\nGH49745\n)\nPerformance improvement in\nmerge()\nwhen not merging on the index - the new index will now be\nRangeIndex\ninstead of\nInt64Index\n(\nGH49478\n)\nPerformance improvement in\nDataFrame.to_dict()\nand\nSeries.to_dict()\nwhen using any non-object dtypes (\nGH46470\n)\nPerformance improvement in\nread_html()\nwhen there are multiple tables (\nGH49929\n)\nPerformance improvement in\nPeriod\nconstructor when constructing from a string or integer (\nGH38312\n)\nPerformance improvement in\nto_datetime()\nwhen using\n'%Y%m%d'\nformat (\nGH17410\n)\nPerformance improvement in\nto_datetime()\nwhen format is given or can be inferred (\nGH50465\n)\nPerformance improvement in\nSeries.median()\nfor nullable dtypes (\nGH50838\n)\nPerformance improvement in\nread_csv()\nwhen passing\nto_datetime()\nlambda-function to\ndate_parser\nand inputs have mixed timezone offsetes (\nGH35296\n)\nPerformance improvement in\nisna()\nand\nisnull()\n(\nGH50658\n)\nPerformance improvement in\nSeriesGroupBy.value_counts()\nwith categorical dtype (\nGH46202\n)\nFixed a reference leak in\nread_hdf()\n(\nGH37441\n)\nFixed a memory leak in\nDataFrame.to_json()\nand\nSeries.to_json()\nwhen serializing datetimes and timedeltas (\nGH40443\n)\nDecreased memory usage in many\nDataFrameGroupBy\nmethods (\nGH51090\n)\nPerformance improvement in\nDataFrame.round()\nfor an integer\ndecimal\nparameter (\nGH17254\n)\nPerformance improvement in\nDataFrame.replace()\nand\nSeries.replace()\nwhen using a large dict for\nto_replace\n(\nGH6697\n)\nMemory improvement in\nStataReader\nwhen reading seekable files (\nGH48922\n)\nBug fixes\n#\nCategorical\n#\nBug in\nCategorical.set_categories()\nlosing dtype information (\nGH48812\n)\nBug in\nSeries.replace()\nwith categorical dtype when\nto_replace\nvalues overlap with new values (\nGH49404\n)\nBug in\nSeries.replace()\nwith categorical dtype losing nullable dtypes of underlying categories (\nGH49404\n)\nBug in\nDataFrame.groupby()\nand\nSeries.groupby()\nwould reorder categories when used as a grouper (\nGH48749\n)\nBug in\nCategorical\nconstructor when constructing from a\nCategorical\nobject and\ndtype=\"category\"\nlosing ordered-ness (\nGH49309\n)\nBug in\nSeriesGroupBy.min()\n,\nSeriesGroupBy.max()\n,\nDataFrameGroupBy.min()\n, and\nDataFrameGroupBy.max()\nwith unordered\nCategoricalDtype\nwith no groups failing to raise\nTypeError\n(\nGH51034\n)\nDatetimelike\n#\nBug in\npandas.infer_freq()\n, raising\nTypeError\nwhen inferred on\nRangeIndex\n(\nGH47084\n)\nBug in\nto_datetime()\nincorrectly raising\nOverflowError\nwith string arguments corresponding to large integers (\nGH50533\n)\nBug in\nto_datetime()\nwas raising on invalid offsets with\nerrors='coerce'\nand\ninfer_datetime_format=True\n(\nGH48633\n)\nBug in\nDatetimeIndex\nconstructor failing to raise when\ntz=None\nis explicitly specified in conjunction with timezone-aware\ndtype\nor data (\nGH48659\n)\nBug in subtracting a\ndatetime\nscalar from\nDatetimeIndex\nfailing to retain the original\nfreq\nattribute (\nGH48818\n)\nBug in\npandas.tseries.holiday.Holiday\nwhere a half-open date interval causes inconsistent return types from\nUSFederalHolidayCalendar.holidays()\n(\nGH49075\n)\nBug in rendering\nDatetimeIndex\nand\nSeries\nand\nDataFrame\nwith timezone-aware dtypes with\ndateutil\nor\nzoneinfo\ntimezones near daylight-savings transitions (\nGH49684\n)\nBug in\nto_datetime()\nwas raising\nValueError\nwhen parsing\nTimestamp\n,\ndatetime.datetime\n,\ndatetime.date\n, or\nnp.datetime64\nobjects when non-ISO8601\nformat\nwas passed (\nGH49298\n,\nGH50036\n)\nBug in\nto_datetime()\nwas raising\nValueError\nwhen parsing empty string and non-ISO8601 format was passed. Now, empty strings will be parsed as\nNaT\n, for compatibility with how is done for ISO8601 formats (\nGH50251\n)\nBug in\nTimestamp\nwas showing\nUserWarning\n, which was not actionable by users, when parsing non-ISO8601 delimited date strings (\nGH50232\n)\nBug in\nto_datetime()\nwas showing misleading\nValueError\nwhen parsing dates with format containing ISO week directive and ISO weekday directive (\nGH50308\n)\nBug in\nTimestamp.round()\nwhen the\nfreq\nargument has zero-duration (e.g. â0nsâ) returning incorrect results instead of raising (\nGH49737\n)\nBug in\nto_datetime()\nwas not raising\nValueError\nwhen invalid format was passed and\nerrors\nwas\n'ignore'\nor\n'coerce'\n(\nGH50266\n)\nBug in\nDateOffset\nwas throwing\nTypeError\nwhen constructing with milliseconds and another super-daily argument (\nGH49897\n)\nBug in\nto_datetime()\nwas not raising\nValueError\nwhen parsing string with decimal date with format\n'%Y%m%d'\n(\nGH50051\n)\nBug in\nto_datetime()\nwas not converting\nNone\nto\nNaT\nwhen parsing mixed-offset date strings with ISO8601 format (\nGH50071\n)\nBug in\nto_datetime()\nwas not returning input when parsing out-of-bounds date string with\nerrors='ignore'\nand\nformat='%Y%m%d'\n(\nGH14487\n)\nBug in\nto_datetime()\nwas converting timezone-naive\ndatetime.datetime\nto timezone-aware when parsing with timezone-aware strings, ISO8601 format, and\nutc=False\n(\nGH50254\n)\nBug in\nto_datetime()\nwas throwing\nValueError\nwhen parsing dates with ISO8601 format where some values were not zero-padded (\nGH21422\n)\nBug in\nto_datetime()\nwas giving incorrect results when using\nformat='%Y%m%d'\nand\nerrors='ignore'\n(\nGH26493\n)\nBug in\nto_datetime()\nwas failing to parse date strings\n'today'\nand\n'now'\nif\nformat\nwas not ISO8601 (\nGH50359\n)\nBug in\nTimestamp.utctimetuple()\nraising a\nTypeError\n(\nGH32174\n)\nBug in\nto_datetime()\nwas raising\nValueError\nwhen parsing mixed-offset\nTimestamp\nwith\nerrors='ignore'\n(\nGH50585\n)\nBug in\nto_datetime()\nwas incorrectly handling floating-point inputs within 1\nunit\nof the overflow boundaries (\nGH50183\n)\nBug in\nto_datetime()\nwith unit of âYâ or âMâ giving incorrect results, not matching pointwise\nTimestamp\nresults (\nGH50870\n)\nBug in\nSeries.interpolate()\nand\nDataFrame.interpolate()\nwith datetime or timedelta dtypes incorrectly raising\nValueError\n(\nGH11312\n)\nBug in\nto_datetime()\nwas not returning input with\nerrors='ignore'\nwhen input was out-of-bounds (\nGH50587\n)\nBug in\nDataFrame.from_records()\nwhen given a\nDataFrame\ninput with timezone-aware datetime64 columns incorrectly dropping the timezone-awareness (\nGH51162\n)\nBug in\nto_datetime()\nwas raising\ndecimal.InvalidOperation\nwhen parsing date strings with\nerrors='coerce'\n(\nGH51084\n)\nBug in\nto_datetime()\nwith both\nunit\nand\norigin\nspecified returning incorrect results (\nGH42624\n)\nBug in\nSeries.astype()\nand\nDataFrame.astype()\nwhen converting an object-dtype object containing timezone-aware datetimes or strings to\ndatetime64[ns]\nincorrectly localizing as UTC instead of raising\nTypeError\n(\nGH50140\n)\nBug in\nDataFrameGroupBy.quantile()\nand\nSeriesGroupBy.quantile()\nwith datetime or timedelta dtypes giving incorrect results for groups containing\nNaT\n(\nGH51373\n)\nBug in\nDataFrameGroupBy.quantile()\nand\nSeriesGroupBy.quantile()\nincorrectly raising with\nPeriodDtype\nor\nDatetimeTZDtype\n(\nGH51373\n)\nTimedelta\n#\nBug in\nto_timedelta()\nraising error when input has nullable dtype\nFloat64\n(\nGH48796\n)\nBug in\nTimedelta\nconstructor incorrectly raising instead of returning\nNaT\nwhen given a\nnp.timedelta64(\"nat\")\n(\nGH48898\n)\nBug in\nTimedelta\nconstructor failing to raise when passed both a\nTimedelta\nobject and keywords (e.g. days, seconds) (\nGH48898\n)\nBug in\nTimedelta\ncomparisons with very large\ndatetime.timedelta\nobjects incorrect raising\nOutOfBoundsTimedelta\n(\nGH49021\n)\nTimezones\n#\nBug in\nSeries.astype()\nand\nDataFrame.astype()\nwith object-dtype containing multiple timezone-aware\ndatetime\nobjects with heterogeneous timezones to a\nDatetimeTZDtype\nincorrectly raising (\nGH32581\n)\nBug in\nto_datetime()\nwas failing to parse date strings with timezone name when\nformat\nwas specified with\n%Z\n(\nGH49748\n)\nBetter error message when passing invalid values to\nambiguous\nparameter in\nTimestamp.tz_localize()\n(\nGH49565\n)\nBug in string parsing incorrectly allowing a\nTimestamp\nto be constructed with an invalid timezone, which would raise when trying to print (\nGH50668\n)\nNumeric\n#\nBug in\nDataFrame.add()\ncannot apply ufunc when inputs contain mixed DataFrame type and Series type (\nGH39853\n)\nBug in arithmetic operations on\nSeries\nnot propagating mask when combining masked dtypes and numpy dtypes (\nGH45810\n,\nGH42630\n)\nBug in\nDataFrame.sem()\nand\nSeries.sem()\nwhere an erroneous\nTypeError\nwould always raise when using data backed by an\nArrowDtype\n(\nGH49759\n)\nBug in\nSeries.__add__()\ncasting to object for list and masked\nSeries\n(\nGH22962\n)\nBug in\nmode()\nwhere\ndropna=False\nwas not respected when there was\nNA\nvalues (\nGH50982\n)\nBug in\nDataFrame.query()\nwith\nengine=\"numexpr\"\nand column names are\nmin\nor\nmax\nwould raise a\nTypeError\n(\nGH50937\n)\nBug in\nDataFrame.min()\nand\nDataFrame.max()\nwith tz-aware data containing\npd.NaT\nand\naxis=1\nwould return incorrect results (\nGH51242\n)\nConversion\n#\nBug in constructing\nSeries\nwith\nint64\ndtype from a string list raising instead of casting (\nGH44923\n)\nBug in constructing\nSeries\nwith masked dtype and boolean values with\nNA\nraising (\nGH42137\n)\nBug in\nDataFrame.eval()\nincorrectly raising an\nAttributeError\nwhen there are negative values in function call (\nGH46471\n)\nBug in\nSeries.convert_dtypes()\nnot converting dtype to nullable dtype when\nSeries\ncontains\nNA\nand has dtype\nobject\n(\nGH48791\n)\nBug where any\nExtensionDtype\nsubclass with\nkind=\"M\"\nwould be interpreted as a timezone type (\nGH34986\n)\nBug in\narrays.ArrowExtensionArray\nthat would raise\nNotImplementedError\nwhen passed a sequence of strings or binary (\nGH49172\n)\nBug in\nSeries.astype()\nraising\npyarrow.ArrowInvalid\nwhen converting from a non-pyarrow string dtype to a pyarrow numeric type (\nGH50430\n)\nBug in\nDataFrame.astype()\nmodifying input array inplace when converting to\nstring\nand\ncopy=False\n(\nGH51073\n)\nBug in\nSeries.to_numpy()\nconverting to NumPy array before applying\nna_value\n(\nGH48951\n)\nBug in\nDataFrame.astype()\nnot copying data when converting to pyarrow dtype (\nGH50984\n)\nBug in\nto_datetime()\nwas not respecting\nexact\nargument when\nformat\nwas an ISO8601 format (\nGH12649\n)\nBug in\nTimedeltaArray.astype()\nraising\nTypeError\nwhen converting to a pyarrow duration type (\nGH49795\n)\nBug in\nDataFrame.eval()\nand\nDataFrame.query()\nraising for extension array dtypes (\nGH29618\n,\nGH50261\n,\nGH31913\n)\nBug in\nSeries()\nnot copying data when created from\nIndex\nand\ndtype\nis equal to\ndtype\nfrom\nIndex\n(\nGH52008\n)\nStrings\n#\nBug in\npandas.api.types.is_string_dtype()\nthat would not return\nTrue\nfor\nStringDtype\nor\nArrowDtype\nwith\npyarrow.string()\n(\nGH15585\n)\nBug in converting string dtypes to âdatetime64[ns]â or âtimedelta64[ns]â incorrectly raising\nTypeError\n(\nGH36153\n)\nBug in setting values in a string-dtype column with an array, mutating the array as side effect when it contains missing values (\nGH51299\n)\nInterval\n#\nBug in\nIntervalIndex.is_overlapping()\nincorrect output if interval has duplicate left boundaries (\nGH49581\n)\nBug in\nSeries.infer_objects()\nfailing to infer\nIntervalDtype\nfor an object series of\nInterval\nobjects (\nGH50090\n)\nBug in\nSeries.shift()\nwith\nIntervalDtype\nand invalid null\nfill_value\nfailing to raise\nTypeError\n(\nGH51258\n)\nIndexing\n#\nBug in\nDataFrame.__setitem__()\nraising when indexer is a\nDataFrame\nwith\nboolean\ndtype (\nGH47125\n)\nBug in\nDataFrame.reindex()\nfilling with wrong values when indexing columns and index for\nuint\ndtypes (\nGH48184\n)\nBug in\nDataFrame.loc()\nwhen setting\nDataFrame\nwith different dtypes coercing values to single dtype (\nGH50467\n)\nBug in\nDataFrame.sort_values()\nwhere\nNone\nwas not returned when\nby\nis empty list and\ninplace=True\n(\nGH50643\n)\nBug in\nDataFrame.loc()\ncoercing dtypes when setting values with a list indexer (\nGH49159\n)\nBug in\nSeries.loc()\nraising error for out of bounds end of slice indexer (\nGH50161\n)\nBug in\nDataFrame.loc()\nraising\nValueError\nwith all\nFalse\nbool\nindexer and empty object (\nGH51450\n)\nBug in\nDataFrame.loc()\nraising\nValueError\nwith\nbool\nindexer and\nMultiIndex\n(\nGH47687\n)\nBug in\nDataFrame.loc()\nraising\nIndexError\nwhen setting values for a pyarrow-backed column with a non-scalar indexer (\nGH50085\n)\nBug in\nDataFrame.__getitem__()\n,\nSeries.__getitem__()\n,\nDataFrame.__setitem__()\nand\nSeries.__setitem__()\nwhen indexing on indexes with extension float dtypes (\nFloat64\n&\nFloat64\n) or complex dtypes using integers (\nGH51053\n)\nBug in\nDataFrame.loc()\nmodifying object when setting incompatible value with an empty indexer (\nGH45981\n)\nBug in\nDataFrame.__setitem__()\nraising\nValueError\nwhen right hand side is\nDataFrame\nwith\nMultiIndex\ncolumns (\nGH49121\n)\nBug in\nDataFrame.reindex()\ncasting dtype to\nobject\nwhen\nDataFrame\nhas single extension array column when re-indexing\ncolumns\nand\nindex\n(\nGH48190\n)\nBug in\nDataFrame.iloc()\nraising\nIndexError\nwhen indexer is a\nSeries\nwith numeric extension array dtype (\nGH49521\n)\nBug in\ndescribe()\nwhen formatting percentiles in the resulting index showed more decimals than needed (\nGH46362\n)\nBug in\nDataFrame.compare()\ndoes not recognize differences when comparing\nNA\nwith value in nullable dtypes (\nGH48939\n)\nBug in\nSeries.rename()\nwith\nMultiIndex\nlosing extension array dtypes (\nGH21055\n)\nBug in\nDataFrame.isetitem()\ncoercing extension array dtypes in\nDataFrame\nto object (\nGH49922\n)\nBug in\nSeries.__getitem__()\nreturning corrupt object when selecting from an empty pyarrow backed object (\nGH51734\n)\nBug in\nBusinessHour\nwould cause creation of\nDatetimeIndex\nto fail when no opening hour was included in the index (\nGH49835\n)\nMissing\n#\nBug in\nIndex.equals()\nraising\nTypeError\nwhen\nIndex\nconsists of tuples that contain\nNA\n(\nGH48446\n)\nBug in\nSeries.map()\ncaused incorrect result when data has NaNs and defaultdict mapping was used (\nGH48813\n)\nBug in\nNA\nraising a\nTypeError\ninstead of return\nNA\nwhen performing a binary operation with a\nbytes\nobject (\nGH49108\n)\nBug in\nDataFrame.update()\nwith\noverwrite=False\nraising\nTypeError\nwhen\nself\nhas column with\nNaT\nvalues and column not present in\nother\n(\nGH16713\n)\nBug in\nSeries.replace()\nraising\nRecursionError\nwhen replacing value in object-dtype\nSeries\ncontaining\nNA\n(\nGH47480\n)\nBug in\nSeries.replace()\nraising\nRecursionError\nwhen replacing value in numeric\nSeries\nwith\nNA\n(\nGH50758\n)\nMultiIndex\n#\nBug in\nMultiIndex.get_indexer()\nnot matching\nNaN\nvalues (\nGH29252\n,\nGH37222\n,\nGH38623\n,\nGH42883\n,\nGH43222\n,\nGH46173\n,\nGH48905\n)\nBug in\nMultiIndex.argsort()\nraising\nTypeError\nwhen index contains\nNA\n(\nGH48495\n)\nBug in\nMultiIndex.difference()\nlosing extension array dtype (\nGH48606\n)\nBug in\nMultiIndex.set_levels\nraising\nIndexError\nwhen setting empty level (\nGH48636\n)\nBug in\nMultiIndex.unique()\nlosing extension array dtype (\nGH48335\n)\nBug in\nMultiIndex.intersection()\nlosing extension array (\nGH48604\n)\nBug in\nMultiIndex.union()\nlosing extension array (\nGH48498\n,\nGH48505\n,\nGH48900\n)\nBug in\nMultiIndex.union()\nnot sorting when sort=None and index contains missing values (\nGH49010\n)\nBug in\nMultiIndex.append()\nnot checking names for equality (\nGH48288\n)\nBug in\nMultiIndex.symmetric_difference()\nlosing extension array (\nGH48607\n)\nBug in\nMultiIndex.join()\nlosing dtypes when\nMultiIndex\nhas duplicates (\nGH49830\n)\nBug in\nMultiIndex.putmask()\nlosing extension array (\nGH49830\n)\nBug in\nMultiIndex.value_counts()\nreturning a\nSeries\nindexed by flat index of tuples instead of a\nMultiIndex\n(\nGH49558\n)\nI/O\n#\nBug in\nread_sas()\ncaused fragmentation of\nDataFrame\nand raised\nerrors.PerformanceWarning\n(\nGH48595\n)\nImproved error message in\nread_excel()\nby including the offending sheet name when an exception is raised while reading a file (\nGH48706\n)\nBug when a pickling a subset PyArrow-backed data that would serialize the entire data instead of the subset (\nGH42600\n)\nBug in\nread_sql_query()\nignoring\ndtype\nargument when\nchunksize\nis specified and result is empty (\nGH50245\n)\nBug in\nread_csv()\nfor a single-line csv with fewer columns than\nnames\nraised\nerrors.ParserError\nwith\nengine=\"c\"\n(\nGH47566\n)\nBug in\nread_json()\nraising with\norient=\"table\"\nand\nNA\nvalue (\nGH40255\n)\nBug in displaying\nstring\ndtypes not showing storage option (\nGH50099\n)\nBug in\nDataFrame.to_string()\nwith\nheader=False\nthat printed the index name on the same line as the first row of the data (\nGH49230\n)\nBug in\nDataFrame.to_string()\nignoring float formatter for extension arrays (\nGH39336\n)\nFixed memory leak which stemmed from the initialization of the internal JSON module (\nGH49222\n)\nFixed issue where\njson_normalize()\nwould incorrectly remove leading characters from column names that matched the\nsep\nargument (\nGH49861\n)\nBug in\nread_csv()\nunnecessarily overflowing for extension array dtype when containing\nNA\n(\nGH32134\n)\nBug in\nDataFrame.to_dict()\nnot converting\nNA\nto\nNone\n(\nGH50795\n)\nBug in\nDataFrame.to_json()\nwhere it would segfault when failing to encode a string (\nGH50307\n)\nBug in\nDataFrame.to_html()\nwith\nna_rep\nset when the\nDataFrame\ncontains non-scalar data (\nGH47103\n)\nBug in\nread_xml()\nwhere file-like objects failed when iterparse is used (\nGH50641\n)\nBug in\nread_csv()\nwhen\nengine=\"pyarrow\"\nwhere\nencoding\nparameter was not handled correctly (\nGH51302\n)\nBug in\nread_xml()\nignored repeated elements when iterparse is used (\nGH51183\n)\nBug in\nExcelWriter\nleaving file handles open if an exception occurred during instantiation (\nGH51443\n)\nBug in\nDataFrame.to_parquet()\nwhere non-string index or columns were raising a\nValueError\nwhen\nengine=\"pyarrow\"\n(\nGH52036\n)\nPeriod\n#\nBug in\nPeriod.strftime()\nand\nPeriodIndex.strftime()\n, raising\nUnicodeDecodeError\nwhen a locale-specific directive was passed (\nGH46319\n)\nBug in adding a\nPeriod\nobject to an array of\nDateOffset\nobjects incorrectly raising\nTypeError\n(\nGH50162\n)\nBug in\nPeriod\nwhere passing a string with finer resolution than nanosecond would result in a\nKeyError\ninstead of dropping the extra precision (\nGH50417\n)\nBug in parsing strings representing Week-periods e.g. â2017-01-23/2017-01-29â as minute-frequency instead of week-frequency (\nGH50803\n)\nBug in\nDataFrameGroupBy.sum()\n,\nDataFrameGroupByGroupBy.cumsum()\n,\nDataFrameGroupByGroupBy.prod()\n,\nDataFrameGroupByGroupBy.cumprod()\nwith\nPeriodDtype\nfailing to raise\nTypeError\n(\nGH51040\n)\nBug in parsing empty string with\nPeriod\nincorrectly raising\nValueError\ninstead of returning\nNaT\n(\nGH51349\n)\nPlotting\n#\nBug in\nDataFrame.plot.hist()\n, not dropping elements of\nweights\ncorresponding to\nNaN\nvalues in\ndata\n(\nGH48884\n)\nax.set_xlim\nwas sometimes raising\nUserWarning\nwhich users couldnât address due to\nset_xlim\nnot accepting parsing arguments - the converter now uses\nTimestamp()\ninstead (\nGH49148\n)\nGroupby/resample/rolling\n#\nBug in\nExponentialMovingWindow\nwith\nonline\nnot raising a\nNotImplementedError\nfor unsupported operations (\nGH48834\n)\nBug in\nDataFrameGroupBy.sample()\nraises\nValueError\nwhen the object is empty (\nGH48459\n)\nBug in\nSeries.groupby()\nraises\nValueError\nwhen an entry of the index is equal to the name of the index (\nGH48567\n)\nBug in\nDataFrameGroupBy.resample()\nproduces inconsistent results when passing empty DataFrame (\nGH47705\n)\nBug in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwould not include unobserved categories in result when grouping by categorical indexes (\nGH49354\n)\nBug in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwould change result order depending on the input index when grouping by categoricals (\nGH49223\n)\nBug in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwhen grouping on categorical data would sort result values even when used with\nsort=False\n(\nGH42482\n)\nBug in\nDataFrameGroupBy.apply()\nand\nSeriesGroupBy.apply\nwith\nas_index=False\nwould not attempt the computation without using the grouping keys when using them failed with a\nTypeError\n(\nGH49256\n)\nBug in\nDataFrameGroupBy.describe()\nwould describe the group keys (\nGH49256\n)\nBug in\nSeriesGroupBy.describe()\nwith\nas_index=False\nwould have the incorrect shape (\nGH49256\n)\nBug in\nDataFrameGroupBy\nand\nSeriesGroupBy\nwith\ndropna=False\nwould drop NA values when the grouper was categorical (\nGH36327\n)\nBug in\nSeriesGroupBy.nunique()\nwould incorrectly raise when the grouper was an empty categorical and\nobserved=True\n(\nGH21334\n)\nBug in\nSeriesGroupBy.nth()\nwould raise when grouper contained NA values after subsetting from a\nDataFrameGroupBy\n(\nGH26454\n)\nBug in\nDataFrame.groupby()\nwould not include a\nGrouper\nspecified by\nkey\nin the result when\nas_index=False\n(\nGH50413\n)\nBug in\nDataFrameGroupBy.value_counts()\nwould raise when used with a\nTimeGrouper\n(\nGH50486\n)\nBug in\nResampler.size()\ncaused a wide\nDataFrame\nto be returned instead of a\nSeries\nwith\nMultiIndex\n(\nGH46826\n)\nBug in\nDataFrameGroupBy.transform()\nand\nSeriesGroupBy.transform()\nwould raise incorrectly when grouper had\naxis=1\nfor\n\"idxmin\"\nand\n\"idxmax\"\narguments (\nGH45986\n)\nBug in\nDataFrameGroupBy\nwould raise when used with an empty DataFrame, categorical grouper, and\ndropna=False\n(\nGH50634\n)\nBug in\nSeriesGroupBy.value_counts()\ndid not respect\nsort=False\n(\nGH50482\n)\nBug in\nDataFrameGroupBy.resample()\nraises\nKeyError\nwhen getting the result from a key list when resampling on time index (\nGH50840\n)\nBug in\nDataFrameGroupBy.transform()\nand\nSeriesGroupBy.transform()\nwould raise incorrectly when grouper had\naxis=1\nfor\n\"ngroup\"\nargument (\nGH45986\n)\nBug in\nDataFrameGroupBy.describe()\nproduced incorrect results when data had duplicate columns (\nGH50806\n)\nBug in\nDataFrameGroupBy.agg()\nwith\nengine=\"numba\"\nfailing to respect\nas_index=False\n(\nGH51228\n)\nBug in\nDataFrameGroupBy.agg()\n,\nSeriesGroupBy.agg()\n, and\nResampler.agg()\nwould ignore arguments when passed a list of functions (\nGH50863\n)\nBug in\nDataFrameGroupBy.ohlc()\nignoring\nas_index=False\n(\nGH51413\n)\nReshaping\n#\nBug in\nDataFrame.pivot_table()\nraising\nTypeError\nfor nullable dtype and\nmargins=True\n(\nGH48681\n)\nBug in\nDataFrame.unstack()\nand\nSeries.unstack()\nunstacking wrong level of\nMultiIndex\nwhen\nMultiIndex\nhas mixed names (\nGH48763\n)\nBug in\nDataFrame.melt()\nlosing extension array dtype (\nGH41570\n)\nBug in\nDataFrame.pivot()\nnot respecting\nNone\nas column name (\nGH48293\n)\nBug in\nDataFrame.join()\nwhen\nleft_on\nor\nright_on\nis or includes a\nCategoricalIndex\nincorrectly raising\nAttributeError\n(\nGH48464\n)\nBug in\nDataFrame.pivot_table()\nraising\nValueError\nwith parameter\nmargins=True\nwhen result is an empty\nDataFrame\n(\nGH49240\n)\nClarified error message in\nmerge()\nwhen passing invalid\nvalidate\noption (\nGH49417\n)\nBug in\nDataFrame.explode()\nraising\nValueError\non multiple columns with\nNaN\nvalues or empty lists (\nGH46084\n)\nBug in\nDataFrame.transpose()\nwith\nIntervalDtype\ncolumn with\ntimedelta64[ns]\nendpoints (\nGH44917\n)\nBug in\nDataFrame.agg()\nand\nSeries.agg()\nwould ignore arguments when passed a list of functions (\nGH50863\n)\nSparse\n#\nBug in\nSeries.astype()\nwhen converting a\nSparseDtype\nwith\ndatetime64[ns]\nsubtype to\nint64\ndtype raising, inconsistent with the non-sparse behavior (\nGH49631\n,:issue:\n50087\n)\nBug in\nSeries.astype()\nwhen converting a from\ndatetime64[ns]\nto\nSparse[datetime64[ns]]\nincorrectly raising (\nGH50082\n)\nBug in\nSeries.sparse.to_coo()\nraising\nSystemError\nwhen\nMultiIndex\ncontains a\nExtensionArray\n(\nGH50996\n)\nExtensionArray\n#\nBug in\nSeries.mean()\noverflowing unnecessarily with nullable integers (\nGH48378\n)\nBug in\nSeries.tolist()\nfor nullable dtypes returning numpy scalars instead of python scalars (\nGH49890\n)\nBug in\nSeries.round()\nfor pyarrow-backed dtypes raising\nAttributeError\n(\nGH50437\n)\nBug when concatenating an empty DataFrame with an ExtensionDtype to another DataFrame with the same ExtensionDtype, the resulting dtype turned into object (\nGH48510\n)\nBug in\narray.PandasArray.to_numpy()\nraising with\nNA\nvalue when\nna_value\nis specified (\nGH40638\n)\nBug in\napi.types.is_numeric_dtype()\nwhere a custom\nExtensionDtype\nwould not return\nTrue\nif\n_is_numeric\nreturned\nTrue\n(\nGH50563\n)\nBug in\napi.types.is_integer_dtype()\n,\napi.types.is_unsigned_integer_dtype()\n,\napi.types.is_signed_integer_dtype()\n,\napi.types.is_float_dtype()\nwhere a custom\nExtensionDtype\nwould not return\nTrue\nif\nkind\nreturned the corresponding NumPy type (\nGH50667\n)\nBug in\nSeries\nconstructor unnecessarily overflowing for nullable unsigned integer dtypes (\nGH38798\n,\nGH25880\n)\nBug in setting non-string value into\nStringArray\nraising\nValueError\ninstead of\nTypeError\n(\nGH49632\n)\nBug in\nDataFrame.reindex()\nnot honoring the default\ncopy=True\nkeyword in case of columns with ExtensionDtype (and as a result also selecting multiple columns with getitem (\n[]\n) didnât correctly result in a copy) (\nGH51197\n)\nBug in\nSeries.any()\nand\nSeries.all()\nreturning\nNA\nfor empty or all null pyarrow-backed data when\nskipna=True\n(\nGH51624\n)\nBug in\nArrowExtensionArray\nlogical operations\n&\nand\n|\nraising\nKeyError\n(\nGH51688\n)\nStyler\n#\nFix\nbackground_gradient()\nfor nullable dtype\nSeries\nwith\nNA\nvalues (\nGH50712\n)\nMetadata\n#\nFixed metadata propagation in\nDataFrame.corr()\nand\nDataFrame.cov()\n(\nGH28283\n)\nOther\n#\nBug in incorrectly accepting dtype strings containing â[pyarrow]â more than once (\nGH51548\n)\nBug in\nSeries.searchsorted()\ninconsistent behavior when accepting\nDataFrame\nas parameter\nvalue\n(\nGH49620\n)\nBug in\narray()\nfailing to raise on\nDataFrame\ninputs (\nGH51167\n)\nContributors\n#\nA total of 260 people contributed patches to this release.  People with a\nâ+â by their names contributed a patch for the first time.\n5j9 +\nABCPAN-rank +\nAarni Koskela +\nAashish KC +\nAbubeker Mohammed +\nAdam MrÃ³z +\nAdam Ormondroyd +\nAditya Anulekh +\nAhmed Ibrahim\nAkshay Babbar +\nAleksa Radojicic +\nAlex +\nAlex Buzenet +\nAlex Kirko\nAllison Kwan +\nAmay Patel +\nAmbuj Pawar +\nAmotz +\nAndreas Schwab +\nAndrew Chen +\nAnton Shevtsov\nAntonio Ossa Guerra +\nAntonio Ossa-Guerra +\nAnushka Bishnoi +\nArda Kosar\nArmin Berres\nAsadullah Naeem +\nAsish Mahapatra\nBailey Lissington +\nBarkotBeyene\nBen Beasley\nBhavesh Rajendra Patil +\nBibek Jha +\nBill +\nBishwas +\nCarlosGDCJ +\nCarlotta Fabian +\nChris Roth +\nChuck Cadman +\nCorralien +\nDG +\nDan Hendry +\nDaniel Isaac\nDavid Kleindienst +\nDavid Poznik +\nDavid Rudel +\nDavidKleindienst +\nDea MarÃ­a LÃ©on +\nDeepak Sirohiwal +\nDennis Chukwunta\nDouglas Lohmann +\nDries Schaumont\nDustin K +\nEdoardo Abati +\nEduardo Chaves +\nEge ÃzgÃ¼roÄlu +\nEkaterina Borovikova +\nEli Schwartz +\nElvis Lim +\nEmily Taylor +\nEmma Carballal Haire +\nErik Welch +\nFangchen Li\nFlorian Hofstetter +\nFlynn Owen +\nFredrik Erlandsson +\nGaurav Sheni\nGeoreth Chow +\nGeorge Munyoro +\nGuilherme Beltramini\nGulnur Baimukhambetova +\nH L +\nHans\nHatim Zahid +\nHighYoda +\nHiki +\nHimanshu Wagh +\nHugo van Kemenade +\nIdil Ismiguzel +\nIrv Lustig\nIsaac Chung\nIsaac Virshup\nJHM Darbyshire\nJHM Darbyshire (iMac)\nJMBurley\nJaime Di Cristina\nJan Koch\nJanVHII +\nJanosh Riebesell\nJasmandeepKaur +\nJeremy Tuloup\nJessica M +\nJonas Haag\nJoris Van den Bossche\nJoÃ£o Meirelles +\nJulia Aoun +\nJustus Magin +\nKang Su Min +\nKevin Sheppard\nKhor Chean Wei\nKian Eliasi\nKostya Farber +\nKotlinIsland +\nLakmal Pinnaduwage +\nLakshya A Agrawal +\nLawrence Mitchell +\nLevi Ob +\nLoic Diridollou\nLorenzo Vainigli +\nLuca Pizzini +\nLucas Damo +\nLuke Manley\nMadhuri Patil +\nMarc Garcia\nMarco Edward Gorelli\nMarco Gorelli\nMarcoGorelli\nMaren Westermann +\nMaria Stazherova +\nMarie K +\nMarielle +\nMark Harfouche +\nMarko Pacak +\nMartin +\nMatheus Cerqueira +\nMatheus Pedroni +\nMatteo Raso +\nMatthew Roeschke\nMeeseeksMachine +\nMehdi Mohammadi +\nMichael Harris +\nMichael Mior +\nNatalia Mokeeva +\nNeal Muppidi +\nNick Crews\nNishu Choudhary +\nNoa Tamir\nNoritada Kobayashi\nOmkar Yadav +\nP. Talley +\nPablo +\nPandas Development Team\nParfait Gasana\nPatrick Hoefler\nPedro Nacht +\nPhilip +\nPietro Battiston\nPooja Subramaniam +\nPranav Saibhushan Ravuri +\nPranav. P. A +\nRalf Gommers +\nRaphSku +\nRichard Shadrach\nRobsdedude +\nRoger\nRoger Thomas\nRogerThomas +\nSFuller4 +\nSalahuddin +\nSam Rao\nSean Patrick Malloy +\nSebastian Roll +\nShantanu\nShashwat +\nShashwat Agrawal +\nShiko Wamwea +\nShoham Debnath\nShubhankar Lohani +\nSiddhartha Gandhi +\nSimon Hawkins\nSoumik Dutta +\nSowrov Talukder +\nStefanie Molin\nStefanie Senger +\nStepfen Shawn +\nSteven Rotondo\nStijn Van Hoey\nSudhansu +\nSven\nSylvain MARIE\nSylvain MariÃ©\nTabea Kossen +\nTaylor Packard\nTerji Petersen\nThierry Moisan\nThomas H +\nThomas Li\nTorsten WÃ¶rtwein\nTsvika S +\nTsvika Shapira +\nVamsi Verma +\nVinicius Akira +\nWilliam Andrea\nWilliam Ayd\nWilliam Blum +\nWilson Xing +\nXiao Yuan +\nXnot +\nYasin Tatar +\nYuanhao Geng\nYvan Cywan +\nZachary Moon +\nZhengbo Wang +\nabonte +\nadrienpacifico +\nalm\namotzop +\nandyjessen +\nanonmouse1 +\nbang128 +\nbishwas jha +\ncalhockemeyer +\ncarla-alves-24 +\ncarlotta +\ncasadipietra +\ncatmar22 +\ncfabian +\ncodamuse +\ndataxerik\ndavidleon123 +\ndependabot[bot] +\nfdrocha +\ngithub-actions[bot]\nhimanshu_wagh +\niofall +\njakirkham +\njbrockmendel\njnclt +\njoelchen +\njoelsonoda +\njoshuabello2550\njoycewamwea +\nkathleenhang +\nkrasch +\nltoniazzi +\nluke396 +\nmilosz-martynow +\nminat-hub +\nmliu08 +\nmonosans +\nnealxm\nnikitaved +\nparadox-lab +\npartev\nraisadz +\nram vikram singh +\nrebecca-palmer\nsarvaSanjay +\nseljaks +\nsilviaovo +\nsmij720 +\nsoumilbaldota +\nstellalin7 +\nstrawberry beach sandals +\ntmoschou +\nuzzell +\nyqyqyq-W +\nyun +\nÃdÃ¡m Lippai\nê¹ëí (Daniel Donghyun Kim) +\nprevious\nWhatâs new in 2.0.1 (April 24, 2023)\nnext\nWhatâs new in 1.5.3 (January 18, 2023)\nOn this page\nEnhancements\nInstalling optional dependencies with pip extras\nIndex\ncan now hold numpy numeric dtypes\nArgument\ndtype_backend\n, to return pyarrow-backed or numpy-backed nullable dtypes\nCopy-on-Write improvements\nOther enhancements\nNotable bug fixes\nDataFrameGroupBy.cumsum()\nand\nDataFrameGroupBy.cumprod()\noverflow instead of lossy casting to float\nDataFrameGroupBy.nth()\nand\nSeriesGroupBy.nth()\nnow behave as filtrations\nBackwards incompatible API changes\nConstruction with datetime64 or timedelta64 dtype with unsupported resolution\nValue counts sets the resulting name to\ncount\nDisallow astype conversion to non-supported datetime64/timedelta64 dtypes\nUTC and fixed-offset timezones default to standard-library tzinfo objects\nEmpty DataFrames/Series will now default to have a\nRangeIndex\nDataFrame to LaTeX has a new render engine\nIncreased minimum versions for dependencies\nDatetimes are now parsed with a consistent format\nOther API changes\nDeprecations\nRemoval of prior version deprecations/changes\nPerformance improvements\nBug fixes\nCategorical\nDatetimelike\nTimedelta\nTimezones\nNumeric\nConversion\nStrings\nInterval\nIndexing\nMissing\nMultiIndex\nI/O\nPeriod\nPlotting\nGroupby/resample/rolling\nReshaping\nSparse\nExtensionArray\nStyler\nMetadata\nOther\nContributors\nShow Source",
    "crawl_status": "success"
  }
]